[{"content":"不同的“降维”思路 “降维”，顾名思义就是降低数据维度（数据维度 = 原始变量个数）。\n主成分分析和因子分析使用的都是“降维”的思想，但两者“降维”的思路不一样，两者“降维”的思路差异，也就造成了两者的“可解释性”差异：\n因子分析则是先提炼因子，再用所提炼的因子来表达原始变量，==可解释性强==。 而主成分分析是通过将原始变量进行线性组合（称为主成分）来达到“降维”的目的，==可解释性弱==； 举个例子：假设我们收集了 100 名学生在 4 门课程（语文、数学、地理、生物）中的考试成绩，我们希望找出数据的潜在结构，减少变量维度，便于分析。\n因子分析： 先设定因子：文科因子和理科因子 语文 = 1 × 文科因子 + 0 × 理科因子 数学 = 0 × 文科因子 + 1 × 理科因子 生物 = 0.3 × 文科因子 + 0.7 × 理科因子 地理 = 0.6 × 文科因子 + 0.4 × 理科因子 主成分分析（这里不太严谨，因为没保证正交）： 理科主成分 = 数学 + 0.7 生物 + 0.4 地理； 文科主成分 = 语文 + 0.3 生物 + 0.6 地理。 主成分分析 主成分分析（Principal Component Analysis，PCA）是研究如何通过原始变量的少数几个线性组合来解释原始变量绝大多数信息的多元统计分析方法。在尽可能地保留原始变量信息的前提下进行降维，从而简化问题的复杂性，==抓住问题的主要矛盾==。 该方法主要基于众多原始变量之间有一定的相关性(即共线性)，则必然存在着起支配作用的共同因素这一想法，通过对原始变量==协方差矩阵或相关系数矩阵==内部结构进行研究。\n使用前提 开展PCA一般需要满足以下前提条件：\n原始数据的变量数目较多，或有数据降维的需求，否则做主成分分析没有意义。 原始数据各个变量之间的共线性或相关关系较强，如果原始变量之间的线性相关程度很小，它们之间不存在简化的数据结构，这时进行主成分分析实际是没有意义的。 在应用PCA之前，需要对其适用性进行统计检验，检验方法有抽样适合性检验(measure sampling adequacy)和巴特利特(Bartlett)球形检验等。 PCA使用前提摘自主成分分析(Principal Component Analysis)——理论介绍\n通俗理解主成分分析 假如原始数据中总共有 100 个原始变量，为方便分析及解释，我们希望把这 100 个原始变量给“浓缩”一下。注意，“浓缩”体现出主成分分析的精髓：\n浓：原始变量蕴含的信息，在操作中要最大程度地保留； 缩：维度要降低。 我们想要尽可能地保留原始数据中的信息（因为降维不可避免地会造成信息的丢失，想象从 3 维空间到 2 维空间）什么时候信息的“利用率”最高呢？答案是正交的时候。变量间相互正交，可以理解为不同变量所蕴含的信息不重复，每个变量中蕴含的信息都是独一无二、无可替代的。\n信息这个词有点抽象，其实有个概念可以很好地量化“信息”这个词：方差。变量的“方差”所蕴含的是“信息量”（理解：变异程度越大，其中所含信息就越多）\n所以：思路清楚了：把原始变量组合成方差大且正交的新变量即可。\n具体操作 我们从包含变量方差信息的矩阵——协方差矩阵入手。\n既想降维，减少变量个数，又想尽可能地保留信息：把方差大的几个原始变量组合起来不就好了。\n对协方差矩阵进行==特征值分解==不就好了。取最大的几个特征值所对应的特征向量作为主成分即可。\n但是特征值分解有个问题，只适用于方阵，且更致命的是：计算量成本巨高。\n所以便引入使用==奇异值分解==的方法。详见推文\n关于奇异值分解，其思想跟 LoRA (针对LLM的一种轻量级微调技术)相似：通过“低秩近似”以减小算法上的开支：\n例如一个 985 × 211 的矩阵，有 20w+ 个元素需要计算；通过低秩近似，用 985 × 10 乘 10 × 211，便只需要计算不到 2w 个元素\n特征值分解和奇异值分解两种方法，都能达到目的。但途径不同，各有千秋。\n使用协方差矩阵还是相关系数矩阵 ？ 各变量量纲差不多时，使用协方差矩阵即可。反之则使用相关系数矩阵（多了一步标准化的操作）\n个人感觉无脑使用相关系数矩阵即可，不知道协方差矩阵在相关系数矩阵面前有什么优势？如有知道的朋友，还请指教。\n主成分分析与聚类分析 挖个坑先。\n因子分析 面对主成分分析中主成分难以解释的问题，因子分析，被当做主成分分析的推广和发展，得到广泛应用。\n因子模型 $$x = \\mu + Af + \\varepsilon$$\n式中 f = $(f_1, f_2, \\cdots, f_m)\u0026rsquo;$ 为公共因子向量，$\\varepsilon = (\\varepsilon_1, \\varepsilon_2, \\cdots, \\varepsilon_p)\u0026rsquo;$ 为特殊因子向量，$A = (a_{ij}): p \\times m$ 称为因子载荷矩阵。\n模型假设：\n公共因子之间不相关； 特殊因子之间不相关； 公共因子和特殊因子之间不相关 下面的任务便是：估计因子载荷矩阵 $A$ 和公共因子向量 $f$。\n可以对比一下回归分析中 $Y=X\\beta+\\varepsilon$ 主要估计的是 $\\beta$ ，$X$ 是已知的观测值。\n因子载荷矩阵常用的估计方法 主成分法、主因子法、极大似然法\n未完待续\n原始变量与主成分/因子间的关系 主成分分析：相关系数和载荷 $x_i = t_{i1}y_1 + t_{i2}y_2 + \\cdots + t_{ip}y_p, \\quad i = 1, 2, \\ldots, p$\n两个指标：相关系数 $\\rho(x_i, y_k)=\\frac{\\sqrt{\\lambda_k}}{\\sqrt{\\sigma_{ii}}}t_{ik}$ 和载荷(即主成分表达式中的系数)\n相关系数：单变量角度，忽略了其它原始变量的前提下==某原始变量与某主成分之间的关系==； 载荷：多变量角度，考虑到了其它原始变量在场的情况下某==原始变量与某主成分之间的关系==。 所有主成分对某原始变量的贡献率：m 个主成分 $y_1, y_2, \u0026hellip;, y_m$ 对原始变量 $x_i$ 的贡献率 $\\rho_{i \\cdot 1, \\cdots, m}^2 = \\sum_{k=1}^{m} \\rho^2(x_i, y_k) = \\sum_{k=1}^{m} \\frac{\\lambda_k t_{ik}^2}{\\sigma_{ii}}$；\n在解释主成分时，看哪个呢？马哲告诉我们：既需要考察相关系数，又需要考察载荷。\n因子分析：因子载荷矩阵 未完待续\n旋转 主成分分析和因子分析中的旋转，目的是为了提高可解释性。\n让每个变量对某个主成分/因子的贡献更清晰（即让因子载荷更接近 0 或 1，减少多个因子共享同一变量的情况）。例如 某主成分 = $0.6X + 0.5Y + 0.5Z + \u0026hellip;$ 有些含糊 某主成分 = $0.9X\u0026rsquo; + 0.1Y\u0026rsquo; + 0.05Z\u0026rsquo; + \u0026hellip;$ 更清晰明了 让主成分/因子更加符合实际意义（即调整主成分/因子，使其更接近数据中方差较大的方向，同时减少信息混杂）。 可以看作一种对“主成分”或“因子”的优化。\n时序数据能进行 PCA 和因子分析吗？ 绝大多数情况都不能，因为时间序列数据在绝大多数情况下都存在自相关性，不是简单随机样本 (要求 iid)，样本协方差矩阵 (相关系数矩阵) 不是总体协方差矩阵 (相关系数矩阵) 的无偏估计，贸然应用样本协方差矩阵 (相关系数矩阵) 会产生较大偏差。\n如处理后的数据消除了自相关性，则可考虑进行 PCA 和因子分析。\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%9D%A5%E8%87%AA%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%9A%84%E9%99%8D%E7%BB%B4%E6%89%93%E5%87%BB/","summary":"不同的“降维”思路 “降维”，顾名思义就是降低数据维度（数据维度 = 原始变量个数）。 主成分分析和因子分析使用的都是“降维”的思想，但两者“降维”的思路不一样，两者“降维”的思路差异，也就造成了两者的“可解释性”差异： 因子分析则是先提炼因子，再用所提炼的因子来表达原始变量，==可解释性","title":"来自因子分析与主成分分析的降维打击"},{"content":"假设检验是统计推断的重要方法！统计推断包括假设检验和参数估计。\n假设检验的定义 假设检验（hypothesis testing）是指从对总体参数所做的一个假设开始，然后搜集样本数据，计算出样本统计量，进而运用这些数据测定假设的总体参数在多大程度上是可靠的，并做出承认还是拒绝该假设的判断。如果进行假设检验时总体的分布形式已知，需要对总体的未知参数进行假设检验，称其为参数假设检验；若对总体分布形式所知甚少，需要对未知分布函数的形式及其他特征进行假设检验，通常称之为非参数假设检验。此外，根据研究者感兴趣的备择假设的内容不同，假设检验还可分为单侧检验（单尾检验）和双侧检验（双尾检验），而单侧检验又分为左侧检验和右侧检验。 - 国家统计局\n所谓假设检验，就是通过样本来推测总体是否具备某种性质。\n假设检验的原理 假设检验使用的是“证伪”的思想，是一种“反证法”。\n假设检验的基本思想是==反证法思想==和==小概率事件原理==。 反证法的思想是首先提出假设（由于未经检验是否成立，所以称为零假设、原假设或无效假设），然后用适当的统计方法确定假设成立的可能性大小，如果可能性小，则认为假设不成立，拒绝它；如果可能性大，还不能认为它不成立。 小概率事件原理，是指小概率事件在一次随机试验中几乎不可能发生，小概率事件发生的概率一般称之为“显著性水平”或“检验水平”，用 $\\alpha$ 表示，而概率小于多少算小概率是相对的，在进行统计分析时要事先规定，通常取 $\\alpha=0.01、0.05、0.10$ 等。 - 国家统计局\n假设检验具体是如何操作的？ 代入给定的 $H_0$ 的参数值建立分布 看看样本“像不像”是从这个分布里头出来的（是否落入拒绝域） 若“像”（落入接受域），这帮样本就是从 $H_0$ 那个分布来的，说明 $H_0$ 对应的参数有可能就是真实参数； 若“不像”（落入拒绝域），好家伙这帮样本不是从 $H_0$ 那个分布来的，说明 $H_0$ 对应的参数大概率不是真实参数（拒绝 $H_0$）。但是 $H_0$ 错误，不代表 $H_1$ 就正确，所以只能说“拒绝 $H_0$”，而不能说“接受 $H_1$”） 如何量化这个“像”与“不像”，详见后面关于显著性水平 $\\alpha$ 和 P-value 的章节。 Over 下结论。 知道了假设检验具体是如何操作的，也就知道了:\n为什么假设检验要将等号放在原假设? 因为检验统计量的计算需要给定一个（假想的）参数值，否则就无法建立相应的分布，不方便计算。比如说，简单的单总体均值检验中，假定 $H_0: \\mu = 20$ 这样才能简单地把确定在这个假想均值确定的抽样分布下，把检验统计量给算出来。不带等号的话，这个分布的确定就困难了。\n假如，我偏不把等号放在原假设，把不等号放在原假设，例如 $H_0: \\mu \u0026lt; 20$ 我取 $\\mu$ 等于多少来建立分布呢？是 19？还是 15？亦或是 10？这时不就出现问题了。\n关于检验统计量的选取 分布是个好东西！\nEg. 现有一关于参数 $\\theta$ 的假设检验：==理论上==只要某统计量 T 的分布与参数 $\\theta$ 有关，即可用它来构造检验统计量。\n因为如果我们能得到 $H_0$ 成立下统计量 T 的分布，即可计算出在该分布中出现样本或其它更极端情况的概率。\n关键是：分布！\n检验统计量的选取 - 这个地方可以想个例子来帮助理解。 假设检验的精髓：不平衡（Imbalance） 假设检验中，原假设 $H_0$ 和备择假设 $H_1$ 的地位是不对等的：\n$H_0$ 通常是被默认为真的假设，需要足够的证据才能拒绝； 而 $H_1$ 则与 $H_0$ 相对立，通常是研究者试图去证明的假设。 打个比方：\n$H_0$ 像是“正统”，“权威”，“传统”，具有一定的“优先权”，默认情况下它都是正确的 而 $H_1$ 则像是“新教”，“创新”，甚至有些“备胎”的味道，需要在将 $H_0$ 拒绝后，才可能“喧宾夺主”，有“翻身”的机会。这里使用的“可能”、“机会”两词是严谨的，因为在实际的假设检验中： 哪怕拒绝了原假设，我们会说在 xx 的显著性水平下能够“拒绝 $H_0$”，而不是“接受 $H_1$”； 同样若没拒绝原假设，我们会说在 xx 的显著性水平下不能“拒绝 $H_0$”，而不是“接受 $H_0$”。 总结：判断一个假设是否『正确』是很难的，然而判断一个假设是否『错误』相对来说就容易多了：因为证伪只需要一个特例就足够了，==一个特例足以推翻一个论点，却远远不能支撑一个论点==。 之所以如此不平衡地设计“假设检验”这个东西，是为了给原假设一定“优先权”，这样有助于控制做出错误结论的风险（特别是第一类错误）。打个比方：“传统”的 $H_0$ 一般都是正确的，为防止左倾冒进分子冲动地推翻，于是在设计上给了它（即 $H_0$ ）一定的“优先权”。（备胎想要成为上位？很难的啦）\n矛盾无处不在，这种“不平衡”的设计，同样具有双面性：虽然确实有助于控制做出错误结论的风险，但如此不平衡的设计导致它并非使用于所有问题。如：\n现有一堆 0-2 之间的样本，判断是来自 $U(0, 1)$ 还是 $U(1, 2)$ 。这个问题就不适用于假设检验。因为在这个问题中 $U(0, 1)$ 和 $U(1, 2)$ 的“地位”是平等的。可通过极大似然估计 MLE 来解决这个问题。（分别计算样本在 $U(0, 1)$ 和 $U(1, 2)$ 的对数似然值即可，看哪个大）\n而像女士品茶（==将牛奶倒入茶==和==将茶倒入牛奶==对奶茶的口味是否有影响）这样的问题，显然是能够根据我们的先验知识做出一定猜测的，这种问题便可以通过假设检验这样的方法来解决。\n通过了解如何做出原假设，能够帮助你更好地理解假设检验的 Imbalance.\n做出原假设的依据 根据现有理论或知识：原假设往往基于现有的理论或广泛接受的知识。例如，如果现有理论表明两种药物效果相同，那么原假设可能就是\u0026quot;这两种药物的效果没有差异”。 简单性或保守性：在统计学中，原假设通常是一个简单假设，它提出了最简单、最保守的情况。例如，“新药与安慰剂效果无差别”是一个比“新药比安慰剂效果好”更简单、更保守的假设。 研究目的：研究者可能会根据研究目的来确定原假设。如果研究目的是证明一种新的干预措施有效，那么原假设可能就是“新干预措施与现有措施效果相同”。 来自北京师范大学-432统计学-2024年-解析\n假设检验中的显著性水平 $\\alpha$ 与功效 $1-\\beta$ 与 P-value 项目 无法拒绝 $H_0$ 拒绝 $H_0$ $H_0$ 为真 $1-\\alpha$ $\\alpha$ ==弃真错误== $H_0$ 为伪 $\\beta$ ==取伪错误== $1-\\beta$ ==功效== 用极限的思想来理解：在固定样本量的前提下，不能同时减小第一类错误 $\\alpha$ 和第二类错误 $\\beta$ ：\n若 $\\alpha=0$ -\u0026gt; 不犯弃真错误 -\u0026gt; 索性直接接受原假设 -\u0026gt; 更有可能犯 $\\beta$ 若 $\\beta=0$ -\u0026gt; 不犯取伪错误 -\u0026gt; 索性直接拒绝原假设 -\u0026gt; 更有可能犯 $\\alpha$ 整个实验来看：$\\alpha + \\beta \\in (0, 2)$ ，若某分类器将所有正例均判为负，将所有负例都判为正，这时 $\\alpha=\\beta=1, \\quad \\alpha+\\beta=2$ 单次实验结果来看，不可能同时发生弃真错误和取伪错误。表格中的四个值 $1-\\alpha, \\quad \\alpha, \\quad \\beta, \\quad 1-\\beta$ 均 $\\in (0, 1)$ 显著性水平 $\\alpha$ 在假设检验中，显著性水平（Significance Level），通常记为 $\\alpha$，是指在原假设 $H_0$ 为真时，错误地拒绝 $H_0$ 的概率，即第一类错误（Type I Error）的概率。换句话说，$\\alpha$ 代表了==我们能够接受的犯第一类错误的概率==（假阳性风险），即我们错误地发现了一个不存在的效应的可能性。\n两类错误，在具体实践中往往更加关注弃真错误 $\\alpha$ ，因为错误地拒绝“传统”、“权威”的原假设 $H_0$ （即弃真错误）所带来的损失，相对于在权威假设 $H_0$ 错误的背景下, 没能找到真正正确的假设 $H_1$，仍误认为权威假设 $H_0$ 是正确的（即取伪错误）而言，是更大的。\n例如，在制药行业，如果一个新药被批准（拒绝 $H_0$ ​）但实际上无效或有害，会带来巨大损失；相反，如果一个有效药物未被批准（即犯第二类错误），虽然可惜，但仍可在未来研究中重新评估。\nP值 P-value P-value：在原假设成立的条件下，出现样本或更极端情况的概率。\n在假设检验实际操作中，可通过比较 P-value 与显著性水平 $\\alpha$ 值的大小比较来判断是否应该拒绝原假设。\n举个稍微有点极端的例子：现有样本来自某正态分布 $N(\\mu, 0)$ ：[8, 17, 10, 9]，考虑假设检验问题 $H_0: \\mu=0$ Vs $H_1: \\mu=10$ 。\n我们来看看此时原假设 $H_0: \\mu=0$ 成立的条件下出现样本或更极端情况的概率，即从 $N(0, 1)$ 中抽到[8, 17, 10, 9]这几个样本的概率，显然很小，即 P-value 很小，P-value 越小，越是应该拒绝原假设。\n想想，咱假设原假设成立，可是发现这个假设下出现我们实际抽到的样本的概率极低，此时理所当然应该拒绝原假设。\nP-value 一般都介于 0-1 之间，怎么量化它的“小”呢？这时显著性水平 $\\alpha$ 就站了出来，大喊一声“我来！”。可以将显著性水平 $\\alpha$ 理解为“阈值”\n若 P-value \u0026lt;= $\\alpha$ ，说明 P-value \u0026lt; 阈值，拒绝原假设 反之则不能拒绝原假设 功效 Power $1-\\beta$ 另外一个稍微“小众”一些的概念 - 功效（Power）：反映检验在面对正确的备择假设时正确做出决定的能力。有点像“抓走坏人”的能力。毫无疑问，越大越好。\n再延伸一个更“小众”的概念 - 势函数 / 功效函数 (Power Funtion)：功效函数（Power Function）是用于衡量检验方法在不同实际参数值下拒绝原假设的概率。==即不同实际参数值下样本观测值落入拒绝域内的概率==。综合反映了第一类错误和功效的情况。\n$$ g(\\theta) = \\begin{cases} \\begin{array}{rl} \\phantom{1 -} \\alpha(\\theta), \u0026amp; \\theta \\in \\Theta_0 \\\\ 1 - \\beta(\\theta), \u0026amp; \\theta \\in \\Theta_1 \\end{array} \\end{cases} $$\n当 $H_0$ 为真时，$g(\\theta)$ 为犯第一类错误的概率 当 $H_1$ 为真时，$g(\\theta)$ 为功效\n既然固定样本容量时，任何检验都不能同时让第一类错误和第二类错误的概率很小，那么 Neyman-Pearson 所提出的原则就是：在保证犯第一类错误的概率不超过指定数值αα的检验中，寻找犯第二类错误概率尽可能小的检验。（N-P准则）\nN-P 准则用势函数 / 功效函数（Power Function）来表达： $$ \\begin{cases} g(\\theta) \\leq \\alpha, \u0026amp; \\theta \\in \\Theta_0 \\\\ g(\\theta) \\text{ 尽可能大}, \u0026amp; \\theta \\in \\Theta_1 \\end{cases} $$\n其它 其它条件不变的前提下，增大样本量，会使 $\\alpha$ 和 $\\beta$ 同时减小吗？争议问题🤔\n会 不会 显著性水平是人为给定，但犯第一类错误 $\\alpha$ 的概率不是恒定的，它会受样本量和犯第二类错误 $\\beta$ 的概率等影响。\n——贾俊平《统计学》和我的概率论老师 增大样本量，只能降低犯第二类错误 $\\beta$ 的概率，而犯第一类错误的概率是由人为设定的显著性水平 $\\alpha$ 来决定的，无法通过增大样本量来降低。\n——《卫生统计学第八版》和 GPT ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E4%B9%9D%E9%98%B3%E7%9C%9F%E7%BB%8F---%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E7%AF%87/","summary":"假设检验是统计推断的重要方法！统计推断包括假设检验和参数估计。 假设检验的定义 假设检验（hypothesis testing）是指从对总体参数所做的一个假设开始，然后搜集样本数据，计算出样本统计量，进而运用这些数据测定假设的总体参数在多大程度上是可靠的，并做出承认还是拒绝该假设的判断","title":"九阳真经 - 假设检验篇"},{"content":"随机事件层面的独立 两个随机事件间的独立性定义： $P(AB)=P(A)P(B)$，即 $P(A|B)=P(A)$ 。不独立，又称相依。\n多个随机事件间的独立性定义：设有 N 个事件 $A_1, A_2, \u0026hellip;, A_N$ 对任意的 $1 ≤ i ≤ j ≤ k ≤ N$ 如下式成立：\n$$ \\begin{aligned} P(A_i A_j) \u0026amp;= P(A_i) P(A_j) \\quad \\text{两两独立} \\\\ P(A_i A_j A_k) \u0026amp;= P(A_i) P(A_j) P(A_k) \\quad \\text{三三独立} \\\\ \u0026amp;\\quad \\vdots \\\\ P(A_i A_j \\cdots A_N) \u0026amp;= P(A_i) P(A_j) \\cdots P(A_N) \\quad \\text{NN独立} \\end{aligned} $$ 则称 N 个事件 $A_1, A_2, \u0026hellip;, A_N$ 相互独立。\n多个事件间相互独立 \u0026lt;=\u0026gt; 多个事件间两两独立、三三独立、……、NN 独立\n将相互独立事件中的任一部分转换为对立事件，所得诸事件依然是相互独立的。\n如若事件 A 与事件 B 独立，则 $A$ 与 $\\bar{B}$, $\\bar{A}$ 与 $\\bar{B}$, $\\bar{A}$ 与 $B$ 均独立。\n多个随机事件情形同样满足\n随机试验层面的独立 独立性与相容性 相容性定义：如果 A 与 B 没有相同的样本点，则称 A 与 B 互不相容。即 A 与 B 不可能同时发生。\n==在事件概率不为 0 的前提==下讨论独立性与相容性的关系：\n独立 =\u0026gt; 相容：$P(AB)=P(A)P(B)≠0$ 说明 A 与 B 之间有交集，两者可能同时发生；但反之则不成立：相容事件不一定独立，例如掷一枚骰子，A: 点数\u0026lt;4 , B: 点数\u0026gt;3，AB 有交集，两者可能同时发生，相容，但不独立。 逆否命题：互不相容 =\u0026gt; 相依（不独立），互不相容 =\u0026gt; $P(AB)=0≠P(A)P(B)$ 说明 A 与 B 不独立。反之同样不成立：相依事件可能相容，例如掷两枚骰子，A：第一次为 6，B：总和大于 8，AB 两事件既相容又相依。 独立性：概率层面，反映前后实验结果是否相互影响； 相容性：事件层面，反映不同事件能否同时发生。\n可以说，两者其实是不同维度的概念。\n独立性与相关性 不相关是指两个变量的相关系数为0，两个变量之间没有线性关系的。\n相关与否，仅是线性层面的。而独立与否，不仅包括线性层面，还包括非线性层面。\n于是可以说：独立的要求，比不相关，要更加严格。\n独立 =\u0026gt; 不相关，反之不成立，事件间不相关（无线性关系）≠\u0026gt; 事件间独立（可能存在非线性关系） 逆否命题：相关 =\u0026gt; 相依，反之同样不成立，相依（不独立，说明事件间有关系）≠\u0026gt; 相关（因为可能不是线性关系，而是非线性关系） ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%85%B3%E4%BA%8E%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%AD%E7%9A%84%E7%8B%AC%E7%AB%8B/","summary":"随机事件层面的独立 两个随机事件间的独立性定义： $P(AB)=P(A)P(B)$，即 $P(A|B)=P(A)$ 。不独立，又称相依。 多个随机事件间的独立性定义：设有 N 个事件 $A_1, A_2, \u0026hellip;, A_N$ 对任意的 $1 ≤ i ≤ j ≤ k ≤ N$ 如下式成立： $$ \\begin{aligned} P(A_i A_j) \u0026amp;= P(A_i) P(A_j) \\quad \\text{两两独立} \\\\ P(A_i A_j A_k) \u0026amp;= P(A_i) P(A_j) P(A_k) \\quad \\text{三三独立} \\\\ \u0026amp;\\quad \\vdots \\\\ P(A_i A_j","title":"关于概率论中的独立"},{"content":"本文使用腾讯 ima.copilt 来搭建知识库\n背景 在准备复试时从小红书、闲鱼等各种渠道获取到很多复试资料。但是文件繁多，部分资料还存在重复问题，不想花太多时间在搜索信息上面，想到前几天在小红书看到很多人推荐用腾讯的 ima.copilt 来搭建知识库。于是灵机一动，便萌生了通过搭建知识库以提升信息检索效率的法子。\n具体操作步骤 首先进入 https://ima.qq.com/ 下载并安装软件客户端； 登录，进入知识库界面； 上传文件，等待系统自动解析； 通过 RAG 技术，ima 会根据你所上传的文件（即你搭建的知识库）来回答你问的问题。 🙏 希望能够顺利进入复试\n","permalink":"https://20250303.xyz/posts/ai%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E4%BD%BF%E7%94%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%8F%90%E5%8D%87%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E5%87%86%E5%A4%87%E6%95%88%E7%8E%87/","summary":"本文使用腾讯 ima.copilt 来搭建知识库 背景 在准备复试时从小红书、闲鱼等各种渠道获取到很多复试资料。但是文件繁多，部分资料还存在重复问题，不想花太多时间在搜索信息上面，想到前几天在小红书看到很多人推荐用腾讯的 ima.copilt 来搭建知识库。于是灵机一动，便萌生了通过搭建知识库以提升信息检索效率的法子。 具体操作","title":"使用知识库提升考研复试准备效率"},{"content":"本人通过 Obsidian + Hugo + Github Page + Github Action 来实现文章管理及博客自动化搭建.\nHugo 渲染白名单 因为不想将 Obsidian 仓库中的一些隐私笔记发布到博客上, 于是选择使用 module.mounts 来配置允许 Hugo 渲染的文件夹.\n1module: 2 mounts: 3 - source: \u0026#34;content/posts/3. Resources-资源 未来/AI人工智能\u0026#34; 4 target: \u0026#34;content/posts/AI人工智能\u0026#34; 5 - source: \u0026#34;content/posts/4. Archives-归档/技术类归档\u0026#34; 6 target: \u0026#34;content/posts/技术类归档\u0026#34; 7\t...... 8 - source: \u0026#34;content/about.md\u0026#34; 9 target: \u0026#34;content/about.md\u0026#34; 10\t...... Hugo 主页展示白名单 因为主要想在博客主页展示技术类相关博文, 所以需要对主页展示内容进行过滤, 在 layouts/_default/list.html 进行配置即可, 具体可参考下面的代码:\n1{{- if .IsHome }} 2{{- $pages = where site.RegularPages \u0026#34;File.Dir\u0026#34; \u0026#34;in\u0026#34; (slice \u0026#34;posts\\\\AI人工智能\\\\\u0026#34; \u0026#34;posts\\\\技术类归档\\\\\u0026#34;) }} 3{{- $pages = where $pages \u0026#34;Params.hiddenInHomeList\u0026#34; \u0026#34;!=\u0026#34; \u0026#34;true\u0026#34; }} 4{{- end }} 倘若不知道文件夹(File. Dir)路径, 尤其是我这种有 mounts 映射的情况, 可以通过在 layouts/_default/list.html 中添加 {{ range site.RegularPages }} \u0026lt;p\u0026gt;{{ .File.Path }} → {{ .File.Dir }}\u0026lt;/p\u0026gt; {{ end }} 代码块, Hugo 会输出文件夹 (File. Dir) 路径 (类似 Python 的 print 函数.), 见下图:\n踩坑记录: 一开始以为在 config.yml 里面设置 mainSections 相关参数即可, 折腾半天. 后面静下心来慢慢读 Hugo 的官网网站上的相关教程, 最后也多亏了 GPT 帮忙.\n上传 Github 白名单 可通过 .gitignore 文件进行设置.\n1 2content/posts/.obsidian/ # .obsidian文件夹内里面有很多杂七杂八的东西 3content/posts/-1. Books/ # 这一目录用于存放电子书, 占用空间 4 5.history/ # VSC的自动保存 6 7public # 无需上传public文件, 因为设置了Github Action自动部署 这个网上很多大佬都有教程 ","permalink":"https://20250303.xyz/posts/%E6%8A%80%E6%9C%AF%E7%B1%BB%E5%BD%92%E6%A1%A3/hugo-%E6%B8%B2%E6%9F%93%E5%8F%8A%E4%B8%BB%E9%A1%B5%E5%B1%95%E7%A4%BA%E7%99%BD%E5%90%8D%E5%8D%95/","summary":"本人通过 Obsidian + Hugo + Github Page + Github Action 来实现文章管理及博客自动化搭建. Hugo 渲染白名单 因为不想将 Obsidian 仓库中的一些隐私笔记发布到博客上, 于是选择使用 module.mounts 来配置允许 Hugo 渲染的文件夹. 1module: 2 mounts: 3 - source: \u0026#34;content/posts/3. Resources-资源 未来/AI人工智能\u0026#34; 4 target: \u0026#34;content/posts/AI人工智能\u0026#","title":"Hugo 渲染及主页展示白名单"},{"content":"参考: # hexo之mongodb修改twikoo评论的管理密码 以及 # Twikoo 找回暗号及密码\n核心目标: 把 MongoDB 中的\u0026quot;ADMIN_PASS\u0026quot;值删除.\n","permalink":"https://20250303.xyz/posts/%E6%8A%80%E6%9C%AF%E7%B1%BB%E5%BD%92%E6%A1%A3/%E5%BF%98%E8%AE%B0-twikoo-%E7%AE%A1%E7%90%86%E9%9D%A2%E6%9D%BF%E7%9A%84%E5%AF%86%E7%A0%81%E6%80%8E%E4%B9%88%E5%8A%9E/","summary":"参考: # hexo之mongodb修改twikoo评论的管理密码 以及 # Twikoo 找回暗号及密码 核心目标: 把 MongoDB 中的\u0026quot;ADMIN_PASS\u0026quot;值删除.","title":"忘记 Twikoo 管理面板的密码怎么办"},{"content":"记得在看《被讨厌的勇气》后，里面有一句话让我印象特别深刻：“影响你的不是事件本身，而是你==看待事件的方式==。”比如精神内耗与否，就与==如何看待事情==息息相关[[关于精神内耗#内耗和复盘]]。再比如经验不是事情本身, 是我们理解事情的方式, 构成了经验。但是需要注意的是，==不要脱离或弱化了“物质的决定性”==。\n","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E4%B8%BB%E8%A7%82%E8%83%BD%E5%8A%A8%E6%80%A7/","summary":"记得在看《被讨厌的勇气》后，里面有一句话让我印象特别深刻：“影响你的不是事件本身，而是你==看待事件的方式==。”比如精神内耗与否，就与==如何看待事情==息息相关[[关于精神内耗#内耗和复盘]]。再比如经验不是事情本身, 是我们理解事情的方式, 构成了经验。但是需要注意的是，==不","title":"关于主观能动性"},{"content":" 如果你因为失去了太阳而流泪，那么你也将失去群星。 - 泰戈尔\n什么是精神内耗 精神内耗表现为个体对未知不确定事件可能的消极原因或后果进行反复的揣摩、思考与分析。 - 百度百科\n心理内耗是指人的自我控制需要消耗心理资源，当资源不足时，个体即处于一种所谓内耗的状态，长期的内耗会让人感到疲惫。 - MBA智库\n内耗和复盘 精神内耗 Vs 真正的复盘，他们之间区别在于：==精神内耗==是过度的无用思考，不断地==反刍==：“我当时为什么要这样做”；==真正的复盘==，虽然也会分析当时做错事的原因，但其重点是：以后==怎么做==以避免再犯类似的错误。 ^ah26bl\n如何应对精神内耗 不要试图去压制自己的反刍思维，这样反而可能会因为白熊效应让负面思考变得更加频繁，这可能会形成恶性循环。\n白熊效应，又称白象效应或反弹效应：刻意抑制某些想法时，实际上会使这些想法更容易浮出水面。一个例子是，当某人积极地试图不去想一只白熊时，他实际上更有可能想象一只白熊。 - Wikipedia\n具体怎么做：\n自我接纳（只要你完成了今天的目标，你就问心无愧。其它的事，无需今日承担。 - 小红书评论） 旁观视角（分清情绪和问题） 积极行动（行动是治愈反刍的良药） 有意识地转移注意力（如正念冥想、心理干预）等等 ","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E7%B2%BE%E7%A5%9E%E5%86%85%E8%80%97/","summary":"如果你因为失去了太阳而流泪，那么你也将失去群星。 - 泰戈尔 什么是精神内耗 精神内耗表现为个体对未知不确定事件可能的消极原因或后果进行反复的揣摩、思考与分析。 - 百度百科 心理内耗是指人的自我控制需要消耗心理资源，当资源不足时，个体即处于一种所谓内耗的状态，长期的内耗会让人感到疲惫。 - MB","title":"关于精神内耗"},{"content":"\u0026ldquo;矛盾\u0026quot;的意思即\u0026quot;两面性\u0026rdquo;(道家的\u0026quot;阴阳\u0026quot;)?\n思考的两面性 - 人类认知边界 × 语言符号系统 思考的正面性，或者通俗来说，思考的优点，是显而易见的。\n下面主要来说说==思考的反面性==：\n语言的界限就是世界的界限。 - 《维特根斯坦与哲学》 知乎的一个解读 没有语言，人类就无法思考。 - 索绪尔 知乎对索绪尔或语言学的一个介绍 道可道，非常（恒）道。 - 老子《道德经》\n结构主义认为语言的结构决定了我们的思维模式。\nearly-nineteenth-century science was in the grip of philosophical determinism—the belief that everything that happens is determined in advance by the initial conditions of the universe and the mathematical formulas that describe its motions. - The lady tasting tea. 也是结构主义？\n是的，但传统结构主义主要应用于语言学、人类学、文化研究等，主要强调符号和意义的系统性。\n深入了解一下结构主义, 索绪尔, 列维斯特劳斯 #哲学 我们只能想到我们能够想到的东西（意象无法表达）, 或者说，我们只能表达出语言能够表达出地东西，只能思考『能够被语言表达』的东西。语言之外的东西，我们无法思考。就好比盲人无法思考光明、聋人无法思考声音、人类无法思考高维空间（“上帝面前，我们都是聋子”）\n人类在认知进程中不断重构\u0026quot;可言说\u0026quot;与\u0026quot;不可言说\u0026quot;的边界形态，例如随着阅历或认知的进长，我们渐渐能够表达从前无法用语言表达的想法或观点，但是，这个边界本身是永远不能被消除的。\n《庄子·内篇·养生主第三》中的哲思：\u0026ldquo;吾生也有涯，而知也无涯。以有涯随无涯，殆已！\u0026quot;，人类的伟大恰在于明知\u0026quot;殆\u0026quot;而仍不懈追寻。\nDDL 的双面性 设计应该是感性的还是理性的？设计是感性重要还是理性重要？ 此类问题一经出现便会引起争议无数，每个人对此都有自己的见解与理由，还有些人主张“理性与感性需要平衡”“既要理性也要感性”，==此类观点看似正确，却缺乏任何实质性的指导价值==。 - 设计的两面性：理性决策与感性表达\n马哲所谓的\u0026quot;辩证分析法\u0026rdquo;, 听上去似乎正确, 但实际上, 就像上面引用的文章所提到的: 缺乏实质性的指导价值.\n","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E9%A9%AC%E5%93%B2%E4%B8%AD%E7%9F%9B%E7%9B%BE%E4%B8%8E%E9%98%B4%E9%98%B3/","summary":"\u0026ldquo;矛盾\u0026quot;的意思即\u0026quot;两面性\u0026rdquo;(道家的\u0026quot;阴阳\u0026quot;)? 思考的两面性 - 人类认知边界 × 语言符号系统 思考的正面性，或者通俗来说，思考的优点，是显而易见的。 下面主要来说说==思考的反面性==： 语言的界限就是世界的界限。 - 《维特根斯坦","title":"关于马哲中矛盾与阴阳"},{"content":"人会借势是强者的表现\n思虑过多的人, 应该去多感受大千世界.\n一件事情, 明知道没有希望, 却在尘埃落定之前, 一直抱有希望. 就像宣判死刑前心怀侥幸的十恶不赦的罪犯.\n世上有那么多人，所以总会有知己吧。世上有那么多人，所以很难会遇到知己吧。\n","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B6%E5%AE%83/","summary":"人会借势是强者的表现 思虑过多的人, 应该去多感受大千世界. 一件事情, 明知道没有希望, 却在尘埃落定之前, 一直抱有希望. 就像宣判死刑前心怀侥幸的十恶不赦的罪犯. 世上有那么多人，所以总会有知己吧。世上有那么多人，所以很难会遇到知己吧。","title":"其它"},{"content":"《湖南农民运动考察报告》 矫枉必须过正, 不过正不能矫枉.\n政权、族权、神权、夫权这四种权利代表了全部封建宗法的思想与制度.\n引而不发，跃如也。 - 农民的使命要农民去完成, 别人代庖是不对的\n《井冈山的斗争》 民主主义: 官长不打士兵, 官兵待遇平等, 士兵有开会说话的自由, 废除繁琐的礼节, 经济公开.\n家族主义: 一个村子一个姓, 支部会议简直同时就是家族会议.\n群众的政治训练 - 群众需要有政治素养\n党与政府的关系\n民权主义革命 - 民权主义是三民主义之一.\n《关于纠正党内的错误思想》 ==本位主义==, 一切只知道为四军打仗, 不知道武装地方群众是红军的重要任务之一. 这是一种放大了的小团体主义.\n俘虏兵的加入, 带来了浓厚的雇佣军队的思想, 使单纯军事观点有了==下层基础==.\n过分相信军事力量, 而不相信人民群众的力量.\n党对军事工作没有积极的注意和讨论, 也是形成一部分同志的单纯军事观点的原因.\n==关于极端民主化==, 『由下而上的民主集权制』, 『先交下级讨论, 再交上级决议』?\n有争论的问题, 要把是非弄明白, 不要调和敷衍.\n党内批评, 不要成了攻击个人.\n绝对平均主义的来源, 和政治上的极端民主化一样, 是手工业者和小农经济的产物, 不过一则见之于政治方面, 一则见之于物质生活方面罢了.\n看这篇文章有看加缪的《堕落》的感觉，被作者当面揭露出劣根性的感觉。原来自己曾经有过的许多想法，其实都有一个专有名词（如 xx 主义），一下抓住事情的本质并提炼成文字，伟人果然是伟人。\n","permalink":"https://20250303.xyz/posts/%E9%98%85%E8%AF%BB/%E6%AF%9B%E9%80%89%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","summary":"《湖南农民运动考察报告》 矫枉必须过正, 不过正不能矫枉. 政权、族权、神权、夫权这四种权利代表了全部封建宗法的思想与制度. 引而不发，跃如也。 - 农民的使命要农民去完成, 别人代庖是不对的 《井冈山的斗争》 民主主义: 官长不打士兵, 官兵待遇平等, 士兵有开会说话的自由, 废除繁琐的礼节, 经济公开.","title":"毛选读书笔记"},{"content":"本地部署都很吃配置\n知识库 KB 知识库（Knowledge base）是用于知识管理的一种==特殊的数据库==，以便于有关领域知识的采集、整理以及提取。知识库中的知识源于领域专家，它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。 - Wikipedia\n例如：法院判例库、医院病例库、产品数据库、某领域论文库、哈利波特魔咒库、章鱼哥的食谱……\n注意！知识库与 LLM 预训练语料库两者不是一个概念，预训练语料库为非结构化文本（乱七八糟的文字, 未清洗）, 且一旦模型预训练完成就不能再更改预训练语料 (如ChatGPT 4o的预训练语料库便截止2023年10月); 而知识库则为结构化或半结构化文本 (有一级标题、二级标题或其他逻辑结构)，且是定期更新与维护的。\n看到很多人推荐 IMA 来搭建知识库.\n智能体 Agent 智能体（英语：intelligent agent）指一个可以==观察周遭环境并作出行动以达致目标==的自主实体。它通常是指（但不一定是）一个软件程序。“智能体”是目前人工智能研究的一个核心概念，统御和联系着各个子领域的研究。 - Wikipedia\n智能体（Agent）是指能够==感知环境并采取行动以实现特定目标==的代理体。它可以是软件、硬件或一个系统，具备自主性、适应性和交互能力。智能体通过感知环境中的变化（如通过传感器或数据输入），根据自身学习到的知识和算法进行判断和决策，进而执行动作以影响环境或达到预定的目标。 - 百度百科\n常见 AI Agent 平台: 字节扣子 (Coze)、腾讯元器、Dify、FastGPT\n所有妄图通过缩短工序而提升效率的低代码平台都会死. (Agent 平台就是 LLM 时代的低代码平台) - 小红书评论\n检索增强生成 RAG Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so ==it references an authoritative knowledge base outside of its training data sources before generating a response.== Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. ==RAG extends the already powerful capabilities of LLMs to specific domains or an organization\u0026rsquo;s internal knowledge base, all without the need to retrain the model.== It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. - AWS\n==检索增强生成==（英语：Retrieval-augmented generation, RAG ) 是赋予生成式人工智能模型信息检索能力的技术。检索增强生成优化大型语言模型(LLM) 的交互方式，让模型根据指定的一组文件回应用户的查询，并使用这些信息增强模型从自身庞大的静态训练数据中提取的信息。检索增强生成技术促使大型语言模型能够使用特定领域或更新后的信息。 - Wikipedia\n可以将RAG理解为\u0026quot;知识库+LLM\u0026quot;, \u0026ldquo;==AI 老员工==\u0026ldquo;的比喻十分形象。\n主流 RAG 框架: LangChain、RAGFlow、AnythingLLM\nRAG 具体工作逻辑 参考 Why does the LLM not use my documents?\nRAG 对知识库中的文件，并不是以一个文档一个文档地对待的，而是将所有文档都“打碎”并进行词嵌入； 当用户发起对话时，也将用户发送的对话内容“打碎”并进行词嵌入； 通过余弦相似度从知识库中寻找与用户发送内容“相似”的“碎片”。 整合 LLM，最后输出 RAG (which is what we use) enables us to chunk the document and then ask retrieve only the bits and pieces the make sense for your question and use that in the context window. This makes larger documents easier to use, but it is at the expense of these types of \u0026ldquo;whole document\u0026rdquo; understandings.- AnythingLLM 社区 RAG, by its very nature is pieces of relevant content. Not the entire text - AnythingLLM 社区\n总结：RAG 的作用不是全文理解，而是从知识库找到与我们所问的问题相关的“碎片”。至于全文理解，这件事是智能体干的事情，当然 LLM 也干得不错。为什么不本地部署一个强悍的 LLM ? 可以参考这里[[#总结]].\n微调 Fine-tuning 微调（又称大模型微调，英语：fine-tuning）是==深度学习中迁移学习的一种方法==，其中预训练模型的权重会在新数据上进行训练。微调可以在整个神经网络上执行，也可以仅在其部分层上执行，此时未进行微调的层会被“冻结”（在反向传播步骤中不更新） - Wikipedia 微调通常通过==监督学习==完成，但也有使用弱监督进行模型微调的技术。 - Wikipedia Low-rank adaptation (==LoRA==) is an adapter-based technique for efficiently fine-tuning models. - Wikipedia\nLLM 总不能样样精通.\n可以理解为：通过利用已有的知识来提高模型在新任务上的表现。\n任何预训练好的模型都能微调！预训练-微调方法属于基于模型的迁移方法（Parameter/Model-based TransferLearning）\n总结 角色 作用 依赖 知识库 ==专业知识库==，提供事实性、权威性的知识数据 被RAG或智能体查询 智能体 ==Agent==，负责与用户交互，并调用RAG或知识库来回答问题 可能使用RAG提升回答能力 RAG ==一种AI架构==，从知识库检索信息，并结合语言模型生成答案 需要知识库作为数据来源 微调 ==迁移学习的一种方法==，使通用模型适配特定领域任务 需要知识库作为微调数据 形象的比喻：微调是考前复习，RAG 是开卷考试。\n然而本地部署并微调一个大模型，对于个人来说成本是极大的；如果退而求其次，微调一个小一点的模型，效果又可能不好；所以实践证实:==在本地部署一个蒸馏版的大模型+RAG 技术是一个更优的解决方案==。\n","permalink":"https://20250303.xyz/posts/ai%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%99%BA%E8%83%BD%E4%BD%93rag%E5%BE%AE%E8%B0%83/","summary":"本地部署都很吃配置 知识库 KB 知识库（Knowledge base）是用于知识管理的一种==特殊的数据库==，以便于有关领域知识的采集、整理以及提取。知识库中的知识源于领域专家，它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。 - Wikipedia 例如：法院判例库、医院病例库、产品数据","title":"知识库、智能体、RAG、微调"},{"content":"✨ 关于我 ✨\n本科统计学在读，对数据科学感兴趣。兴趣广泛，涉猎天文、业余天文摄影、音乐\n目前正在准备考研复试 / 春招\n📚 这里有什么 📚\n▫️ 数据科学相关：统计学、机器学习的学习笔记\n▫️ 人工智能相关：记录捣鼓 AI 过程中的踩坑经历以及灵感\n▫️ 一些阅读笔记：偶尔写些读书记录\n欢迎同好一块交流 🤝\n","permalink":"https://20250303.xyz/about/","summary":"✨ 关于我 ✨ 本科统计学在读，对数据科学感兴趣。兴趣广泛，涉猎天文、业余天文摄影、音乐 目前正在准备考研复试 / 春招 📚 这里有什么 📚 ▫️ 数据科学相关：统计学、机器学习的学习笔记 ▫️ 人工智能相关：记录捣鼓 AI 过程中的踩坑经历以及灵感 ▫️ 一些阅读笔记：偶尔写些读书记录 欢迎同好一块交流 🤝","title":"关于我"}]