[{"content":"概率质量函数 pmf，用于离散型随机变量：非负性、正则性。pmf 的函数值不能大于 1\n概率密度函数 pdf，用于连续型随机变量：非负性、正则性。pdf 函数值可能大于 1（用密度去理解）\n⚠️存在既非离散又非连续型的随机变量！\n概率分布函数 cdf，单调性（单调非减）、有界性、右连续性、非负性\n连续型 r.v. 的分布函数必是连续函数, 证明；但其 pdf 不一定连续，可能有跳跃间断点。\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/pdf-pmf-cdf/","summary":"概率质量函数 pmf，用于离散型随机变量：非负性、正则性。pmf 的函数值不能大于 1 概率密度函数 pdf，用于连续型随机变量：非负性、正则性。pdf 函数值可能大于 1（用密度去理解） ⚠️存在既非离散又非连续型的随机变量！ 概率分布函数 cdf，单调性（单调非减）、有界性、右连续性、非负性 连续","title":"pdf pmf cdf"},{"content":" 德国学者 Gauss (1777‐1855)于 1809 年提出最⼩⼆乘法。 英国遗传学家 Galton (1822‐1911)于 1886 年发表关于回归的开⼭论⽂ 《遗传结构中向中⼼的回归》Regression towardsMediocrity in heredity structure)。[[统计学史#^tp89te]] 回归分析是处理变量之间的==相关关系==的⼀种统计⽅法和技术。\n相关分析：⽤⼀个指标 (相关系数) 来表明现象间相互依存关系的密切程度。以现象之间是否相关、相关的⽅向和密切程度等为研究内容，不区分⾃变量和因变量，不关⼼相关关系的表现形态。 回归分析：对具有相关关系的现象，根据其相关关系的具体形态，选择⼀个合适的数学模型 (回归⽅程) 来近似地表达变量间的平均变化关系。 相关分析是回归分析的基础和前提；回归分析是相关分析的深⼊和继续。 ⼴义的相关分析包括回归分析。 模型假设 样本回归模型 $y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i$. 理论回归模型 $y=\\beta_0+\\beta_1x+\\varepsilon$. 经验回归方程 $\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1x$. 回归模型 $E(y|x)=\\beta_0+\\beta_1x$ 从平均意义上表达了变量 $y$ 与 $x$ 之间的统计规律性 如何理解回归模型中的条件期望？在给定解释变量 $X$ 取特定值 $x$ 时，被解释变量 $Y$ 的平均值。 ⚠️G-M 假设不要求扰动项服从正态分布！\n正态性假设: $\\varepsilon_i \\sim N(0, \\sigma^2)$, 且 $\\varepsilon_i$ 间相互独立。\n正态假定下 MLE 与 OLSE 等价，MLE最大化似然函数，OLSE最小化损失函数。\n⼀元线性回归中⾃变量和残差的关系：在⼀元线性回归模型中， ⾃变量 (X) 和残差 (ε) 理论上应该是独⽴的。残差代表了实际观测值与通过回归线预测值之间的差异。如果⾃变量和残差不独⽴，可能表明模型中存在遗漏变量或⾮线性关系没有被正确模型化。\n违背基本假设的几种情况 1 多重共线性\n定义：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。\n问题：$\\beta$ 方差 $\\sigma^2(X\u0026rsquo;X)^{-1}$ 偏大，估计精度低，不稳定，t 统计量↓，不易拒绝 $H_0$.\n诊断：方差扩大因子 $VIF_j=\\frac{1}{1-R_j^2}≥10$、条件数 $=\\frac{\\lambda_{max}}{\\lambda_{min}}≥100$、直接看特征根（是否接近 0）\n解决⽅法：可以通过以下方法来解决：\n变量选择； 使⽤主成分回归（PCR），$X$ 中有多重共线性的变量，那就不使用 $X$ 来进行回归分析，转而==使用 $X$ 对应的主成分矩阵==。 偏最小二乘法：相较于通过寻找响应和独立变量之间最小方差的超平面，偏最小二乘法通过==投影预测变量和观测变量到一个新空间==来寻找一个线性回归模型。 岭回归（$X\u0026rsquo;X$ 不可逆？那就==加上一个正则项== $X\u0026rsquo;X+kI$ 这样就可逆了），代价是：有偏。通过岭迹图来确定 $k$ 值 岭迹图图片。 Lasso 回归。详见下图： 图源博客园博文 注意：不是说一个模型有多重共线性情况，那这个模型就完全不行。如果说是利用模型去做经济结构分析，那么要尽可能避免多重共线性；==而如果说只是利用模型去做经济预测，只要保证自变量的相关类型在未来时期中保持不变，即使回归模型中含有严重多重共线性的变量，也可以得到较好的预测结果。==\n2 异⽅差性 问题：虽仍是无偏估计，但不是 MVUE；显著性检验失效，回归方程应用效果不理想。 诊断：绘制残差图、斯皮尔曼检验 解决⽅法：可以使⽤加权最⼩⼆乘法（WLS）或对变量进⾏变换（BOX-COX）。\n3 ⾃相关\n产生原因：遗漏关键变量、经济变量的滞后性、回归函数的错误、蛛网现象、不恰当的数据预处理方式（如差分变换） 问题：t 值↑，易犯拒真错误；估计值不再是 MVUE；MES 严重↓ 诊断：绘制 $e_i, e_{i-1}$ 散点图、自相关系数绝对值接近 1、DW 值离 2 的距离越远自相关越严重。 解决⽅法：迭代法、差分法、使⽤⼴义最⼩⼆乘法（GLS）、时间序列模型（如 ARIMA）或对数据进行BOX-COX 变换。 4 ⾮线性关系 问题：如果因变量与⾃变量之间的关系是⾮线性的，线性回归模型⽆法准确描述这种关系。 解决⽅法：可以使⽤⾮线性回归模型，或者对变量进⾏变换（如对数变换、多项式回归）。\n5 数据量不⾜ 问题：如果样本量太少，回归模型的参数估计可能不准确，模型容易过拟合。 解决⽅法：增加数据量，或者使⽤正则化⽅法（如 Lasso 回归）来防⽌过拟合。\n6 忽略重要变量 问题：如果模型中遗漏了重要的⾃变量，会导致回归系数估计有偏。 解决⽅法：通过领域知识或变量选择⽅法（如逐步回归）来识别重要变量。\n7 异常点、高杠杆点与强影响点\n注意：异常点不一定是强影响点，强影响点也不一定是异常点、高杠杆点不一定是强影响点，强影响点也不一定是高杠杆点 问题：异常值会对回归模型的拟合产⽣较⼤影响，导致模型失真。 解决⽅法：可以通过剔除异常值或使⽤稳健回归⽅法。 诊断： 参考 判断异常点，异常值点：对既定模型偏离很⼤的数据点 标准化残差 $ZRE_i=\\frac{e_i}{\\hat{\\sigma}}$ 绝对值＞3 学生化残差 $SRE=\\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}$ 绝对值＞3, 相较于标准化残差，学生化残差剔除了高杠杆值的影响； 判断高杠杆点：杠杆值 $h_{ii}=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{\\sum_{j = 0}^{n} (x_j-\\bar{x})^2}$, ==高杠杆点≠强影响点≠异常点==，在⾃变量空间中远离数据中⼼的点，有==把拟合直线拖向⾃⼰==的倾向，称之为⾼杠杆点 判断强影响点：库克距离 $D_i=\\frac{\\sum_{j = 0}^{n} (y_j-y_{j(i)})^2}{p ·MSE}$ ＞0.5, 度量去除某一数据点后其它样本拟合值的变化。强影响点是对统计推断产⽣较⼤影响的数据点，删除该点会导致拟合模型的实质性变化，如参数估计值、拟合值或检验值发⽣较⼤变化 变量选择中评价模型的指标 自由度调整复决定系数越大越好； AIC 和 BIC：这两个准则都是基于似然函数，考虑了模型复杂度的惩罚。在⽐较多个模型时，AIC 和BIC 较⼩的模型被认为更优； $C_p$ 统计量越小越好； 均⽅误差：计算模型预测值与实际值差值的平⽅的平均值。MSE 越⼩，模型的预测准确度越⾼ 。 均⽅根误差：MSE 的平⽅根。RMSE 是衡量模型预测误差的常⽤标准，越⼩表示模型预测越准确。 平均绝对误差：计算模型预测值与实际值差值的绝对值的平均值。MAE 提供了预测误差的另⼀种衡量，对异常值的敏感度低于 MSE。 决定系数：衡量模型预测值的变异性占总变异性的⽐例，值范围从 0 到 1。$R^2$ 值越接近 1 ，表示模型解释的变异性越⼤，拟合度越好。2 调整 $R^2$：对 $R^2$ 进⾏调整，考虑了模型中变量的数量。在模型中添加更多变量时，即使这些变量对模型的贡献很⼩，$R^2$ 也可能会增加。调整后的 $R^2$ 提供了⼀个更为准确的衡量标准。 残差图分析：通过观察残差（实际值与预测值之差）的分布情况，可以评估模型是否满⾜线性回归的假设，例如残差的独⽴性、正态性和⽅差⻬性。 选择合适的评估⽅法取决于具体的研究⽬的和数据特性。通常，结合使⽤多种评估指标可以更全⾯地了解回归模型的性能。\n残差 误差等概念辨析 后续再更新\u0026hellip;\n残差中的 QQ 图，对残差的正态性检验？\n区间估计 \u0026amp;区间预测 在回归分析中，如何处理⽆序变量？ 处理无序变量，核心思想就是“编码”，参考 解决（几乎）任何机器学习问题：处理分类变量篇（上篇）\n设置哑变量。哑变量：又称为虚拟变量、虚设变量或名义变量。对于有n个分类属性的自变量，通常需要选取1个分类作为参照，因此可以产生n-1个哑变量。==哑变量即 One-Hot Encoding!==\nOne-Hot Encoding：这是最基础也是最常见的转换手段之一。它会给每个类别创建一个新的二进制列，并标记是否存在该值(0 或者 1)。然而这种方法可能会导致维度爆炸的问题，在类别过多的情况下生成大量的新特性。 下面的其它 Encoding 方法先浅浅涉猎，在归纳机器学习的时候再深入了解。\nBinary Encoding：为了解决 one-hot 编码带来的高维问题，可以采用 binary encoding 技术。这种算法首先对所有唯一标签按顺序编号，然后将其转化为二进制数表示。相比于onehot,binary能有效减少新增加的feature数量. BaseN Encoding: 类似于binary但是更通用的形式，它可以基于任意基数(N)，而不仅仅是2来进行编码。 Target Encoding / Mean Encoding : 这种方法通过计算目标变量在每一类下的平均值来代替原来的类别信息。对于回归任务而言就是连续数值均值；如果是分类任务，则可以用概率分布替代。但需要注意的是这种方式容易造成过拟合所以一般配合正则化项一起使用。 Feature Hashing (Hash Trick) ：这个技巧能够把无限大小的空间映射到固定长度向量上。hash function将会接收字符串输入并返回一个整数索引用于填充vector相应位置。 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/","summary":"德国学者 Gauss (1777‐1855)于 1809 年提出最⼩⼆乘法。 英国遗传学家 Galton (1822‐1911)于 1886 年发表关于回归的开⼭论⽂ 《遗传结构中向中⼼的回归》Regression towardsMediocrity in heredity structure)。[[统计学史#^tp89te]] 回归分析是处理变量之间的==相关关系==的⼀种统计⽅法和","title":"回归分析"},{"content":"茎叶图 优点：\n直观展示数据分布：茎叶图通过将数据分成“茎”和“叶”两部分，能够直观地展示数据的分布情况。 保留原始数据：与直⽅图不同，茎叶图保留了原始数据的详细信息，每个数据点都可以从图中直接读取。 简单易绘制：茎叶图不需要复杂的计算或⼯具，⼿⼯即可快速绘制。 适合⼩数据集：茎叶图特别适合展示⼩数据集，能够清晰地显示每个数据点的位置。 缺点：\n茎叶图只便于表示两位有效数字的数据； 茎叶图只方便记录两组的数据； 茎叶图不适用于大数据集。 作⽤：\n数据探索：在数据分析的初期，茎叶图可以帮助快速了解数据的分布和集中趋势。 异常值检测：通过观察茎叶图，可以很容易地发现数据中的异常值或离群点。 数据⽐较：可以绘制多个茎叶图来⽐较不同数据集之间的分布情况。 直方图 优点：\n直观展示数据分布 适用于大规模数据集 可以通过多个直方图对比不同数据集的分布差异。 缺点：\n丢失数据原始信息 绘制复杂 作用：\n数据探索：直观感受的集中趋势、离散程度、偏态和峰态；发现数据的模式，比如是否呈正态分布、偏态分布或多峰分布。 识别异常值，发现数据中的极端情况。 箱线图 优点：\n方便多组数据比较； 识别异常值； 使用数据量大的情形 缺点：\n丢失数据原始信息 对数据量较少的情况不够直观 不适用小数据情形 作用：\n直观展示数据的分布特征（如中位数、四分位数、离散程度）。 识别异常值，帮助发现数据中的极端值。 便于对比多个数据集的分布差异，尤其适用于数据间的偏态比较。 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/","summary":"茎叶图 优点： 直观展示数据分布：茎叶图通过将数据分成“茎”和“叶”两部分，能够直观地展示数据的分布情况。 保留原始数据：与直⽅图不同，茎叶图保留了原始数据的详细信息，每个数据点都可以从图中直接读取。 简单易绘制：茎叶图不需要复杂的计算或⼯具，⼿⼯即可快速绘制。 适合⼩数据集：茎叶图特别适","title":"数据可视化"},{"content":" ——转自 《一张图说明二项分布、泊松分布、指数分布、几何分布、负二项分布、伽玛分布的联系》\n二项分布的近似 n 大 p 不是很小, $np \\rightarrow \\infty$, 实际中常用 $np\u0026gt;5, nq\u0026gt;5$ 来作为使用正态分布来近似二项分布的条件（棣莫弗-拉普拉斯中心极限定理）\nn 大 p 很小且 $np \\rightarrow \\lambda$ 时可用泊松分布近似\n泊松分布是在不知道事件的可能发生总次数的情况下对小概率事件建模，又叫泊松小数法则。是一个计数过程。$\\lambda\u0026gt;10$ 时也可用正态分布近似泊松分布（不同教材对 $\\lambda$ 大小要求不同）\n根据CLT，在大样本且满足独立同分布、总体方差有限的条件下，==样本均值（或样本和）==的分布近似服从正态分布，无论原总体分布如何。\n泊松分布与指数分布 这两个分布的参数 $\\lambda$ 的含义是同样的，用于衡量事件发生的频率（单位时间发生 $\\lambda$ 次）\n几何分布与超几何分布 几何分布，对应几何级数\n超几何分布，对应超几何级数\n详见 知乎问答\n分布的可加性 独立可加性定义：相互独立的 xx 分布加完之后还是 xx 分布。\n具有可加性的分布：\n泊松分布，设 $X_1 \\sim Poisson(\\lambda_1), X_2 \\sim Poisson(\\lambda_2)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim Poisson(\\lambda_1+\\lambda_2)$. 伽马分布，设 $X_1 \\sim \\Gamma(\\alpha_1, \\lambda), X_2 \\sim \\Gamma(\\alpha_2, \\lambda)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim \\Gamma(\\alpha_1+\\alpha_2, \\lambda)$. 二项分布，设 $X_1 \\sim b(n, p), X_2 \\sim b(m, p)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim b(n+m, p)$. 正态分布，设 $X \\sim N(\\mu_1, \\sigma_1^2), Y \\sim N(\\mu_y, \\sigma_y^2)$, $X$ 和 $Y$ 相互独立，则 $aX + bY \\sim N\\left(a\\mu_1 + b\\mu_2, a^2\\sigma_1^2 + b^2\\sigma_2^2\\right)$. 负二项分布（r 取整数时为帕斯卡分布），设 $X_1 \\sim Nb(r_1, p), X_2 \\sim Nb(r_2, p)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim Nb(r_1+r_2, p)$. 卡方分布，设 $X_1 \\sim \\chi^2(n), X_2 \\sim \\chi^2(m)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim \\chi^2(n+m)$. 柯西分布。 不具有可加性的分布：\n0-1分布，参数 $p$ 相同的 $0-1$ 分布相加为二项分布，不是 $0-1$ 分布。 几何分布 $Ge(p)$, “常在河边站哪有不湿鞋”，指事件首次发生时所进行的试验次数。参数 $p$ 相同的几何分布相加为负二项分布，不是几何分布。 均匀分布。 指数分布，$Exp(\\lambda) \\sim Ga(1, \\lambda)$, 参数 $\\lambda$ 相同的指数分布相加为伽马分布，不是指数分布。 贝塔分布。 超几何分布。 神通广大的伽马和贝塔分布 （函数）： 伽马分布：\n伽马分布不仅可以与指数分布关联，也可与卡方分布相关联: $Ga(\\frac{n}{2}, \\frac{1}{2}) \\sim \\chi^2(n)$. 若 $X\\sim Ga(\\alpha, \\lambda)$, 则当 $k\u0026gt;0$ 时，$kX \\sim Ga(\\alpha, \\frac{\\lambda}{k})$. 伽马函数:\n$n \\in N^+, \\Gamma(n+1)=n\\Gamma(n)=n!$ $\\Gamma(1)=0!=1, \\quad \\Gamma(\\frac{1}{2})=\\sqrt{\\pi}$. 贝塔分布：\n$Be(1, 1)=U(0, 1)$. 贝塔函数（注意不是贝塔分布）:\n$B(a, b)=B(b, a)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$. 分布的无记忆性 $P(X\u0026gt;t+s|X\u0026gt;s)=P(X\u0026gt;t)$. 无记忆是指任何特定情况下的条件概率看起来就像初始的无条件概率。推导见 推文\n指数分布。指数分布通常被称为寿命分布，灯泡 $P(还能用5年|已经用了1年)=P(能用5年)$, 看上去很反常，但其实这里指的是“理想灯泡”，寿命足够长，只会因为其它不可控因素报废。 几何分布。 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%85%B3%E4%BA%8E%E5%88%86%E5%B8%83/","summary":"——转自 《一张图说明二项分布、泊松分布、指数分布、几何分布、负二项分布、伽玛分布的联系》 二项分布的近似 n 大 p 不是很小, $np \\rightarrow \\infty$, 实际中常用 $np\u0026gt;5, nq\u0026gt;5$ 来作为使用正态分布来近似二项分布的条件（棣莫弗-拉普拉斯中心极限定理） n 大 p 很小且 $np \\rightarrow \\lambda$ 时可用泊松分布近似 泊松分布是在不知道事件的可能发生总次","title":"关于分布"},{"content":"抽样方法 不同抽样方法各有千秋。抽样方法的选取，取决于研究目的和对性价比的考量。\n抽样调查可以分为两类，即概率抽样和非概率抽样。==概率抽样==是按照随机原则进行抽样，不加主观因素，组成总体的每个单位都有被抽中的概率（非零概率），可以避免样本出现偏差，样本对总体有很强的代表性。==非概率抽样==是按主观意向进行的抽样（非随机的），组成总体的很大部分单位没有被抽中的机会（零概率），使调查很容易出现倾向性偏差。—— 抽样调查的主要方法\n概率抽样 ==简单随机抽样==，需要抽样框（有关总体全部单位的名录）当样本量N很大时，构造这样的抽样框并不容易。且根据这种方法抽出的个体较为分散，会给后续调查实施增加困难。因此，在规模较大的调查中很少直接采用简单随机抽样，一般是把这种方法和其他抽样方法结合起来使用。 ==分层抽样==（类型抽样），它首先将要研究的总体按某种特征或某种规则划分为不同的层（组），然后按照等比例或最优比例的方式从每一层（组）中独立、随机地抽取个体，最后将各层的样本结合起来对总体的目标量进行估计。优点是抽样误差小，缺点是不适用当总体无法彻底划分为不相交的子组时的场景。 ==整群抽样==（集团抽样），理论上要求群内差异大，群间差异小（每个群抽到哪个都差不多，希望群内差异大，蕴含信息量大）。首先，抽取样本时只需要群的抽样框，而不必要求包括所有单位的抽样框。这大大简化了编制抽样框的工作量；其次，由于群通常是由那些地理位置邻近的或隶属于同一系统的单位所构成，因此调查的地点相对集中，从而节省了调查费用，方便了调查的实施。其主要缺点是估计的精度较差，因为同一群内的单位或多或少有些相似，在样本量相同的条件下整群抽样的抽样误差通常比较大。 ==系统抽样==（等距抽样、机械抽样），它是将总体N个个体按某种顺序排列，按规则确定一个随机起点，再每隔一定间隔逐个抽取样本单位的抽样方法。典型的系统抽样是先从数字1-k之间随机抽取一个数字r作为初始单位，以后依次取r+k，r+2k，…。系统抽样的主要优点是操作简便，如果有辅助信息，对总体内的单位进行有组织的排列，可以有效地提高估计的精度；缺点是对估计量方差的估计比较困难（因为样本单位间的相关性？）。 ==多阶段抽样==，将抽样过程分阶段进行，每个阶段使用的抽样方法往往不同，即将各种抽样方法结合使用。每增加一个抽样阶段，就会增添一份估计误差。 非概率抽样 ==方便抽样==（便利抽样、偶遇抽样），主要用于初期评估的探索性研究。调查过程中由调查员依据方便的原则自行确定抽入样本的个体。 ==判断抽样==，调查者根据主观经验和判断从总体中选取有代表性的个体构成样本的一种非概率抽样方法。它不能获得估计值的精度，其精度取决于抽样者的经验，适用于总体中的个体极不相同而样本容量又很小的情况。 ==自愿抽样==，指被调查者自愿参加，成为样本中的一分子，向调查人员提供有关信息。它可以反映某类群体的一般看法。 ==滚雪球抽样==，先找到最初的样本，然后根据他们提供的信息去获得新的个体形成样本。这种过程不断继续，直到完成规定的样本容量为止。滚雪球抽样常用于对稀少的特定群体的调查。 ==配额抽样==，非概率版的分层抽样。==配额抽样==，非概率版的分层抽样。调查人员将调查总体样本按一定标志分类或分层，确定各类（层）单位的样本数额，在配额内主观地任意抽选样本的抽样方式。 抽样误差 抽样误差，是随机性误差，只存在概率抽样中存在，可以计算并控制！普查的 Sample Error 为 0. 估计总体均值时 $E=z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}$, 估计总体比例时 $E=z_{\\alpha/2}\\sqrt{\\frac{\\pi(1-\\pi)}{n}}$, 移项即可计算出样本量 $n$. ⚠️注意！在计算某抽样调查所需样本量 $n$ 时，抽样调查还没开始！还不知道样本标准差 $s$ 和样本比例 $p$, 故抽样误差的计算公式中使用的是总体参数。 最主要与样本量大小有关； 其次还与总体变异性、抽样方法(有放回和无放回差个修正系数)、抽样调查的组织形式、辅助信息的使用（如分层抽样中的分层依据）和抽样框的准确性等因素有关。 非抽样误差，与样本随机性无关，因而在概率抽样和非概率抽样中都存在。不能通过增大样本容量来控制，仅能通过调查过程的质量控制来控制 ==抽样框误差==，仅在概率抽样中存在。指目标总体和抽样总体不一致时产生的误差。目标总体是指调查研究对象的全体，而抽样总体是从目标总体中抽选样本的总体。 丢失目标总体单元 包含非目标总体单元 复合连接，抽样框中的单元与总体目标单元不完全是一对一连接 不正确的辅助信息等 回答误差 理解误差 记忆误差 有意识误差 无回答误差 调查员误差 测量误差 有无放回、有序无序，共四种情况 上图图源知乎 https://zhuanlan.zhihu.com/p/48248142\n无序抽样使用 $C$ 组合；有序抽样使用 $A$ 排列。还是很好理解的。\n是否放回，决定了：\n样本是否可能重复 所抽取的样本间是否独立 抽样时总体数量是否发生变化 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%8A%BD%E6%A0%B7%E6%8A%80%E6%9C%AF/","summary":"抽样方法 不同抽样方法各有千秋。抽样方法的选取，取决于研究目的和对性价比的考量。 抽样调查可以分为两类，即概率抽样和非概率抽样。==概率抽样==是按照随机原则进行抽样，不加主观因素，组成总体的每个单位都有被抽中的概率（非零概率），可以避免样本出现偏差，样本对总体有很强的代表性。==非","title":"抽样技术"},{"content":"概率的定义 概率有古典定义、几何定义、频率定义（统计定义）和公理化定义（柯尔莫戈洛夫公理）共四种定义。（算上茆书后面提到的主观定义，那就一共有 5 种定义）\n其中古典定义、几何定义和频率定义（统计定义）都满足概率的公理化定义？\n具体介绍可参见该博文 概率的四种定义及公理化定义产生\n区别古典概率与古典统计学派：\n古典概率的定义基于等可能性假设，主要用于有限离散样本空间。 古典统计学派（频率学派），区别于贝叶斯学派，通过大量独立实验将概率解释为统计均值（大数定律）。古典统计学派（频率学派）使用的像是概率的频率定义（统计定义），而不是古典定义，不要搞混了！ 贝特朗悖论等概率悖论推动了概率公理化定义的出现（在此之前概率并未得到严谨定义，从而造成了许多悖论）。概率的公理化定义 $(\\Omega, \\mathscr{F}, P)$ 约束了样本空间 $\\Omega$, 测度空间 $\\mathscr{F}$, 和定义在 $\\mathscr{F}$ 上的概率测度、实值函数 $P$ 满足非负性、正则性和可列可加性.\n那么贝塔朗悖论对应的问题应该如何来解决呢？计算机随机模拟是一种办法（在圆上任意选择两个不同的点，模拟个十万八千次）。\n其中测度空间 $\\mathscr{F}$ 是样本空间 $\\Omega$ 的子集族，通常可以取 $2^{\\Omega}$. (空集+单元素集+……全集)\n例如：投掷一枚硬币的实验在概率公理化定义下可以描述如下：$H$ 表示正面，$T$ 表示反面。\n样本空间：$\\Omega = {H, T}$。 事件空间：$\\mathcal{F} = {\\varnothing, {H}, {T}, {H, T}}$。 概率测度：对于所有 $A \\in \\mathcal{F}$，定义： $P(\\varnothing) = 0, \\quad P(H) = \\frac{1}{2}, \\quad P(T) = \\frac{1}{2}, \\quad P({H, T}) = 1$ 关于概率 \u0026amp; 测度 有限或无限可列个概率为 0 的事件的并仍然是 0。 无限不可列个概率为 0 的事件的并可以有正概率。 概率 0 ≠ 绝对不可能发生，只是测度为 0(人家只是零测集，不是不可能事件)；但 P (不可能事件)=0\n同样，概率 1≠必然事件，一定发生，只是测度为 1. 但 P (必然事件)=1\nLebesgue 测度告诉我们，整个区间的测度是 1，而单个点的测度是 0。\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%A6%82%E7%8E%87%E7%9A%84%E5%AE%9A%E4%B9%89/","summary":"概率的定义 概率有古典定义、几何定义、频率定义（统计定义）和公理化定义（柯尔莫戈洛夫公理）共四种定义。（算上茆书后面提到的主观定义，那就一共有 5 种定义） 其中古典定义、几何定义和频率定义（统计定义）都满足概率的公理化定义？ 具体介绍可参见该博文 概率的四种定义及公理化定义产生 区别古典概率","title":"概率的定义"},{"content":"定义：用于估计未知参数 $\\theta$ 的统计量 $\\hat{\\theta}=\\theta(x_1, x_2, \u0026hellip;, x_n)$ 称为 $\\theta$ 的估计量，或称为 $\\theta$ 的点估计，简称估计。注意：==点估计是统计量（样本的函数）==\n补充统计量定义：设 $x_1​, x_2​, \u0026hellip;, x_n​$ 为取自某总体的样本，若样本函数 $T=T(x_1​, x_2​, \u0026hellip;, x_n​)$ 中不含有任何未知参数，则称 $T$ 为统计量。统计量的分布称为抽样分布。\n常用的点估计方法包括矩估计、极大似然估计、MUVE、BLUE、最大后验估计等\n点估计是指使用样本数据构造样本统计量来估计总体参数时，使用一个点的数值表示“最佳估计值”。\n与区间估计形成对比：区间估计通常是一个置信区间。\n矩估计 Momnet 用样本矩替换总体矩，用经验分布替换总体函数。\n理论基础：\n格列文科定理（茆定理 5.2.1：当样本量相当大时，经验分布函数 $F_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} I(X_i \\leq x)$ 是总体分布函数 $F(x)$ 的一个良好的近似，这也正是经典统计学中一切统计推断都以样本为依据的原因） 一阶矩的收敛：若一阶矩存在（如 $E(X)\u0026lt;∞$），辛钦大数定律，或切比雪夫大数定律。 高阶矩的收敛：若高阶矩存在（如 $E(X^k)\u0026lt;∞$），强大数定律（将弱大数定律拓展到 k 阶矩）。 ⚠️注意！\n分母为$N-1$的方差的无偏估计 $\\hat{\\sigma}^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\hat{\\mu})^2$ 不是矩估计！有偏估计 $\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{\\mu})^2$ 才是。\n极大似然估计 又名“事后诸葛亮估计” 极大似然估计（Maximum Likelihood Estimation, MLE）是一种基于样本数据估计概率模型参数的经典方法。其核心思想是：在给定观测数据的情况下，选择使得这些数据出现概率最大的参数值作为点估计值。\n似然函数（Likelihood Function）是一个==关于参数的函数==，它表示在给定参数 $\\theta$ 的情况下观察到数据 $X$ 的概率: $L(\\theta)=L(\\theta; X)=P(X|\\theta)$. 从这个式子可以看出，似然函数是关于样本 $X$ 的函数，没错，似然函数的概念听起来很陌生，但实际上就是我们在概率论当中看到的各种概率分布 $f(x)$. 而极大似然估计就是找到使似然函数最大的参数 $\\theta$, 即 $\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} L (\\theta) = \\arg\\max_{\\theta} P (X|\\theta)$\nMLE 之所以又名“事后诸葛亮估计”，或“马后炮”估计，是因为它就是“看结果猜原因，怎么合理怎么来”；擅长解释已发生的事，但未必能预测未来。\n引入一点贝叶斯 贝叶斯后验概率 $P(\\theta|X)=\\frac{P(X|\\theta)×P(\\theta)}{P(X)}=\\frac{似然×先验}{P(X)}$\n怎么理解贝叶斯后验概率公式？ #统计学 ✅ 2025-03-08 说到贝叶斯，就不得不提及古典频率学派和贝叶斯学派：\n频率学派（古典统计学）：认为参数真值是固定的且未知的常数，观察到的数据是随机的。 具体做法：根据大数定律，通过大量重复试验，用频率逼近概率 贝叶斯学派：贝叶斯学派认为参数真值不是固定的，是一个随机变量，而观察到的数据是固定的。这个随机变量可用一个概率分布去描述，这个分布称为先验分布， 具体做法：开始时给定先验概率，通过数据不断修正，最后得出后验概率，或后验分布。因引入主观的先验分布而受到大量质疑，但记得在一个视频里讲到过：随着修正的不断进行，先验分布的“权重”也在不断下降。 置信区间\n(confidence interval, CI) 可信区间\n(credibility interval,CI) 学派 古典统计学派，或频率学派 贝叶斯学派 观点 参数是一个未知的常数 参数是随机变量 两个区间的区别 对于一个给定的 95% 置信区间，含义是：构造区间 100 次，有 95 次包含参数真值。 对于一个给定的 95% 可信区间，含义是：参数真值有 95% 的概率落入该区间内。 回归 MLE：MLE 与 Bayes 的区别 要将『似然函数』与『后验概率』相区别：与似然函数 $P(X|\\theta)$ 恰好相反，后验概率是在给定数据 $X$ 后，参数 $\\theta$ 的概率 $P(\\theta|X)$. 注意是参数 $\\theta$ 的概率。进一步地，『后验分布』则是关于参数 $\\theta$ 的分布。\nMLE 不具有唯一性 $\\hat{\\beta}=(X^TX)^{-1}X^TY$\n唯⼀性条件： 如果似然函数是严格凸的，且数据满⾜⼀定的正则条件, $(X^TX)$ 可逆，那么极⼤似然估计的结果是唯⼀的。例如，在⾼斯分布中，均值和⽅差的极⼤似然估计是唯⼀的。对于简单的模型（如线性回归），极⼤似然估计通常有唯⼀解。\n⾮唯⼀性情况： 似然函数⾮凸或多峰，或者数据存在共线性问题, $(X^TX)$ 不可逆，极⼤似然估计可能会有多个局部极⼤值，导致解不唯⼀。\n矩估计 VS 极大似然估计 常考！ 矩估计 极大似然估计 原理简单、使用方便、计算便捷 计算复杂 可以在不知道总体分布但知道样本矩的情况下对总体进行推断 需在总体分布已知的情况下才能使用（需对总体分布做假设） 未充分利用样本信息，即不一定是充分统计量的函数,估计精度不高，只有当样本量较大时才能保证其优良性 充分利用样本信息，一般都与充分统计量有关，估计精度高 一般情况下具有相合性 具有不变性和渐进正态性，大样本下也具有相合性 不适用于总体矩不存在的分布，如柯西分布 不唯一性 MVUE 等 Rao-Blackwell 定理：假设 $g(X)$ 是 $\\theta$ 的一个任意估计， $T$ 是一个充分统计量。那么 $g(X)$ 相对于给定 $T(X)$ 的条件期望是一个比 $g(X)$ 更好的估计量（至少不差于）。\n由于MVUE不一定总能达到C-R下界，这个定理提供了不用C-R下界的寻找MVUE的方法。\n定理告诉我们，通过条件期望，可以利⽤充分统计量改进估计量的效率。\n评估点估计的标准 很多概念，需要多过几遍，晕🥴\n无偏性 定义：若对 $\\forall \\theta \\in \\Theta,$ 有 $E_\\theta(\\hat{\\theta}) = \\theta$ 则称 $\\hat{\\theta}$ 为 $\\theta$ 的无偏估计。\n并非所有参数都存在无偏估计。\n参数是否可估：取决于是否有无偏估计。\n有效性 无偏估计比较方差，有偏估计比较均方误差。\n方差一般用来计算样本的离散程度，而均方误差则可以用做衡量模型拟合的一个度量\n无偏估计量不一定是最有效的！\n附：MSE 分解公式推导： 大样本情形 - 相合性，又称一致性 Consistency ==弱相合性==，当样本量 $n \\to \\infty$ 时，估计量 $\\hat{\\theta}_n$ 依概率收敛于真实参数 $\\theta.$ 记为 $\\hat{\\theta}_n \\xrightarrow{P} \\theta$ 即 $\\forall \\epsilon \u0026gt; 0, \\quad \\lim_{n \\to \\infty} P\\left (|\\hat{\\theta}_n - \\theta| \\geq \\epsilon\\right) = 0.$\n弱相合性被认为是对估计的一个最基本的要求，如果一个估计量，在样本量不断增大的时，他都不能把被估参数估计到任意的指定的精度 $\\varepsilon$ ，那么这个估计是很值得怀疑的。通常，不满足弱相合性要求的估计不予考虑。\n弱相合估计不止一个：样本有偏方差和样本无偏方差都是 $\\sigma^2$ 的相合估计。\n弱相合性判定：$lim_{n \\to \\infty}E(\\hat{\\theta}_n)=\\theta, \\quad lim_{n \\to \\infty}Var(\\hat{\\theta}_n)=0$\n==强相合性==，当样本量 $n \\to \\infty$ 时，估计量 $\\hat{\\theta}_n$ 几乎必然收敛于真实参数 $\\theta$\n即 $P\\left (\\lim_{n \\to \\infty} \\hat{\\theta}_n = \\theta\\right) = 1.$ 也称 $\\hat{\\theta}_n$ 几乎必然(Alomst Sure)收敛到 $\\theta$, 记为 $\\hat{\\theta}_n \\xrightarrow{a.s.} \\theta.$\n强弱相合性之间的差异类似强弱大数定律之间的差异：\n弱大数定律 强大数定律 依概率收敛 几乎必然收敛 $\\bar{X}_n \\xrightarrow{P} \\mu$ $\\bar{X}_n \\xrightarrow{a.s.} \\mu$ 允许样本均值无限次偏离 $\\mu$（但概率趋近于零）例如抛硬币实验中偶尔出现连续多次正面 彻底排除了无限次偏离的可能，确保样本均值最终“锁定”在 $\\mu$ 附近 由强大数定律可知，矩估计总是相合的。\n大样本情形 - 渐进正态性 当样本容量趋向无穷大时，某些估计量（如最大似然估计）会收敛到一个正态分布。\n在相合性的基础上进一步关注收敛速度和分布特性。\n极大似然估计具有渐进正态性。\n衡量点估计的标准的不变性 定义：如果某一个统计量 $T(x)$ 是某个参数 $\\theta$ 的无偏估计具有某个性质，将统计量经过 $g$ 变换之后为 $g(T(x))$，变换后的参数 $g(\\theta)$ 同样具有原有性质。\n极大似然估计具有不变性，完备性具有不变性，矩估计也具有不变性\n无偏估计不具有不变性，若 $\\hat{\\theta}$ 是 $\\theta$ 的无偏估计， $g(\\hat{\\theta})$ 则不一定是 $\\theta$ 的无偏估计，除非 $g(\\theta)$ 是线性函数；\n充分性不具有不变性，除非 $g(\\theta)$ 是严格单调函数\n相合性不具有不变性，除非 $g(\\theta)$ 是连续函数\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E7%82%B9%E4%BC%B0%E8%AE%A1%E7%9B%B8%E5%85%B3%E5%BD%92%E7%BA%B3/","summary":"定义：用于估计未知参数 $\\theta$ 的统计量 $\\hat{\\theta}=\\theta(x_1, x_2, \u0026hellip;, x_n)$ 称为 $\\theta$ 的估计量，或称为 $\\theta$ 的点估计，简称估计。注意：==点估计是统计量（样本的函数）== 补充统计量定义：设 $x_1​, x_2​, \u0026hellip;, x_n​$ 为取自某总体的样本，若样本函数 $T=T(x_1​, x_2​, \u0026hellip;, x_n​)$ 中不含有任何未知参数，则称 $T$","title":"点估计相关归纳"},{"content":"主成分分析 主成分分析（Principal Component Analysis，PCA）是研究如何==通过原始变量的少数几个线性组合来解释原始变量绝大多数信息==的多元统计分析方法。在尽可能地保留原始变量信息的前提下进行降维，从而简化问题的复杂性，==抓住问题的主要矛盾==。 该方法主要基于众多原始变量之间有一定的相关性(即共线性)，则必然存在着起支配作用的共同因素这一想法，来对原始变量==协方差矩阵或相关系数矩阵==内部结构进行研究。\n使用前提 开展PCA一般需要满足以下前提条件：\n原始数据的变量数目较多，或有数据降维的需求，否则做主成分分析没有意义。 原始数据各个变量之间的共线性或相关关系较强，如果原始变量之间的线性相关程度很小，它们之间不存在简化的数据结构，这时进行主成分分析实际是没有意义的。 在应用PCA之前，需要对其适用性进行统计检验，检验方法有 KMO 抽样适合性检验(Measure of Sampling Adequacy，检验原始变量之间的相关系数和偏相关系数的相对大小) 巴特利特(Bartlett)球形检验(检验原始变量间的相关程度，即共线性程度) 等等。 PCA使用前提摘自主成分分析(Principal Component Analysis)——理论介绍\n通俗理解主成分分析 假如原始数据中总共有 100 个原始变量，为方便分析及解释，我们希望把这 100 个原始变量给“浓缩”一下。注意，“浓缩”体现出主成分分析的精髓：\n浓：原始变量蕴含的信息，在操作中要最大程度地保留； 缩：维度要降低。 我们想要尽可能地保留原始数据中的信息（因为降维不可避免地会造成信息的丢失，想象从 3 维空间到 2 维空间）什么时候信息的“利用率”最高呢？答案是正交的时候。变量间相互正交，可以理解为不同变量所蕴含的信息不重复，每个变量中蕴含的信息都是独一无二、无可替代的。\n信息这个词有点抽象，其实有个概念可以很好地量化“信息”这个词：方差。变量的“方差”所蕴含的是“信息量”（理解：变异程度越大，其中所含信息就越多）\n所以：思路清楚了：把原始变量组合成方差大且正交的新变量即可。\n具体操作 我们从包含变量方差信息的矩阵——协方差矩阵入手。\n既想降维，减少变量个数，又想尽可能地保留信息：把方差大的几个原始变量组合起来不就好了。\n对协方差矩阵进行==特征值分解==（实对称矩阵必可相似对角化）不就好了。取最大的几个特征值所对应的特征向量作为主成分即可。\n但是特征值分解有个问题，只适用于方阵，且更致命的是：计算量成本巨高。\n所以便引入使用==奇异值分解==的方法。详见推文\n关于奇异值分解，其思想跟 LoRA (针对LLM的一种轻量级微调技术)相似：通过“低秩近似”以减小算法上的开支：\n例如一个 985 × 211 的矩阵，有 20w+ 个元素需要计算；通过低秩近似，用 985 × 10 乘 10 × 211，便只需要计算不到 2w 个元素\n特征值分解和奇异值分解两种方法，都能达到目的。但途径不同，各有千秋。\n使用协方差矩阵还是相关系数矩阵 ？ 各变量量纲差不多时，使用协方差矩阵即可。反之则使用相关系数矩阵（多了一步标准化的操作）\n个人感觉无脑使用相关系数矩阵即可，不知道协方差矩阵在相关系数矩阵面前有什么优势？如有知道的朋友，还请指教。\n因子分析 面对主成分分析中主成分难以解释的问题，因子分析，被当做主成分分析的推广和发展，得到广泛应用。\n因子模型 $$x = \\mu + Af + \\varepsilon$$\n式中 f = $(f_1, f_2, \\cdots, f_m)\u0026rsquo;$ 为公共因子向量，$\\varepsilon = (\\varepsilon_1, \\varepsilon_2, \\cdots, \\varepsilon_p)\u0026rsquo;$ 为特殊因子向量，$A = (a_{ij}): p \\times m$ 称为因子载荷矩阵。\n模型假设：\n公共因子之间不相关； 特殊因子之间不相关； 公共因子和特殊因子之间不相关 下面的任务便是：估计因子载荷矩阵 $A$ 和公共因子向量 $f$。\n可以对比一下回归分析中 $Y=X\\beta+\\varepsilon$ 主要估计的是 $\\beta$ ，$X$ 是已知的观测值。\n因子载荷矩阵常用的估计方法 主成分法：（直接像 PCA 一样进行特征分解，又称谱分解）、主因子法、极大似然法\n主成分分析与因子分析的区别与联系 “降维”，顾名思义就是降低数据维度（数据维度 = 原始变量个数）。\n主成分分析和因子分析使用的都是“降维”的思想，但两者“降维”的思路不一样，两者“降维”的思路差异，也就造成了两者的“可解释性”差异：\n主成分分析是通过将原始变量进行线性组合（称为主成分）来达到“降维”的目的，==可解释性弱==； 而因子分析则是先提炼因子，再用所提炼的因子来表达原始变量，==可解释性强==。 举个例子：假设我们收集了 100 名学生在 4 门课程（语文、数学、地理、生物）中的考试成绩，我们希望找出数据的潜在结构，减少变量维度，便于分析。\n主成分分析（这里不太严谨，因为没保证正交）： 理科主成分 = 数学 + 0.7 生物 + 0.4 地理； 文科主成分 = 语文 + 0.3 生物 + 0.6 地理。 因子分析： 先设定因子：文科因子和理科因子 语文 = 1 × 文科因子 + 0 × 理科因子 数学 = 0 × 文科因子 + 1 × 理科因子 生物 = 0.3 × 文科因子 + 0.7 × 理科因子 地理 = 0.6 × 文科因子 + 0.4 × 理科因子 原始变量与主成分/因子间的关系 主成分分析：相关系数和载荷 $x_i = t_{i1}y_1 + t_{i2}y_2 + \\cdots + t_{ip}y_p, \\quad i = 1, 2, \\ldots, p$\n两个指标：相关系数 $\\rho(x_i, y_k)=\\frac{\\sqrt{\\lambda_k}}{\\sqrt{\\sigma_{ii}}}t_{ik}$ 和载荷(即主成分表达式中的系数)\n相关系数：单变量角度，忽略了其它原始变量的前提下==某原始变量与某主成分之间的关系==； 载荷：多变量角度，考虑到了其它原始变量在场的情况下某==原始变量与某主成分之间的关系==。 所有主成分对某原始变量的贡献率：m 个主成分 $y_1, y_2, \u0026hellip;, y_m$ 对原始变量 $x_i$ 的贡献率 $\\rho_{i \\cdot 1, \\cdots, m}^2 = \\sum_{k=1}^{m} \\rho^2(x_i, y_k) = \\sum_{k=1}^{m} \\frac{\\lambda_k t_{ik}^2}{\\sigma_{ii}}$；\n在解释主成分时，看哪个呢？马哲告诉我们：既需要考察相关系数，又需要考察载荷。\n因子分析：因子载荷矩阵和因子得分矩阵 ==因子载荷矩阵==是一个 变量×因子 的矩阵，其中元素即各个原始变量的因子表达式的系数，表达提取的公因子对原始变量的影响程度。\n通过因子载荷矩阵可以得到原始指标变量的线性组合，如 $X_1=a_{11}*F_1+a_{12}*F_2+a_{13}*F_3$, 其中 $X_1$ 为指标变量 1，$a_{11}, a_{12}, a_{13}$ 分别为与变量 $X_1$ 在同一行的因子载荷，$F_1, F_2, F_3$ 分别为提取的公因子；\n因子载荷矩阵 $A$ 中的元素 $a_{ij}$ ：表示 $x_i$ 与 $f_j$ 之间的==协方差或相关系数==（取决于是否标准化） 因子载荷矩阵 $A$ 的行元素平方和：反映公共因子对 $x_i$ 的影响，称为==共性方差==。特殊因子对 $x_i$ 的方差贡献则称为 $\\sigma_i^2$ ==特殊方差==。 因子载荷矩阵 $A$ 的列元素平方和：可视为公共因子 $f_i$ 对 $x_1, x_2, \u0026hellip;, x_p$ 的总方差贡献，是衡量公共因子 $f_i$ 重要性的一个尺度。 因子载荷矩阵 $A$ 的所有元素平方和：公共因子 $f_1, f_2, \u0026hellip;, f_m$ 对总方差的累计贡献。 公共因子的估计值, 称为因子得分 (factor scores)\n==因子得分矩阵==是一个 样本×因子 的矩阵，表示个体在潜在因子上的得分。它显示了每个观测对象（样本）在各个因子上的得分。因子得分反映了每个观测对象在因子上的位置或表现，数值越大，表示该对象在该因子上的得分越高。\n因子得分可以通过两种方法：加权最小二乘法和回归法（之所以称为回归法，是因为在回归分析中，条件均值被称之为回归函数）对不可观测的随机变量 $f_1, f_2, \u0026hellip;, f_m$ 的取值进行估计（但不算是参数估计）\n通过一个例子来加深理解：\n关于旋转💫 主成分分析和因子分析中的旋转，就是让各变量在单主成分（因子）上有高额载荷，而在其它主成分（因子）是只有小到中等的载荷。其目的是为了提高可解释性。\n让每个变量对某个主成分/因子的贡献更清晰（即让因子载荷更接近 0 或 1，减少多个因子共享同一变量的情况）。例如 某主成分 = $0.6X + 0.5Y + 0.5Z + \u0026hellip;$ 有些含糊 某主成分 = $0.9X\u0026rsquo; + 0.1Y\u0026rsquo; + 0.05Z\u0026rsquo; + \u0026hellip;$ 更清晰明了 让主成分/因子更加符合实际意义（即调整主成分/因子，使其更接近数据中方差较大的方向，同时减少信息混杂）。 可以看作一种对“主成分”或“因子”的优化。\n主成分分析中也可以旋转，但一般情况下并不需要旋转，因为既然选择了主成分分析，一般都不会太在意主成分的“可解释性”。\n时序数据能进行 PCA 和因子分析吗？ 绝大多数情况都不能，因为时间序列数据在绝大多数情况下都存在自相关性，不是简单随机样本 (要求 iid)，样本协方差矩阵 (相关系数矩阵) 不是总体协方差矩阵 (相关系数矩阵) 的无偏估计，贸然应用样本协方差矩阵 (相关系数矩阵) 会产生较大偏差。\n如处理后的数据消除了自相关性，则可考虑进行 PCA 和因子分析。\n聚类分析 聚类分析（Cluster Analysis）是一种无监督学习技术，主要用于将数据样本划分成多个类别或簇（Cluster），使得在同一簇内的数据相似性较高，不同簇之间的数据相似性较低。\n距离度量：明考夫斯基距离（绝对值距离（又名曼哈顿距离）、欧氏距离、切比雪夫距离）、兰氏距离（加权版的曼哈顿距离）、马氏距离（协方差距离） 可参考其它推文链接1、 链接2 相似系数：夹角余弦、相关系数 不同的聚类方法： 不同聚类方法效果对比图 系统聚类法（层次聚类法） 凝聚的层次聚类：最短距离法、最长距离法、类平均法、重心法、中间距离法、离差平方和法（Ward\u0026rsquo;s method）. 上图图源 CSDN @阿伦很努力 推文 分裂的层次聚类：略，分裂其实就和凝聚反过来一样 动态聚类法（逐步聚类法）：K-means（初值敏感）、K-means++（随机初值，轮盘法更新聚类中心）、bi-kmeans（优化局部最优问题）。==K-Means 简单快速，适用大数据集，但只适用于平均值能被定义的情形，且初值敏感==。 基于密度的聚类方法：DBSCAN 新方法：核聚类、谱聚类、量子聚类等 用目测法在主成分得分图上可以进行直观的聚类，其中包含着正规聚类方法所反映不出的丰富信息，由此或许可以得到比正规聚类方法更为合理的聚类结果。\n判别分析 判别分析(discriminant analysis)属于“有监督学习”方法，从所谓“训练样本”经过分析计算得到一个判别规则，对新的数据可以利用判别规则判断新数据观测的类属。训练样本中既有用来分类的解释变量（自变量），又有真实的类属（标签，因变量）。\n常用判别分析方法有距离判别、Fisher判别和Bayes判别，Logistics回归也经常用在判别问题中（尤其是两类的判别），分类树也是用于判别的方法。\n距离判别：根据待判定样本与已知类别样本之间的距离远近做出判别。典型算法如：K最近邻(K-Nearest Neighbor，简称 KNN，“邻居是啥，我就是啥”)\n费希尔判别：通过“投影”简化问题。（与 SVM 的思想刚好相反，SVM 是通过超平面升维）。如二分类问题上的线性判别分析（Linear Discriminant Analysis，简称LDA）。 线性判别分析LDA 详见 [[机器学习 Machine Learning]] 贝叶斯判别：最大后验概率法、最小期望误判代价法。典型算法：朴素贝叶斯分类器(Naive Bayesian Classification)，哪个类别后验概率大，就属于哪个类别（拉普拉斯平滑解决特征未出现在训练集导致后验概率为 0 的问题）\n当标签（因变量）只有两个类时，判别分析问题与假设检验问题有相似之处。\n假设检验问题更强调统计推断的严谨性（拒绝原假设的理由一定要充分）； 两类的判别问题并不强调某个类别，或者按照先验概率、损失函数对不同类别施加不同的影响。 可以这样理解：假设检验是有“偏向”的，而判别分析是“平等”的 [[九阳真经 - 假设检验篇#假设检验的精髓：不平衡（Imbalance）]] 线性判别分析与其他分类算法有以下联系：转载\n与逻辑回归的联系：逻辑回归是一种基于概率模型的分类算法，它可以看作是线性判别分析在多元正态分布假设下的一种特殊情况。==逻辑回归假设输入变量之间是独立的，而线性判别分析则不作这一假设==。 与支持向量机的联系：支持向量机是一种通过最小化损失函数来训练分类器的算法。在线性情况下，==支持向量机可以看作是线性判别分析在不使用正态分布假设的情况下的一种特殊情况==。 与决策树的联系：决策树是一种基于树状结构的分类算法，它可以自动选择特征并构建分类器。决策树与线性判别分析的主要区别在于==决策树不需要任何假设，而线性判别分析则需要正态分布假设==。 与朴素贝叶斯分类器的联系：朴素贝叶斯分类器是一种基于贝叶斯定理的分类算法，它假设输入变量之间是独立的。朴素贝叶斯分类器与线性判别分析的主要区别在于==朴素贝叶斯分类器不需要正态分布假设==。 ==区分 K-Means 和 KNN 两种不同算法！K-Means 用于聚类分析；而 KNN 用于判别分析。==\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/","summary":"主成分分析 主成分分析（Principal Component Analysis，PCA）是研究如何==通过原始变量的少数几个线性组合来解释原始变量绝大多数信息==的多元统计分析方法。在尽可能地保留原始变量信息的前提下进行降维，从而简化问题的复杂性，==抓住问题的主要矛盾==。 该方法主要基于众多原始变","title":"多元统计分析复习笔记"},{"content":"假设检验是统计推断的重要方法！统计推断包括假设检验和参数估计。\n假设检验的定义 假设检验（hypothesis testing）是指从对总体参数所做的一个假设开始，然后搜集样本数据，计算出样本统计量，进而运用这些数据测定假设的总体参数在多大程度上是可靠的，并做出承认还是拒绝该假设的判断。如果进行假设检验时总体的分布形式已知，需要对总体的未知参数进行假设检验，称其为参数假设检验；若对总体分布形式所知甚少，需要对未知分布函数的形式及其他特征进行假设检验，通常称之为非参数假设检验。此外，根据研究者感兴趣的备择假设的内容不同，假设检验还可分为单侧检验（单尾检验）和双侧检验（双尾检验），而单侧检验又分为左侧检验和右侧检验。 - 国家统计局\n所谓假设检验，就是通过样本来推测总体是否具备某种性质。\n假设检验的原理 假设检验使用的是“证伪”的思想，是一种“反证法”。\n假设检验的基本思想是==反证法思想==和==小概率事件原理==。 反证法的思想是首先提出假设（由于未经检验是否成立，所以称为零假设、原假设或无效假设），然后用适当的统计方法，根据已有的样本，来确定假设成立的可能性大小，如果可能性小，则认为假设不成立，拒绝它；如果可能性大，还不能认为它不成立。 小概率事件原理，是指小概率事件在一次随机试验中几乎不可能发生，小概率事件发生的概率一般称之为“显著性水平”或“检验水平”，用 $\\alpha$ 表示，而概率小于多少算小概率是相对的，在进行统计分析时要事先规定，通常取 $\\alpha=0.01、0.05、0.10$ 等。 - 国家统计局\n假设检验具体是如何操作的？ 代入给定的 $H_0$ 的参数值建立分布 看看样本“像不像”是从这个分布里头出来的（是否落入拒绝域） 若“像”（落入接受域），这帮样本就是从 $H_0$ 那个分布来的，说明 $H_0$ 对应的参数有可能就是真实参数； 若“不像”（落入拒绝域），好家伙这帮样本不是从 $H_0$ 那个分布来的，说明 $H_0$ 对应的参数大概率不是真实参数（拒绝 $H_0$）。但是 $H_0$ 错误，不代表 $H_1$ 就正确，所以只能说“拒绝 $H_0$”，而不能说“接受 $H_1$”） 如何量化这个“像”与“不像”，详见后面关于显著性水平 $\\alpha$ 和 P-value 的章节。 Over 下结论。 知道了假设检验具体是如何操作的，也就知道了:\n为什么假设检验要将等号放在原假设? 因为检验统计量的计算需要给定一个（假想的）参数值，否则就无法建立相应的分布，不方便计算。比如说，简单的单总体均值检验中，假定 $H_0: \\mu = 20$ 这样才能简单地把确定在这个假想均值确定的抽样分布下，把检验统计量给算出来。不带等号的话，这个分布的确定就困难了。\n假如，我偏不把等号放在原假设，把不等号放在原假设，例如 $H_0: \\mu \u0026lt; 20$ 我取 $\\mu$ 等于多少来建立分布呢？是 19？还是 15？亦或是 10？这时不就出现问题了。\n关于检验统计量的选取 分布是个好东西！\nThe statistical models of distributions, however, enable us to describe the mathematical nature of that randomness. - 《The lady tasting tea》\nEg. 现有一关于参数 $\\theta$ 的假设检验：==理论上==只要某统计量 T 的分布与参数 $\\theta$ 有关，即可用它来构造检验统计量。\n因为如果我们能得到 $H_0$ 成立下统计量 T 的分布，即可计算出在该分布中出现样本或其它更极端情况的概率。\n关键是：分布！\n检验统计量的选取 - 这个地方可以想个例子来帮助理解。 假设检验的精髓：不平衡（Imbalance） 假设检验中，原假设 $H_0$ 和备择假设 $H_1$ 的地位是不对等的：\n$H_0$ 通常是被默认为真的假设，需要足够的证据才能拒绝； 而 $H_1$ 则与 $H_0$ 相对立，通常是研究者试图去证明的假设。 打个比方：\n$H_0$ 像是“正统”，“权威”，“传统”，具有一定的“优先权”，默认情况下它都是正确的 而 $H_1$ 则像是“新教”，“创新”，甚至有些“备胎”的味道，需要在将 $H_0$ 拒绝后，才可能“喧宾夺主”，有“翻身”的机会。这里使用的“可能”、“机会”两词是严谨的，因为在实际的假设检验中： 哪怕拒绝了原假设，我们会说在 xx 的显著性水平下能够“拒绝 $H_0$”，而不是“接受 $H_1$”； 同样若没拒绝原假设，我们会说在 xx 的显著性水平下不能“拒绝 $H_0$”，而不是“接受 $H_0$”。 总结：判断一个假设是否『正确』是很难的，然而判断一个假设是否『错误』相对来说就容易多了：因为证伪只需要一个特例就足够了，==一个特例足以推翻一个论点，却远远不能支撑一个论点==。 之所以如此不平衡地设计“假设检验”这个东西，是为了给原假设一定“优先权”，这样有助于控制做出错误结论的风险（特别是第一类错误）。打个比方：“传统”的 $H_0$ 一般都是正确的，为防止左倾冒进分子冲动地推翻，于是在设计上给了它（即 $H_0$ ）一定的“优先权”。（备胎想要成为上位？很难的啦）\n矛盾无处不在，这种“不平衡”的设计，同样具有双面性：虽然确实有助于控制做出错误结论的风险，但如此不平衡的设计导致它并非使用于所有问题。如：\n现有一堆 0-2 之间的样本，判断是来自 $U(0, 1)$ 还是 $U(1, 2)$ 。这个问题就不适用于假设检验。因为在这个问题中 $U(0, 1)$ 和 $U(1, 2)$ 的“地位”是平等的。可通过极大似然估计 MLE 来解决这个问题。（分别计算样本在 $U(0, 1)$ 和 $U(1, 2)$ 的对数似然值即可，看哪个大）\n而像女士品茶（==将牛奶倒入茶==和==将茶倒入牛奶==对奶茶的口味是否有影响）这样的问题，显然是能够根据我们的先验知识做出一定猜测的，这种问题便可以通过假设检验这样的方法来解决。\n通过了解如何做出原假设，能够帮助你更好地理解假设检验的 Imbalance.\n做出原假设的依据 根据现有理论或知识：原假设往往基于现有的理论或广泛接受的知识。例如，如果现有理论表明两种药物效果相同，那么原假设可能就是\u0026quot;这两种药物的效果没有差异”。 简单性或保守性：在统计学中，原假设通常是一个简单假设，它提出了最简单、最保守的情况。例如，“新药与安慰剂效果无差别”是一个比“新药比安慰剂效果好”更简单、更保守的假设。 研究目的：研究者可能会根据研究目的来确定原假设。如果研究目的是证明一种新的干预措施有效，那么原假设可能就是“新干预措施与现有措施效果相同”。 来自北京师范大学-432统计学-2024年-解析\n假设检验中的显著性水平 $\\alpha$ 与功效 $1-\\beta$ 与 P-value 项目 无法拒绝 $H_0$ 拒绝 $H_0$ $H_0$ 为真 $1-\\alpha$ $\\alpha$ ==弃真错误== $H_0$ 为伪 $\\beta$ ==取伪错误== $1-\\beta$ ==功效== 用极限的思想来理解：在固定样本量的前提下，不能同时减小第一类错误 $\\alpha$ 和第二类错误 $\\beta$ ：\n若 $\\alpha=0$ -\u0026gt; 不犯弃真错误 -\u0026gt; 索性直接接受原假设 -\u0026gt; 更有可能犯 $\\beta$ 若 $\\beta=0$ -\u0026gt; 不犯取伪错误 -\u0026gt; 索性直接拒绝原假设 -\u0026gt; 更有可能犯 $\\alpha$ 整个实验来看：$\\alpha + \\beta \\in (0, 2)$ ，若某分类器将所有正例均判为负，将所有负例都判为正，这时 $\\alpha=\\beta=1, \\quad \\alpha+\\beta=2$ 单次实验结果来看，不可能同时发生弃真错误和取伪错误。表格中的四个值 $1-\\alpha, \\quad \\alpha, \\quad \\beta, \\quad 1-\\beta$ 均 $\\in (0, 1)$ 显著性水平 $\\alpha$ 在假设检验中，显著性水平（Significance Level），通常记为 $\\alpha$，是指在原假设 $H_0$ 为真时，错误地拒绝 $H_0$ 的概率，即第一类错误（Type I Error）的概率。换句话说，$\\alpha$ 代表了==我们能够接受的犯第一类错误的概率==（假阳性风险），即我们错误地发现了一个不存在的效应的可能性。\n两类错误，在具体实践中往往更加关注弃真错误 $\\alpha$ ，因为错误地拒绝“传统”、“权威”的原假设 $H_0$ （即弃真错误）所带来的损失，相对于在权威假设 $H_0$ 错误的背景下, 没能找到真正正确的假设 $H_1$，仍误认为权威假设 $H_0$ 是正确的（即取伪错误）而言，是更大的。\n例如，在制药行业，如果一个新药被批准（拒绝 $H_0$ ​）但实际上无效或有害，会带来巨大损失；相反，如果一个有效药物未被批准（即犯第二类错误），虽然可惜，但仍可在未来研究中重新评估。\nP值 P-value P-value：在原假设成立的条件下，出现样本或更极端情况的概率。\n在假设检验实际操作中，可通过比较 P-value 与显著性水平 $\\alpha$ 值的大小比较来判断是否应该拒绝原假设。\n举个稍微有点极端的例子：现有样本来自某正态分布 $N(\\mu, 0)$ ：[8, 17, 10, 9]，考虑假设检验问题 $H_0: \\mu=0$ Vs $H_1: \\mu=10$ 。\n我们来看看此时原假设 $H_0: \\mu=0$ 成立的条件下出现样本或更极端情况的概率，即从 $N(0, 1)$ 中抽到[8, 17, 10, 9]这几个样本的概率，显然很小，即 P-value 很小，P-value 越小，越是应该拒绝原假设。\n想想，咱假设原假设成立，可是发现这个假设下出现我们实际抽到的样本的概率极低，此时理所当然应该拒绝原假设。\nP-value 一般都介于 0-1 之间，怎么量化它的“小”呢？这时显著性水平 $\\alpha$ 就站了出来，大喊一声“我来！”。可以将显著性水平 $\\alpha$ 理解为“阈值”\n若 P-value \u0026lt;= $\\alpha$ ，说明 P-value \u0026lt; 阈值，拒绝原假设 反之则不能拒绝原假设 功效 Power $1-\\beta$ 另外一个稍微“小众”一些的概念 - 功效（Power）：反映检验在面对正确的备择假设时正确做出决定的能力。有点像“抓走坏人”的能力。毫无疑问，越大越好。\n再延伸一个更“小众”的概念 - 势函数 / 功效函数 (Power Funtion)：功效函数（Power Function）是用于衡量检验方法在不同实际参数值下拒绝原假设的概率。==即不同实际参数值下样本观测值落入拒绝域内的概率==。综合反映了第一类错误和功效的情况。\n$$ g(\\theta) = \\begin{cases} \\begin{array}{rl} \\phantom{1 -} \\alpha(\\theta), \u0026amp; \\theta \\in \\Theta_0 \\\\ 1 - \\beta(\\theta), \u0026amp; \\theta \\in \\Theta_1 \\end{array} \\end{cases} $$\n当 $H_0$ 为真时，$g(\\theta)$ 为犯第一类错误的概率 当 $H_1$ 为真时，$g(\\theta)$ 为功效\n既然固定样本容量时，任何检验都不能同时让第一类错误和第二类错误的概率很小，那么 (Jerzy) Neyman-(Egon) Pearson 所提出的原则就是：在保证犯第一类错误的概率不超过指定数值 $\\alpha$ 的检验中，寻找犯第二类错误 $\\beta$ 概率尽可能小的检验。（N-P准则）\nN-P 准则用势函数 / 功效函数（Power Function）来表达： $$ \\begin{cases} g(\\theta) \\leq \\alpha, \u0026amp; \\theta \\in \\Theta_0 \\\\ g(\\theta) \\text{ 尽可能大}, \u0026amp; \\theta \\in \\Theta_1 \\end{cases} $$ 势函数的作用：\n假设检验与区间估计的关系 24北师应统真题，茆P326\n区别 1：\n假设检验是判断一个有关总体未知参数的命题是否成立的问题，且只能证伪； 而区间估计是构造一个未知参数最合理的取值范围的问题。 区间估计所提供的信息要比假设检验更加丰富。 区别 2：值得细品\n置信区间是针对==参数==的集合，置信区间是用来估计参数的，这个很好理解； 拒绝域与接受域是针对==样本==的集合，拒绝域+接受域=样本空间。 联系：拒绝域与置信区间的对偶关系\n双侧检验问题的接受域即置信区间 单侧检验问题的接受域即置信上/下限，例单侧检验 $H_0:\\mu≤\\mu_0 \\quad vs \\quad H_1:\\mu\u0026gt;\\mu_0$, 其接受域 $W={u=\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}}≤z_{1-\\alpha}}$, 移项: $\\mu≥\\bar{x}-z_{1-\\alpha}\\frac{\\sigma}{\\sqrt{n}}$. 便得到了参数 $\\mu$ 的 $1-\\alpha$ 置信下限。 其它 其它条件不变的前提下，增大样本量，会使 $\\alpha$ 和 $\\beta$ 同时减小吗？争议问题🤔\n会 ==不会== 目前占上方 显著性水平是人为给定，但犯第一类错误 $\\alpha$ 的概率不是恒定的，它会受样本量和犯第二类错误 $\\beta$ 的概率等影响。\n——贾俊平《统计学》和我的概率论老师 增大样本量，只能降低犯第二类错误 $\\beta$ 的概率（提升功效，因为随着样本量↑，估计精度↑），而犯第一类错误的概率是由人为设定的显著性水平 $\\alpha$ 来决定的，无法通过增大样本量来降低。\n——《卫生统计学第八版》和 GPT，以及 DeepSeek 选择合适的检验⽅法或改进实验设计，可以在⼀定程度上优化两类错误的平衡。例如，使⽤更⾼效的统计模型或增加实验的灵敏度。\n两个正态总体均值差的检验 方差已知：u 统计量 方差未知情形，我们需要先做⼀个⽅差⻬性检验，来判断两个总体的⽅差是否相等。 方差相等但未知：Student\u0026rsquo;s t-test, 带 $Sw$ 的那个公式 方差不等且未知：Welch\u0026rsquo;s t-test，使用 Satterthwaite 公式计算自由度 非正态总体： 小样本：Mann-Whitney U 检验（也称为 Wilcoxon 秩和检验） 大样本：近似正态 其它非参数检验方法：Bootstrap ⽅法、置换检验…… ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E4%B9%9D%E9%98%B3%E7%9C%9F%E7%BB%8F---%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E7%AF%87/","summary":"假设检验是统计推断的重要方法！统计推断包括假设检验和参数估计。 假设检验的定义 假设检验（hypothesis testing）是指从对总体参数所做的一个假设开始，然后搜集样本数据，计算出样本统计量，进而运用这些数据测定假设的总体参数在多大程度上是可靠的，并做出承认还是拒绝该假设的判断","title":"九阳真经 - 假设检验篇"},{"content":"随机事件层面的独立 在概率论里，\u0026ldquo;独立\u0026quot;并不意味着两个事件没有任何关系。 独立意味着一个事件的发生与否都不改变另一个事件发生的概率。 - 知乎文章\n数学定义见下：\n两个随机事件间的独立性定义： $P(AB)=P(A)P(B)$，即 $P(A|B)=P(A)$ 。不独立，又称相依。\n多个随机事件间的独立性定义：设有 N 个事件 $A_1, A_2, \u0026hellip;, A_N$ 对任意的 $1 ≤ i ≤ j ≤ k ≤ N$ 如下式成立：为什么不使用 $i, j, k \\in [1, N]$ ？避免重复吗？\n$$ \\begin{aligned} P(A_i A_j) \u0026amp;= P(A_i) P(A_j) \\quad \\text{两两独立} \\\\ P(A_i A_j A_k) \u0026amp;= P(A_i) P(A_j) P(A_k) \\quad \\text{三三独立} \\\\ \u0026amp;\\quad \\vdots \\\\ P(A_i A_j \\cdots A_N) \u0026amp;= P(A_i) P(A_j) \\cdots P(A_N) \\quad \\text{NN独立} \\end{aligned} $$ 则称 N 个事件 $A_1, A_2, \u0026hellip;, A_N$ 相互独立。\n多个事件间相互独立 \u0026lt;=\u0026gt; 多个事件间两两独立、三三独立、……、NN 独立\n概率为 0 的事件与任何事件都独立。\n将相互独立事件中的任一部分转换为对立事件，所得诸事件依然是相互独立的。\n如若事件 A 与事件 B 独立，则 $A$ 与 $\\bar{B}$, $\\bar{A}$ 与 $\\bar{B}$, $\\bar{A}$ 与 $B$ 均独立。\n多个随机事件情形同样满足\n随机试验层面的独立 独立性与相容性 相容性定义：如果 A 与 B 没有相同的样本点，则称 A 与 B 互不相容。即 A 与 B 不可能同时发生。\n==在事件概率不为 0 的前提==下讨论独立性与相容性的关系：\n独立 =\u0026gt; 相容：$P(AB)=P(A)P(B)≠0$ 说明 A 与 B 之间有交集，两者可能同时发生；但反之则不成立：相容事件不一定独立，例如掷一枚骰子，A: 点数\u0026lt;4 , B: 点数\u0026gt;3，AB 有交集，两者可能同时发生，相容，但不独立。 逆否命题：互不相容 =\u0026gt; 相依（不独立），互不相容 =\u0026gt; $P(AB)=0≠P(A)P(B)$ 说明 A 与 B 不独立。反之同样不成立：相依事件可能相容，例如掷两枚骰子，A：第一次为 6，B：总和大于 8，AB 两事件既相容又相依。 独立性：概率层面，反映前后实验结果是否相互影响； 相容性：事件层面，反映不同事件能否同时发生。\n可以说，两者其实是不同维度的概念。\n独立性与相关性 不相关是指两个变量的相关系数为0，$E(XY)=E(X)E(Y)$，两个变量之间没有线性关系。\n相关与否，仅是线性层面的。而独立与否，不仅包括线性层面，还包括非线性层面。\n于是可以说：独立的要求，比不相关，要更加严格。\n独立 =\u0026gt; 不相关，反之不成立，事件间不相关（无线性关系）≠\u0026gt; 事件间独立（可能存在非线性关系） 逆否命题：相关 =\u0026gt; 相依，反之同样不成立，相依（不独立，说明事件间有关系）≠\u0026gt; 相关（因为可能不是线性关系，而是非线性关系） ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%85%B3%E4%BA%8E%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%AD%E7%9A%84%E7%8B%AC%E7%AB%8B/","summary":"随机事件层面的独立 在概率论里，\u0026ldquo;独立\u0026quot;并不意味着两个事件没有任何关系。 独立意味着一个事件的发生与否都不改变另一个事件发生的概率。 - 知乎文章 数学定义见下： 两个随机事件间的独立性定义： $P(AB)=P(A)P(B)$，即 $P(A|B)=P(A)$ 。不独立，又称相依。 多个随机事件间的独立","title":"关于概率论中的独立"},{"content":"本文使用腾讯 ima.copilt 来搭建知识库\n背景 在准备复试时从小红书、闲鱼等各种渠道获取到很多复试资料。但是文件繁多，部分资料还存在重复问题，不想花太多时间在搜索信息上面，想到前几天在小红书看到很多人推荐用腾讯的 ima.copilt 来搭建知识库。于是灵机一动，便萌生了通过搭建知识库以提升信息检索效率的法子。\n具体操作步骤 首先进入 https://ima.qq.com/ 下载并安装软件客户端； 登录，进入知识库界面； 上传文件，等待系统自动解析； 通过 RAG 技术，ima 会根据你所上传的文件（即你搭建的知识库）来回答你问的问题。 🙏 希望能够顺利进入复试\n","permalink":"https://20250303.xyz/posts/ai%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E4%BD%BF%E7%94%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%8F%90%E5%8D%87%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E5%87%86%E5%A4%87%E6%95%88%E7%8E%87/","summary":"本文使用腾讯 ima.copilt 来搭建知识库 背景 在准备复试时从小红书、闲鱼等各种渠道获取到很多复试资料。但是文件繁多，部分资料还存在重复问题，不想花太多时间在搜索信息上面，想到前几天在小红书看到很多人推荐用腾讯的 ima.copilt 来搭建知识库。于是灵机一动，便萌生了通过搭建知识库以提升信息检索效率的法子。 具体操作","title":"使用知识库提升考研复试准备效率"},{"content":"本人通过 Obsidian + Hugo + Github Page + Github Action 来实现文章管理及博客自动化搭建.\nHugo 渲染白名单 因为不想将 Obsidian 仓库中的一些隐私笔记发布到博客上, 于是选择使用 module.mounts 来配置允许 Hugo 渲染的文件夹.\n1module: 2 mounts: 3 - source: \u0026#34;content/posts/3. Resources-资源 未来/AI人工智能\u0026#34; 4 target: \u0026#34;content/posts/AI人工智能\u0026#34; 5 - source: \u0026#34;content/posts/4. Archives-归档/技术类归档\u0026#34; 6 target: \u0026#34;content/posts/技术类归档\u0026#34; 7\t...... 8 - source: \u0026#34;content/about.md\u0026#34; 9 target: \u0026#34;content/about.md\u0026#34; 10\t...... Hugo 主页展示白名单 因为主要想在博客主页展示技术类相关博文, 所以需要对主页展示内容进行过滤, 在 layouts/_default/list.html 进行配置即可, 具体可参考下面的代码:\n1{{- if .IsHome }} 2{{- $pages = where site.RegularPages \u0026#34;File.Dir\u0026#34; \u0026#34;in\u0026#34; (slice \u0026#34;posts\\\\AI人工智能\\\\\u0026#34; \u0026#34;posts\\\\技术类归档\\\\\u0026#34;) }} 3{{- $pages = where $pages \u0026#34;Params.hiddenInHomeList\u0026#34; \u0026#34;!=\u0026#34; \u0026#34;true\u0026#34; }} 4{{- end }} 倘若不知道文件夹(File. Dir)路径, 尤其是我这种有 mounts 映射的情况, 可以通过在 layouts/_default/list.html 中添加 {{ range site.RegularPages }} \u0026lt;p\u0026gt;{{ .File.Path }} → {{ .File.Dir }}\u0026lt;/p\u0026gt; {{ end }} 代码块, Hugo 会输出文件夹 (File. Dir) 路径 (类似 Python 的 print 函数.), 见下图:\n踩坑记录: 一开始以为在 config.yml 里面设置 mainSections 相关参数即可, 折腾半天. 后面静下心来慢慢读 Hugo 的官网网站上的相关教程, 最后也多亏了 GPT 帮忙.\n上传 Github 白名单 可通过 .gitignore 文件进行设置.\n1 2content/posts/.obsidian/ # .obsidian文件夹内里面有很多杂七杂八的东西 3content/posts/-1. Books/ # 这一目录用于存放电子书, 占用空间 4 5.history/ # VSC的自动保存 6 7public # 无需上传public文件, 因为设置了Github Action自动部署 这个网上很多大佬都有教程 ","permalink":"https://20250303.xyz/posts/%E6%8A%80%E6%9C%AF%E7%B1%BB%E5%BD%92%E6%A1%A3/hugo-%E6%B8%B2%E6%9F%93%E5%8F%8A%E4%B8%BB%E9%A1%B5%E5%B1%95%E7%A4%BA%E7%99%BD%E5%90%8D%E5%8D%95/","summary":"本人通过 Obsidian + Hugo + Github Page + Github Action 来实现文章管理及博客自动化搭建. Hugo 渲染白名单 因为不想将 Obsidian 仓库中的一些隐私笔记发布到博客上, 于是选择使用 module.mounts 来配置允许 Hugo 渲染的文件夹. 1module: 2 mounts: 3 - source: \u0026#34;content/posts/3. Resources-资源 未来/AI人工智能\u0026#34; 4 target: \u0026#34;content/posts/AI人工智能\u0026#","title":"Hugo 渲染及主页展示白名单"},{"content":"参考: # hexo之mongodb修改twikoo评论的管理密码 以及 # Twikoo 找回暗号及密码\n核心目标: 把 MongoDB 中的\u0026quot;ADMIN_PASS\u0026quot;值删除.\n","permalink":"https://20250303.xyz/posts/%E6%8A%80%E6%9C%AF%E7%B1%BB%E5%BD%92%E6%A1%A3/%E5%BF%98%E8%AE%B0-twikoo-%E7%AE%A1%E7%90%86%E9%9D%A2%E6%9D%BF%E7%9A%84%E5%AF%86%E7%A0%81%E6%80%8E%E4%B9%88%E5%8A%9E/","summary":"参考: # hexo之mongodb修改twikoo评论的管理密码 以及 # Twikoo 找回暗号及密码 核心目标: 把 MongoDB 中的\u0026quot;ADMIN_PASS\u0026quot;值删除.","title":"忘记 Twikoo 管理面板的密码怎么办"},{"content":"记得在看《被讨厌的勇气》后，里面有一句话让我印象特别深刻：“影响你的不是事件本身，而是你==看待事件的方式==。”比如精神内耗与否，就与==如何看待事情==息息相关[[关于精神内耗#内耗和复盘]]。再比如经验不是事情本身, 是我们理解事情的方式, 构成了经验。但是需要注意的是，==不要脱离或弱化了“物质的决定性”==。\n","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E4%B8%BB%E8%A7%82%E8%83%BD%E5%8A%A8%E6%80%A7/","summary":"记得在看《被讨厌的勇气》后，里面有一句话让我印象特别深刻：“影响你的不是事件本身，而是你==看待事件的方式==。”比如精神内耗与否，就与==如何看待事情==息息相关[[关于精神内耗#内耗和复盘]]。再比如经验不是事情本身, 是我们理解事情的方式, 构成了经验。但是需要注意的是，==不","title":"关于主观能动性"},{"content":" 如果你因为失去了太阳而流泪，那么你也将失去群星。 - 泰戈尔\n什么是精神内耗 精神内耗表现为个体对未知不确定事件可能的消极原因或后果进行反复的揣摩、思考与分析。 - 百度百科\n心理内耗是指人的自我控制需要消耗心理资源，当资源不足时，个体即处于一种所谓内耗的状态，长期的内耗会让人感到疲惫。 - MBA智库\n内耗和复盘 精神内耗 Vs 真正的复盘，他们之间区别在于：==精神内耗==是过度的无用思考，不断地==反刍==：“我当时为什么要这样做”；==真正的复盘==，虽然也会分析当时做错事的原因，但其重点是：以后==怎么做==以避免再犯类似的错误。 ^ah26bl\n如何应对精神内耗 不要试图去压制自己的反刍思维，这样反而可能会因为白熊效应让负面思考变得更加频繁，这可能会形成恶性循环。\n白熊效应，又称白象效应或反弹效应：刻意抑制某些想法时，实际上会使这些想法更容易浮出水面。一个例子是，当某人积极地试图不去想一只白熊时，他实际上更有可能想象一只白熊。 - Wikipedia\n具体怎么做：\n自我接纳（只要你完成了今天的目标，你就问心无愧。其它的事，无需今日承担。 - 小红书评论） 旁观视角（分清情绪和问题） 积极行动（行动是治愈反刍的良药） 有意识地转移注意力（如正念冥想、心理干预）等等 ","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E7%B2%BE%E7%A5%9E%E5%86%85%E8%80%97/","summary":"如果你因为失去了太阳而流泪，那么你也将失去群星。 - 泰戈尔 什么是精神内耗 精神内耗表现为个体对未知不确定事件可能的消极原因或后果进行反复的揣摩、思考与分析。 - 百度百科 心理内耗是指人的自我控制需要消耗心理资源，当资源不足时，个体即处于一种所谓内耗的状态，长期的内耗会让人感到疲惫。 - MB","title":"关于精神内耗"},{"content":"\u0026ldquo;矛盾\u0026quot;的意思即\u0026quot;两面性\u0026rdquo;(道家的\u0026quot;阴阳\u0026quot;)?\n思考的两面性 - 人类认知边界 × 语言符号系统 思考的正面性，或者通俗来说，思考的优点，是显而易见的。\n下面主要来说说==思考的反面性==：\n语言的界限就是世界的界限。 - 《维特根斯坦与哲学》 知乎的一个解读 没有语言，人类就无法思考。 - 索绪尔 知乎对索绪尔或语言学的一个介绍 道可道，非常（恒）道。 - 老子《道德经》\n结构主义认为语言的结构决定了我们的思维模式。\nearly-nineteenth-century science was in the grip of philosophical determinism—the belief that everything that happens is determined in advance by the initial conditions of the universe and the mathematical formulas that describe its motions. - The lady tasting tea. 也是结构主义？\n是的，但传统结构主义主要应用于语言学、人类学、文化研究等，主要强调符号和意义的系统性。\n深入了解一下结构主义, 索绪尔, 列维斯特劳斯 #哲学 我们只能想到我们能够想到的东西（意象无法表达）, 或者说，我们只能表达出语言能够表达出地东西，只能思考『能够被语言表达』的东西。语言之外的东西，我们无法思考。就好比盲人无法思考光明、聋人无法思考声音、人类无法思考高维空间（“上帝面前，我们都是聋子”）\n人类在认知进程中不断重构\u0026quot;可言说\u0026quot;与\u0026quot;不可言说\u0026quot;的边界形态，例如随着阅历或认知的进长，我们渐渐能够表达从前无法用语言表达的想法或观点，但是，这个边界本身是永远不能被消除的。\n《庄子·内篇·养生主第三》中的哲思：\u0026ldquo;吾生也有涯，而知也无涯。以有涯随无涯，殆已！\u0026quot;，人类的伟大恰在于明知\u0026quot;殆\u0026quot;而仍不懈追寻。\nDDL 的双面性 设计应该是感性的还是理性的？设计是感性重要还是理性重要？ 此类问题一经出现便会引起争议无数，每个人对此都有自己的见解与理由，还有些人主张“理性与感性需要平衡”“既要理性也要感性”，==此类观点看似正确，却缺乏任何实质性的指导价值==。 - 设计的两面性：理性决策与感性表达\n马哲所谓的\u0026quot;辩证分析法\u0026rdquo;, 听上去似乎正确, 但实际上, 就像上面引用的文章所提到的: 缺乏实质性的指导价值.\n","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E9%A9%AC%E5%93%B2%E4%B8%AD%E7%9F%9B%E7%9B%BE%E4%B8%8E%E9%98%B4%E9%98%B3/","summary":"\u0026ldquo;矛盾\u0026quot;的意思即\u0026quot;两面性\u0026rdquo;(道家的\u0026quot;阴阳\u0026quot;)? 思考的两面性 - 人类认知边界 × 语言符号系统 思考的正面性，或者通俗来说，思考的优点，是显而易见的。 下面主要来说说==思考的反面性==： 语言的界限就是世界的界限。 - 《维特根斯坦","title":"关于马哲中矛盾与阴阳"},{"content":"《湖南农民运动考察报告》 矫枉必须过正, 不过正不能矫枉.\n政权、族权、神权、夫权这四种权利代表了全部封建宗法的思想与制度.\n引而不发，跃如也。 - 农民的使命要农民去完成, 别人代庖是不对的\n《井冈山的斗争》 民主主义: 官长不打士兵, 官兵待遇平等, 士兵有开会说话的自由, 废除繁琐的礼节, 经济公开.\n家族主义: 一个村子一个姓, 支部会议简直同时就是家族会议.\n群众的政治训练 - 群众需要有政治素养\n党与政府的关系\n民权主义革命 - 民权主义是三民主义之一.\n《关于纠正党内的错误思想》 ==本位主义==, 一切只知道为四军打仗, 不知道武装地方群众是红军的重要任务之一. 这是一种放大了的小团体主义.\n俘虏兵的加入, 带来了浓厚的雇佣军队的思想, 使单纯军事观点有了==下层基础==.\n过分相信军事力量, 而不相信人民群众的力量.\n党对军事工作没有积极的注意和讨论, 也是形成一部分同志的单纯军事观点的原因.\n==关于极端民主化==, 『由下而上的民主集权制』, 『先交下级讨论, 再交上级决议』?\n有争论的问题, 要把是非弄明白, 不要调和敷衍.\n党内批评, 不要成了攻击个人.\n绝对平均主义的来源, 和政治上的极端民主化一样, 是手工业者和小农经济的产物, 不过一则见之于政治方面, 一则见之于物质生活方面罢了.\n看这篇文章有看加缪的《堕落》的感觉，被作者当面揭露出劣根性的感觉。原来自己曾经有过的许多想法，其实都有一个专有名词（如 xx 主义），一下抓住事情的本质并提炼成文字，伟人果然是伟人。\n","permalink":"https://20250303.xyz/posts/%E9%98%85%E8%AF%BB/%E6%AF%9B%E9%80%89%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","summary":"《湖南农民运动考察报告》 矫枉必须过正, 不过正不能矫枉. 政权、族权、神权、夫权这四种权利代表了全部封建宗法的思想与制度. 引而不发，跃如也。 - 农民的使命要农民去完成, 别人代庖是不对的 《井冈山的斗争》 民主主义: 官长不打士兵, 官兵待遇平等, 士兵有开会说话的自由, 废除繁琐的礼节, 经济公开.","title":"毛选读书笔记"},{"content":"本地部署都很吃配置\n知识库 KB 知识库（Knowledge base）是用于知识管理的一种==特殊的数据库==，以便于有关领域知识的采集、整理以及提取。知识库中的知识源于领域专家，它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。 - Wikipedia\n例如：法院判例库、医院病例库、产品数据库、某领域论文库、哈利波特魔咒库、章鱼哥的食谱……\n注意！知识库与 LLM 预训练语料库两者不是一个概念，预训练语料库为非结构化文本（乱七八糟的文字, 未清洗）, 且一旦模型预训练完成就不能再更改预训练语料 (如ChatGPT 4o的预训练语料库便截止2023年10月); 而知识库则为结构化或半结构化文本 (有一级标题、二级标题或其他逻辑结构)，且是定期更新与维护的。\n看到很多人推荐 IMA 来搭建知识库.\n智能体 Agent 智能体（英语：intelligent agent）指一个可以==观察周遭环境并作出行动以达致目标==的自主实体。它通常是指（但不一定是）一个软件程序。“智能体”是目前人工智能研究的一个核心概念，统御和联系着各个子领域的研究。 - Wikipedia\n智能体（Agent）是指能够==感知环境并采取行动以实现特定目标==的代理体。它可以是软件、硬件或一个系统，具备自主性、适应性和交互能力。智能体通过感知环境中的变化（如通过传感器或数据输入），根据自身学习到的知识和算法进行判断和决策，进而执行动作以影响环境或达到预定的目标。 - 百度百科\n常见 AI Agent 平台: 字节扣子 (Coze)、腾讯元器、Dify、FastGPT\n所有妄图通过缩短工序而提升效率的低代码平台都会死. (Agent 平台就是 LLM 时代的低代码平台) - 小红书评论\n检索增强生成 RAG Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so ==it references an authoritative knowledge base outside of its training data sources before generating a response.== Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. ==RAG extends the already powerful capabilities of LLMs to specific domains or an organization\u0026rsquo;s internal knowledge base, all without the need to retrain the model.== It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. - AWS\n==检索增强生成==（英语：Retrieval-augmented generation, RAG ) 是赋予生成式人工智能模型信息检索能力的技术。检索增强生成优化大型语言模型(LLM) 的交互方式，让模型根据指定的一组文件回应用户的查询，并使用这些信息增强模型从自身庞大的静态训练数据中提取的信息。检索增强生成技术促使大型语言模型能够使用特定领域或更新后的信息。 - Wikipedia\n可以将RAG理解为\u0026quot;知识库+LLM\u0026quot;, \u0026ldquo;==AI 老员工==\u0026ldquo;的比喻十分形象。\n主流 RAG 框架: LangChain、RAGFlow、AnythingLLM\nRAG 具体工作逻辑 参考 Why does the LLM not use my documents?\nRAG 对知识库中的文件，并不是以一个文档一个文档地对待的，而是将所有文档都“打碎”并进行词嵌入； 当用户发起对话时，也将用户发送的对话内容“打碎”并进行词嵌入； 通过余弦相似度从知识库中寻找与用户发送内容“相似”的“碎片”。 整合 LLM，最后输出 RAG (which is what we use) enables us to chunk the document and then ask retrieve only the bits and pieces the make sense for your question and use that in the context window. This makes larger documents easier to use, but it is at the expense of these types of \u0026ldquo;whole document\u0026rdquo; understandings.- AnythingLLM 社区 RAG, by its very nature is pieces of relevant content. Not the entire text - AnythingLLM 社区\n总结：RAG 的作用不是全文理解，而是从知识库找到与我们所问的问题相关的“碎片”。至于全文理解，这件事是智能体干的事情，当然 LLM 也干得不错。为什么不本地部署一个强悍的 LLM ? 可以参考这里[[#总结]].\n微调 Fine-tuning 微调（又称大模型微调，英语：fine-tuning）是==深度学习中迁移学习的一种方法==，其中预训练模型的权重会在新数据上进行训练。微调可以在整个神经网络上执行，也可以仅在其部分层上执行，此时未进行微调的层会被“冻结”（在反向传播步骤中不更新） - Wikipedia 微调通常通过==监督学习==完成，但也有使用弱监督进行模型微调的技术。 - Wikipedia Low-rank adaptation (==LoRA==) is an adapter-based technique for efficiently fine-tuning models. - Wikipedia\nLLM 总不能样样精通.\n可以理解为：通过利用已有的知识来提高模型在新任务上的表现。\n任何预训练好的模型都能微调！预训练-微调方法属于基于模型的迁移方法（Parameter/Model-based TransferLearning）\n总结 角色 作用 依赖 知识库 ==专业知识库==，提供事实性、权威性的知识数据 被RAG或智能体查询 智能体 ==Agent==，负责与用户交互，并调用RAG或知识库来回答问题 可能使用RAG提升回答能力 RAG ==一种AI架构==，从知识库检索信息，并结合语言模型生成答案 需要知识库作为数据来源 微调 ==迁移学习的一种方法==，使通用模型适配特定领域任务 需要知识库作为微调数据 形象的比喻：微调是考前复习，RAG 是开卷考试。\n然而本地部署并微调一个大模型，对于个人来说成本是极大的；如果退而求其次，微调一个小一点的模型，效果又可能不好；所以实践证实:==在本地部署一个蒸馏版的大模型+RAG 技术是一个更优的解决方案==。\n","permalink":"https://20250303.xyz/posts/ai%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%99%BA%E8%83%BD%E4%BD%93rag%E5%BE%AE%E8%B0%83/","summary":"本地部署都很吃配置 知识库 KB 知识库（Knowledge base）是用于知识管理的一种==特殊的数据库==，以便于有关领域知识的采集、整理以及提取。知识库中的知识源于领域专家，它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。 - Wikipedia 例如：法院判例库、医院病例库、产品数据","title":"知识库、智能体、RAG、微调"},{"content":"✨ 关于我 ✨\n本科统计学在读，对数据科学感兴趣。兴趣广泛，涉猎天文、业余天文摄影、音乐\n目前正在准备考研复试 / 春招\n📚 这里有什么 📚\n▫️ 数据科学相关：统计学、机器学习的学习笔记\n▫️ 人工智能相关：记录捣鼓 AI 过程中的踩坑经历以及灵感\n▫️ 一些阅读笔记：偶尔写些读书记录\n欢迎同好一块交流 🤝\n","permalink":"https://20250303.xyz/about/","summary":"✨ 关于我 ✨ 本科统计学在读，对数据科学感兴趣。兴趣广泛，涉猎天文、业余天文摄影、音乐 目前正在准备考研复试 / 春招 📚 这里有什么 📚 ▫️ 数据科学相关：统计学、机器学习的学习笔记 ▫️ 人工智能相关：记录捣鼓 AI 过程中的踩坑经历以及灵感 ▫️ 一些阅读笔记：偶尔写些读书记录 欢迎同好一块交流 🤝","title":"关于我"}]