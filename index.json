[{"content":" 德国学者 Gauss (1777‐1855) 于 1809 年提出最小二乘法。 英国遗传学家 Galton (1822‐1911) 于 1886 年发表关于回归的开山论文 《遗传结构中向中⼼的回归》Regression towards Mediocrity in heredity structure)。[[统计学史#^tp89te]] 回归分析是处理变量之间的==相关关系==的⼀种统计⽅法和技术。\n相关分析：⽤⼀个指标 (相关系数) 来表明现象间相互依存关系的密切程度。以现象之间是否相关、相关的⽅向和密切程度等为研究内容，不区分⾃变量和因变量，不关⼼相关关系的表现形态。 回归分析：对具有相关关系的现象，根据其相关关系的具体形态，选择⼀个合适的数学模型 (回归⽅程) 来近似地表达变量间的平均变化关系。 相关分析是回归分析的基础和前提；回归分析是相关分析的深⼊和继续。 ⼴义的相关分析包括回归分析。 模型假设 样本回归模型 $y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i$. 理论回归模型 $y=\\beta_0+\\beta_1x+\\varepsilon$. 经验回归方程 $\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1x$. 回归模型 $E(y|x)=\\beta_0+\\beta_1x$ 从平均意义上表达了变量 $y$ 与 $x$ 之间的统计规律性 如何理解回归模型中的条件期望？在给定解释变量 $X$ 取特定值 $x$ 时，被解释变量 $Y$ 的平均值。 回归分析的基本假定：\n因变量和自变量之间存在线性关系 观测值应该相互独立，避免自相关问题 残差应该近似服从正态分布，这一假定有助于进行统计推断和构建置信区间 残差的方差在不同自变量取值的情况下应该大致相等，避免异方差问题 自变量之间应该是线性无关的，避免多重共线性问题 自变量 x 为非随机变量 可以通过残差分析、散点图、Q-Q图等方法来检验这些基本假定的成立程度\n⚠️G-M 假设不要求扰动项服从正态分布！\n在高斯马尔科夫条件下，最小二乘估计是在所有线性无偏估计中具有最小方差的，被称为BLUE（Best Linear Unbiased Estimators）\n正态性假设: $\\varepsilon_i \\sim N(0, \\sigma^2)$, 且 $\\varepsilon_i$ 间相互独立。\n正态假定下 MLE 与 OLSE 等价，MLE最大化似然函数，OLSE最小化损失函数。\n⼀元线性回归中⾃变量和残差的关系：在⼀元线性回归模型中， ⾃变量 (X) 和残差 (ε) 理论上应该是独⽴的。残差代表了实际观测值与通过回归线预测值之间的差异。如果⾃变量和残差不独⽴，可能表明模型中存在遗漏变量或⾮线性关系没有被正确模型化。\n最小二乘估计和最大似然估计是否都符合无偏性？回归系数均无偏，方差均无偏（需修正）\n模型的显著性检验 F 检验：整体回归模型的显著性检验。先进行 F-test 检验整体，再使用 t-test 检验单个。 t 检验：单个回归系数的显著性检验； 相关系数的显著性检验（或称样本决定系数的显著性检验）, 利用 $r^2=\\frac{F}{F+(n-2)}$. 以上三个检验再一元线性回归时是等价的，但在多元线性回归场合，经推广 F 检验仍可用，另两个检验就无法使用了。\n违背基本假设的几种情况 相较于一元线性回归，多元线性回归面临着额外的几个问题：\n多重共线性； 变量数量变多时，易导致模型复杂度增加，易造成过拟合，影响模型泛化能力； 各变量尺度不一样，需要进行数据标准化； 变量之间可能存在非线性关系或交互作用； 1. 多重共线性\n定义：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。\n原因：经济变量之间的共同变化趋势、利用截面数据建立模型的问题、模型中包含滞后变量、样本自身数据的原因。\n问题：$\\beta$ 方差 $\\sigma^2(X\u0026rsquo;X)^{-1}$ 偏大，估计精度低，不稳定，t 统计量↓，不易拒绝 $H_0$.\n诊断：容忍度 $1-R_j^2$ ＜0.1、方差扩大因子 $VIF_j=\\frac{1}{1-R_j^2}≥10$、条件数 $=\\frac{\\lambda_{max}}{\\lambda_{min}}≥100$、直接看特征根（是否接近 0）\n解决⽅法：可以通过以下方法来解决：\n变量选择； 增加样本观测量：对时间序列资料就是增大观测次数，对截面数据资料就是增加观测对象； 利用先验信息：把事先知道的关系包含近回归模型中； 横截面数据和时间序列数据并用。 使⽤主成分回归（PCR），$X$ 中有多重共线性的变量，那就不使用 $X$ 来进行回归分析，转而==使用 $X$ 对应的主成分矩阵==。 偏最小二乘法：相较于通过寻找响应和独立变量之间最小方差的超平面，偏最小二乘法通过==投影预测变量和观测变量到一个新空间==来寻找一个线性回归模型。 岭回归（$X\u0026rsquo;X$ 不可逆？那就==加上一个正则项== $X\u0026rsquo;X+kI$ 这样就可逆了），代价是：有偏。通过岭迹图来确定 $k$ 值岭迹图图片 Lasso 回归。详见下图： 图源博客园博文 注意：不是说一个模型有多重共线性情况，那这个模型就完全不行。如果说是利用模型去做经济结构分析，那么要尽可能避免多重共线性；==而如果说只是利用模型去做经济预测，只要保证自变量的相关类型在未来时期中保持不变，即使回归模型中含有严重多重共线性的变量，也可以得到较好的预测结果。==\n2. 异⽅差性\n原因：省略重要解释变量、测量误差、模型函数设定错误、截面数据中总体各单位的差异。 问题： 虽参数仍是无偏估计（OLSE 的无偏性仅依赖于随机误差项的零均值假设），但不是 MVUE（依赖 G-M 条件）； 参数方差的估计量是有偏的（虽然就算没有出现异方差情况也是无偏的）； 显著性检验失效，回归方程应用效果不理想。 诊断：绘制残差图、斯皮尔曼等级相关系数检验、white 检验 解决⽅法：可以使⽤加权最小二乘法，通过给不同的观测值赋予不同的权重 $w_i$，使误差较大的样本权重较小，从而改进估计；或对变量进⾏变换（BOX-COX）。 3. ⾃相关\n产生原因：被解释变量的自相关、遗漏关键变量、随机误差项的自相关、经济变量的滞后性（惯性）、回归函数的错误、蛛网现象、不恰当的数据预处理方式（如差分变换） 问题：斜率系数的最小二乘估计量仍然线性无偏（参数OLSE 的无偏性仅依赖于随机误差项的零均值假定），但参数方程仍然有偏；预测的精确度降低；t 值↑，易犯拒真错误；估计值不再是 MVUE；MES 严重↓ 诊断：绘制 $e_i, e_{i-1}$ 散点图、绘制 $e_t$ 时间序列图、自相关系数绝对值接近 1、DW 值离 2 的距离越远自相关越严重。 解决⽅法：迭代法、差分法、广义最小二乘法（GLS）、时间序列模型（如 ARIMA）或对数据进行BOX-COX 变换。 4. ⾮线性关系\n问题：如果因变量与⾃变量之间的关系是⾮线性的，线性回归模型⽆法准确描述这种关系。 解决⽅法：可以使⽤⾮线性回归模型，或者对变量进⾏变换（如对数变换、多项式回归）。 5. 数据量不⾜\n问题：如果样本量太少，回归模型的参数估计可能不准确，模型容易过拟合。 解决⽅法：增加数据量，或者使⽤正则化⽅法（如 Lasso 回归）来防⽌过拟合。 6. 忽略重要变量\n问题：如果模型中遗漏了重要的⾃变量，会导致回归系数估计有偏。 解决⽅法：通过领域知识或变量选择⽅法（如逐步回归）来识别重要变量。 7. 异常点、高杠杆点与强影响点\n注意：异常点不一定是强影响点，强影响点也不一定是异常点、高杠杆点不一定是强影响点，强影响点也不一定是高杠杆点。 问题：异常值会对回归模型的拟合产⽣较⼤影响，导致模型失真。 解决⽅法：可以通过剔除异常值或使⽤稳健回归⽅法。 诊断： 参考 判断异常点，异常值点：对既定模型偏离很⼤的数据点 标准化残差 $ZRE_i=\\frac{e_i}{\\hat{\\sigma}}$ 绝对值＞3 学生化残差 $SRE=\\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}$ 绝对值＞3, 相较于标准化残差，学生化残差剔除了高杠杆值的影响； 判断高杠杆点：杠杆值 $h_{ii}=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{\\sum_{j = 0}^{n} (x_j-\\bar{x})^2}$, ==高杠杆点≠强影响点≠异常点==，在⾃变量空间中远离数据中⼼的点，有==把拟合直线拖向⾃⼰==的倾向，称之为⾼杠杆点 判断强影响点：库克距离 $D_i=\\frac{\\sum_{j = 0}^{n} (y_j-y_{j(i)})^2}{p ·MSE}$ ＞0.5, 度量去除某一数据点后其它样本拟合值的变化。强影响点是对统计推断产⽣较⼤影响的数据点，删除该点会导致拟合模型的实质性变化，如参数估计值、拟合值或检验值发⽣较⼤变化。==库克距离是一个综合指标，结合了杠杆值和残差大小，用来评估数据点对回归系数估计的整体影响。== 变量选择中评价模型的指标 自由度调整复决定系数越大越好； AIC 和 BIC：这两个准则都是基于似然函数，考虑了模型复杂度的惩罚。在⽐较多个模型时，AIC 和BIC 较⼩的模型被认为更优； ^grw9h5 $C_p$ 统计量越小越好； 调整 $R^2$：对 $R^2$ 进⾏调整，考虑了模型中变量的数量。在模型中添加更多变量时，即使这些变量对模型的贡献很⼩，$R^2$ 也可能会增加。调整后的 $R^2$ 提供了⼀个更为准确的衡量标准。 残差图分析：通过观察残差（实际值与预测值之差）的分布情况，可以评估模型是否满⾜线性回归的假设，例如残差的独⽴性、正态性和⽅差⻬性。 下面这几个有些许风险：选择解释变量时不应以 SSE 为标准，因为变量↑，SSE 一定↓（想象过拟合，调整 $R^2$ 就解决了这个问题），可以看看各变量系数是否显著。 均⽅误差：计算模型预测值与实际值差值的平⽅的平均值。MSE 越⼩，模型的预测准确度越⾼ 。 均⽅根误差：MSE 的平⽅根。RMSE 是衡量模型预测误差的常⽤标准，越⼩表示模型预测越准确。 平均绝对误差：计算模型预测值与实际值差值的绝对值的平均值。MAE 提供了预测误差的另⼀种衡量，对异常值的敏感度低于 MSE。 决定系数：衡量模型预测值的变异性占总变异性的比例，值范围从 0 到 1。$R^2$ 值越接近 1 ，表示模型解释的变异性越⼤，拟合度越好。 选择合适的评估⽅法取决于具体的研究目的和数据特性。通常，结合使⽤多种评估指标可以更全⾯地了解回归模型的性能。\n❗注意弄清楚 F 检验统计量和 $R^2$ 的表达式: $F=\\frac{MSR}{MSE}$, $R^2=\\frac{SSR}{SST}$\n误差、残差与偏差 误差:观测值与真实值的偏离，如多次测量取平均值中的“测量误差”； 残差:观测值与模型估计值的偏离，理想情况下，如果模型是最优的，并且符合假设（如线性回归假设误差是独立同分布的），那么残差可以被视为==随机噪声==。但如果模型存在欠拟合、遗漏变量、异方差性等问题，==残差可能不仅仅是随机噪声，而是包含了系统性误差==。 偏差:观测值与模型估计值的偏离，排除噪声的影响，偏差是模型无法准确表达数据关系导致，比如模型过于简单，非线性的数据关系采用线性模型建模等，反映模型本身的精确度。 注：左图来自论文，右图来自文章\n针对欠拟合和过拟合的处理方式如下：\n欠拟合：偏差过大，做特征工程、减小(弱) 正则化系数； 过拟合：方差过大，可增加样本、减少特征、增加(强)正则化系数； 过拟合会导致方差偏大，核心原因在于模型对训练数据的依赖过强，从而导致其泛化能力下降。在偏差-方差分解（Bias-Variance Tradeoff）中：$\\mathbb{E}[(\\hat{y} - y)^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Noise}$\n过拟合时，模型在训练集上的偏差几乎总是很小，而在验证集上，偏差可能不明显，但高方差会导致测试误差增大。\n过拟合会导致方差偏大的本质原因是模型对训练数据的学习过度，使得它对新数据的预测波动性增加，从而导致测试误差的不稳定性。这也就是为什么在深度学习或机器学习任务中，我们通常需要正则化（如L1/L2正则化、Dropout）、数据增强或早停（Early Stopping）等方法来降低方差，提高泛化能力。\n残差分析中的 QQ 图：用于对残差进行正态性检验（P-P图和Q-Q图都是用于检验样本的概率分布是否服从某种理论分布。一定要严格落在对角线上吗？普通直线行不行？Ans: 行！残差近似于⼀条直线：说明残差的分布与正态分布⾮常接近，即残差基本服从正态分布）\n各种“系数”概念辨析 决定系数（又称拟合优度、判定系数） $R^2 = \\frac{SSR}{SST} = \\frac{\\sum_{i=1}^{n} (\\hat{y}_i - \\overline{y})^2}{\\sum_{i=1}^{n} (y_i - \\overline{y})^2}$. 它指的是回归平方和占总平方和的比重, 表示自变量对因变量随机性的解释比例, 同时也反映了自变量和因变量的线性相关性强弱。\n分为一元和多元两个情形讨论。\n一元情形 Pearson 相关系数的平方 $r^2$ = 决定系数 $R^2$ （又称拟合优度、判定系数）\n多元情形 偏相关系数：在控制其他变量的影响后，两个变量之间的净相关关系。\n复相关系数 R：因变量 Y 与多个自变量的线性组合之间的相关系数。\n决定系数 $R^2$ =复相关系数 R 的平方。\n区间估计 \u0026amp; 区间预测 有截距项 Vs 无截距项 有截距项回归模型残差和为 0；无截距项回归模型残差和不一定为 0\n下面讨论方差分析自由度，其中 k 为自变量个数。⚠️==不同教材对 k 的定义可能不一样！==\n有截距项方差分析，实际参数为 k+1 个(截距项)，自由度: SST: n-1, SSR: k, SSE: n-k-1 无截距项方差分析，实际参数就是 k 个，自由度: SST: n, SSR: k, SSE: n-k 在回归分析中，如何处理无序变量？ 分两种情况：自变量是无序变量、因变量是无序变量（Logistic 回归，在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法）。\n下面详细写写自变量是无序变量的情形：\n处理无序变量，核心思想就是“编码”，参考 解决（几乎）任何机器学习问题：处理分类变量篇（上篇）\n设置哑变量。哑变量：又称为虚拟变量、虚设变量或名义变量。对于有n个分类属性的自变量，通常需要选取1个分类作为参照，因此可以产生n-1个哑变量。==哑变量即 One-Hot Encoding!==\nOne-Hot Encoding：这是最基础也是最常见的转换手段之一。它会给每个类别创建一个新的二进制列，并标记是否存在该值(0 或者 1)。然而这种方法可能会导致维度爆炸的问题，在类别过多的情况下生成大量的新特性。\n下面的其它 Encoding 方法先浅浅涉猎，在归纳机器学习的时候再深入了解。\nBinary Encoding; BaseN Encoding; Target Encoding / Mean Encoding; Feature Hashing (Hash Trick). 写在后面 - 区分因果关系和相关关系 因果关系的证明通常不是来自于单独的统计检验，而是来自于谨慎的实验设计。\n鉴别因果性最好和最科学的方法就是控制变量与盲测，例如美国FDD药品测试的随机双盲测试。\n图灵奖得主、“贝叶斯网络之父” Judea Pearl写了一本书，《为什么：关于因果关系的新科学》, 阐述了因果之梯：从低到高分别是：关联、干预、反事实推理。\n第一层级是关联（Association），即相关关系：“事件A发生时，事件B也发生”。注意：并不能得出事件之间的影响方向，比如是不是因为事件A的发生导致了事件B的发生。\n第二层级是干预（Intervention），当通过干预使事件A 发生改变时，事件B是否会跟着随之改变。\n第三层级是反事实（Counterfactual），借由想象（Imaging），也可以理解为“执果索因”、“以终推始”，可以想象下，如果想让事件B发生某种变化时，能否通过改变事件A来实现。\n参考这篇知乎文章。\n等复试完看看这本书 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/","summary":"德国学者 Gauss (1777‐1855) 于 1809 年提出最小二乘法。 英国遗传学家 Galton (1822‐1911) 于 1886 年发表关于回归的开山论文 《遗传结构中向中⼼的回归》Regression towards Mediocrity in heredity structure)。[[统计学史#^tp89te]] 回归分析是处理变量之间的==相关关系==的⼀种统计⽅法","title":"回归分析"},{"content":"假设检验是统计推断的重要方法！统计推断包括假设检验和参数估计。\n假设检验的定义 假设检验（hypothesis testing）是指从对总体参数所做的一个假设开始，然后搜集样本数据，计算出样本统计量，进而运用这些数据测定假设的总体参数在多大程度上是可靠的，并做出承认还是拒绝该假设的判断。如果进行假设检验时总体的分布形式已知，需要对总体的未知参数进行假设检验，称其为参数假设检验；若对总体分布形式所知甚少，需要对未知分布函数的形式及其他特征进行假设检验，通常称之为非参数假设检验。此外，根据研究者感兴趣的备择假设的内容不同，假设检验还可分为单侧检验（单尾检验）和双侧检验（双尾检验），而单侧检验又分为左侧检验和右侧检验。 - 国家统计局\n所谓假设检验，就是通过样本来推测总体是否具备某种性质。\n若假设可用一个参数的集合表示, 该假设检验问题称为参数假设检验问题, 否则称为非参数假设检验问题。 - 其它地方看到，感觉怪怪的。\n假设检验的原理 假设检验使用的是“证伪”的思想，是一种“反证法”。\n假设检验的基本思想是==反证法思想==和==小概率事件原理==。 反证法的思想是首先提出假设（由于未经检验是否成立，所以称为零假设、原假设或无效假设），然后用适当的统计方法，根据已有的样本，来确定假设成立的可能性大小，如果可能性小，则认为假设不成立，拒绝它；如果可能性大，还不能认为它不成立。\n小概率事件原理，是指小概率事件在一次随机试验中几乎不可能发生，小概率事件发生的概率一般称之为“显著性水平”或“检验水平”，用 $\\alpha$ 表示，而概率小于多少算小概率是相对的，在进行统计分析时要事先规定，通常取 $\\alpha=0.01、0.05、0.10$ 等。 - 国家统计局\n🤯反证法是数学上的严格证明方法，结论绝对正确；而假设检验属于统计学范畴，结论具有一定的概率风险，因此不一定正确。\n假设检验具体是如何操作的？ 代入给定的 $H_0$ 的参数值建立分布 看看样本“像不像”是从这个分布里头出来的（是否落入拒绝域） 若“像”（落入接受域），这帮样本就是从 $H_0$ 那个分布来的，说明 $H_0$ 对应的参数有可能就是真实参数； 若“不像”（落入拒绝域），好家伙这帮样本不是从 $H_0$ 那个分布来的，说明 $H_0$ 对应的参数大概率不是真实参数（拒绝 $H_0$）。但是 $H_0$ 错误，不代表 $H_1$ 就正确，所以只能说“拒绝 $H_0$”，而不能说“接受 $H_1$”） 如何量化这个“像”与“不像”，详见后面关于显著性水平 $\\alpha$ 和 P-value 的章节。 Over 下结论。 知道了假设检验具体是如何操作的，也就知道了:\n为什么假设检验要将等号放在原假设? 因为检验统计量的计算需要给定一个（假想的）参数值，否则就无法建立相应的分布，不方便计算。比如说，简单的单总体均值检验中，假定 $H_0: \\mu = 20$ 这样才能简单地把确定在这个假想均值确定的抽样分布下，把检验统计量给算出来。不带等号的话，这个分布的确定就困难了。\n假如，我偏不把等号放在原假设，把不等号放在原假设，例如 $H_0: \\mu \u0026lt; 20$ 我取 $\\mu$ 等于多少来建立分布呢？是 19？还是 15？亦或是 10？这时不就出现问题了。\n关于检验统计量的选取 分布是个好东西！\nThe statistical models of distributions, however, enable us to describe the mathematical nature of that randomness. - 《The lady tasting tea》\nEg. 现有一关于参数 $\\theta$ 的假设检验：==理论上==只要某统计量 T 的分布与参数 $\\theta$ 有关，即可用它来构造检验统计量。\n因为如果我们能得到 $H_0$ 成立下统计量 T 的分布，即可计算出在该分布中出现样本或其它更极端情况的概率。\n关键是：分布！\n检验统计量的选取 - 这个地方可以想个例子来帮助理解。 ✅ 2025-03-14 举个例子：考虑两样本量分别为 n, m 的独立正态总体，样本方差比检验 $H_0:\\sigma_1^2=\\sigma_2^2 \\quad Vs \\quad H_1:\\sigma_1^2 \u0026gt; \\sigma_2^2$. 此处使用下分位数。\n检验统计量 $T_1=\\frac{s_1^2}{s_2^2}=10$, 拒绝域 $W_1={T_1≥F_{1-\\alpha}(n-1, m-1)}$. 检验统计量 $T_2=\\frac{s_2^2}{s_1^2}=0.1$, 拒绝域 $W_2={T_2≤F_{\\alpha}(m-1, n-1)}$. 可以看到，选择不同的检验统计量，所构造出来的拒绝域的方向也不同！所以在实际假设检验中，对于“拒绝域的方向一般与 $H_1$ 方向相同”这样的结论要慎用。 ⚠️区分统计量和枢轴量（检验统计量也是统计量）：\n统计量不含参，但统计量的分布含参；如 $\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$; 枢轴量含参，但枢轴量的分布不含参。如 $Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}$. 枢轴量主要用于区间估计和假设检验中。 假设检验的精髓：不平衡（Imbalance） 假设检验中，原假设 $H_0$ 和备择假设 $H_1$ 的地位是不对等的：\n$H_0$ 通常是被默认为真的假设，需要足够的证据才能拒绝； 而 $H_1$ 则与 $H_0$ 相对立，通常是研究者试图去证明的假设。 打个比方：\n$H_0$ 像是“正统”，“权威”，“传统”，具有一定的“优先权”，默认情况下它都是正确的 而 $H_1$ 则像是“新教”，“创新”，甚至有些“备胎”的味道，需要在将 $H_0$ 拒绝后，才可能“喧宾夺主”，有“翻身”的机会。这里使用的“可能”、“机会”两词是严谨的，因为在实际的假设检验中： 哪怕拒绝了原假设，我们会说在 xx 的显著性水平下能够“拒绝 $H_0$”，而不是“接受 $H_1$”； 同样若没拒绝原假设，我们会说在 xx 的显著性水平下不能“拒绝 $H_0$”，而不是“接受 $H_0$”。 总结：判断一个假设是否『正确』是很难的，然而判断一个假设是否『错误』相对来说就容易多了：因为证伪只需要一个特例就足够了，==一个特例足以推翻一个论点，却远远不能支撑一个论点==。 之所以如此不平衡地设计“假设检验”这个东西，是为了给原假设一定“优先权”，这样有助于控制做出错误结论的风险（特别是第一类错误）。打个比方：“传统”的 $H_0$ 一般都是正确的，为防止左倾冒进分子冲动地推翻，于是在设计上给了它（即 $H_0$ ）一定的“优先权”。（备胎想要成为上位？很难的啦）\n矛盾无处不在，这种“不平衡”的设计，同样具有双面性：虽然确实有助于控制做出错误结论的风险，但如此不平衡的设计导致它并非使用于所有问题。如：\n现有一堆 0-2 之间的样本，判断是来自 $U(0, 1)$ 还是 $U(1, 2)$ 。这个问题就不适用于假设检验。因为在这个问题中 $U(0, 1)$ 和 $U(1, 2)$ 的“地位”是平等的。可通过极大似然估计 MLE 来解决这个问题。（分别计算样本在 $U(0, 1)$ 和 $U(1, 2)$ 的对数似然值即可，看哪个大）\n而像女士品茶（==将牛奶倒入茶==和==将茶倒入牛奶==对奶茶的口味是否有影响）这样的问题，显然是能够根据我们的先验知识做出一定猜测的，这种问题便可以通过假设检验这样的方法来解决。\n通过了解如何做出原假设，能够帮助你更好地理解假设检验的 Imbalance.\n做出原假设的依据 根据现有理论或知识：原假设往往基于现有的理论或广泛接受的知识。例如，如果现有理论表明两种药物效果相同，那么原假设可能就是\u0026quot;这两种药物的效果没有差异”。 简单性或保守性：在统计学中，原假设通常是一个简单假设，它提出了最简单、最保守的情况。例如，“新药与安慰剂效果无差别”是一个比“新药比安慰剂效果好”更简单、更保守的假设。 研究目的：研究者可能会根据研究目的来确定原假设。如果研究目的是证明一种新的干预措施有效，那么原假设可能就是“新干预措施与现有措施效果相同”。 来自北京师范大学-432统计学-2024年-解析\n那么问题来了：如何理解交换 $H_0$, $H_1$ 导致的检验结果变化呢？==体会其中的“不平衡”==\n交换 $H_0$ 和 $H_1$, 检验的方向发生了改变，==就代表研究者的立场发生了改变==，这样做将导致就同一样本数据得到的结论可能相同、但也可能相反。也就是：虽然数据还是来自于同个样本同样的数据，但原先用来支撑先前观点的论据，这时候可能就用来打脸了。\n假设检验中的显著性水平 $\\alpha$ 与功效 $1-\\beta$ 与 P-value 项目 无法拒绝 $H_0$ 拒绝 $H_0$ $H_0$ 为真 $1-\\alpha$ $\\alpha$ ==弃真错误== $H_0$ 为伪 $\\beta$ ==取伪错误== $1-\\beta$ ==功效== 用极限的思想来理解：在固定样本量的前提下，不能同时减小第一类错误 $\\alpha$ 和第二类错误 $\\beta$ ：\n若 $\\alpha=0$ -\u0026gt; 不犯弃真错误 -\u0026gt; 索性直接接受原假设 -\u0026gt; 更有可能犯 $\\beta$ 若 $\\beta=0$ -\u0026gt; 不犯取伪错误 -\u0026gt; 索性直接拒绝原假设 -\u0026gt; 更有可能犯 $\\alpha$ 整个实验来看：$\\alpha + \\beta \\in (0, 2)$ ，若某分类器将所有正例均判为负，将所有负例都判为正，这时 $\\alpha=\\beta=1, \\quad \\alpha+\\beta=2$ 单次实验结果来看，不可能同时发生弃真错误和取伪错误。表格中的四个值 $1-\\alpha, \\quad \\alpha, \\quad \\beta, \\quad 1-\\beta$ 均 $\\in (0, 1)$ 显著性水平 $\\alpha$ 在假设检验中，显著性水平（Significance Level），通常记为 $\\alpha$，是指在原假设 $H_0$ 为真时，错误地拒绝 $H_0$ 的概率，即第一类错误（Type I Error）的概率。换句话说，$\\alpha$ 代表了==我们能够接受的犯第一类错误的概率==（假阳性风险），即我们错误地发现了一个不存在的效应的可能性。\n两类错误，在具体实践中往往更加关注弃真错误 $\\alpha$ ，因为错误地拒绝“传统”、“权威”的原假设 $H_0$ （即弃真错误）所带来的损失，相对于在权威假设 $H_0$ 错误的背景下, 没能找到真正正确的假设 $H_1$，仍误认为权威假设 $H_0$ 是正确的（即取伪错误）而言，是更大的。\n例如，在制药行业，如果一个新药被批准（拒绝 $H_0$ ​）但实际上无效或有害，会带来巨大损失；相反，如果一个有效药物未被批准（即犯第二类错误），虽然可惜，但仍可在未来研究中重新评估。\nP值 P-value P-value：在原假设成立的条件下，出现样本或更极端情况的概率。\n在假设检验实际操作中，可通过比较 P-value 与显著性水平 $\\alpha$ 值的大小比较来判断是否应该拒绝原假设。\n举个稍微有点极端的例子：现有样本来自某正态分布 $N(\\mu, 0)$ ：[8, 17, 10, 9]，考虑假设检验问题 $H_0: \\mu=0$ Vs $H_1: \\mu=10$ 。\n我们来看看此时原假设 $H_0: \\mu=0$ 成立的条件下出现样本或更极端情况的概率，即从 $N(0, 1)$ 中抽到[8, 17, 10, 9]这几个样本的概率，显然很小，即 P-value 很小，P-value 越小，越是应该拒绝原假设。\n想想，咱假设原假设成立，可是发现这个假设下出现我们实际抽到的样本的概率极低，此时理所当然应该拒绝原假设。\nP-value 一般都介于 0-1 之间，怎么量化它的“小”呢？这时显著性水平 $\\alpha$ 就站了出来，大喊一声“我来！”。可以将显著性水平 $\\alpha$ 理解为“阈值”\n若 P-value \u0026lt;= $\\alpha$ ，说明 P-value \u0026lt; 阈值，拒绝原假设 反之则不能拒绝原假设 关于 P 值的争议 $p-value = P(T(X)\u0026gt;T(x)|H_0成立)$, 记后验概率 $\\alpha_0=P(H_0成立|x)$. 相对于 P 值，后验概率 $\\alpha_0$ 更有意义（联系贝叶斯定理，如医学领域假阳性情况）\nLindley 悖论：当样本量足够大时，$\\alpha_0$ 可以趋于 1，而 p-value 接近于 0，即利用 P 值检验和贝叶斯检验得到的结果相悖。而贝叶斯检验中后验概率的计算需要依赖先验分布的主观假定，（其实经典假设检验中的显著性水平 $\\alpha$ 也是人为主观设定的）这就引起了经典假设检验和贝叶斯假设检验的重要争议。\nP-值一般过于高估拒绝 $H_0$ 的证据，尤其在大样本情况下更容易出现显著差异，大样本情形下抽样结果与 $H_0$ 的微小差别，就能得到一个极小的 P 值，易犯拒真错误。\n所以在某些特定的情况下，贝叶斯检验法较 P 值检验法具有一定的优越性。参考 P值的局限性 \u0026amp; 贝叶斯假设检定\n功效 Power $1-\\beta$ 另外一个稍微“小众”一些的概念 - 功效（Power）：反映检验在面对正确的备择假设时正确做出决定的能力。有点像“抓走坏人”的能力。毫无疑问，越大越好。\n再延伸一个更“小众”的概念 - 势函数 / 功效函数 (Power Funtion)：功效函数（Power Function）是用于衡量检验方法在不同实际参数值下拒绝原假设的概率。==即不同实际参数值下样本观测值落入拒绝域内的概率==。综合反映了第一类错误和功效的情况。\n$$ g(\\theta) = \\begin{cases} \\begin{array}{rl} \\phantom{1 -} \\alpha(\\theta), \u0026amp; \\theta \\in \\Theta_0 \\\\ 1 - \\beta(\\theta), \u0026amp; \\theta \\in \\Theta_1 \\end{array} \\end{cases} $$\n当 $H_0$ 为真时，$g(\\theta)$ 为犯第一类错误的概率 当 $H_1$ 为真时，$g(\\theta)$ 为功效\n既然固定样本容量时，任何检验都不能同时让第一类错误和第二类错误的概率很小，那么 (Jerzy) Neyman-(Egon) Pearson 所提出的原则就是：在保证犯第一类错误的概率不超过指定数值 $\\alpha$ 的检验中，寻找犯第二类错误 $\\beta$ 概率尽可能小的检验。（N-P准则）\nN-P 准则用势函数 / 功效函数（Power Function）来表达： $$ \\begin{cases} g(\\theta) \\leq \\alpha, \u0026amp; \\theta \\in \\Theta_0 \\\\ g(\\theta) \\text{ 尽可能大}, \u0026amp; \\theta \\in \\Theta_1 \\end{cases} $$ 势函数的作用：\n如何评价某个假设检验的好坏 显著性水平 $\\alpha$，与第一类错误相关联； 功效 $1-\\beta$, 与第二类错误相关联； 假设条件的严苛性，与稳健性相关联。 计算复杂度 假设检验与区间估计的关系 24北师应统真题，茆P326\n区别 1：\n假设检验是判断一个有关总体未知参数的命题是否成立的问题，且只能证伪； 而区间估计是构造一个未知参数最合理的取值范围的问题。 区间估计所提供的信息要比假设检验更加丰富。 区别 2：值得细品🤔\n置信区间是针对==参数==的集合，置信区间是用来估计参数的，这个很好理解； 拒绝域与接受域是针对==样本==的集合，拒绝域+接受域=样本空间。 区别 3：\n区间估计求得的是以样本估计值为中心的==双侧==置信区间 假设检验==既有双侧检验，也有单侧检验==； 区别 4：这和区别 1 是一脉相承的，立足于大/小概率，取决于从正面(证实)/反面(证伪)来看待问题。\n区间估计==立足于大概率==，通常以较大的把握程度（可信度）1－α 去估计总体参数的置信区间； 假设检验==立足于小概率==，通常是给定很小的显著性水平α 去检验对总体参数的先验假设是否成立。 联系：\n拒绝域与置信区间的对偶关系： 双侧检验问题的==接受域==即置信区间 单侧检验问题的==接受域==即置信上/下限，例单侧检验 $H_0:\\mu≤\\mu_0 \\quad vs \\quad H_1:\\mu\u0026gt;\\mu_0$, 其接受域 $W={u=\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}}≤z_{1-\\alpha}}$, 移项: $\\mu≥\\bar{x}-z_{1-\\alpha}\\frac{\\sigma}{\\sqrt{n}}$. 便得到了参数 $\\mu$ 的 $1-\\alpha$ 置信下限。 都是根据样本信息推断总体参数，以抽样分布为理论依据，建立在概率论基础之上的推断，推断结果都有风险； 其它 其它条件不变的前提下，增大样本量，会使 $\\alpha$ 和 $\\beta$ 同时减小吗？争议问题🤔\n会 ==不会== 目前占上风 显著性水平是人为给定，但犯第一类错误 $\\alpha$ 的概率不是恒定的，它会受样本量和犯第二类错误 $\\beta$ 的概率等影响。\n——贾俊平《统计学》和我的概率论老师 增大样本量，只能降低犯第二类错误 $\\beta$ 的概率（提升功效，因为随着样本量↑，估计精度↑），而犯第一类错误的概率是由人为设定的显著性水平 $\\alpha$ 来决定的，无法通过增大样本量来降低。\n——《卫生统计学第八版》和 GPT，以及 DeepSeek 选择合适的检验⽅法或改进实验设计，可以在⼀定程度上优化两类错误的平衡。例如，使⽤更⾼效的统计模型或增加实验的灵敏度。\n两个正态总体均值差的检验 方差已知：u 统计量 方差未知情形，我们需要先做⼀个⽅差⻬性检验，来判断两个总体的⽅差是否相等。 方差相等但未知：Student\u0026rsquo;s t-test, 带 $Sw$ 的那个公式 方差不等且未知：Welch\u0026rsquo;s t-test，使用 Satterthwaite 公式计算自由度 非正态总体： 小样本：Mann-Whitney U 检验（也称为 Wilcoxon 秩和检验） 大样本：近似正态 其它非参数检验方法：Bootstrap ⽅法、置换检验…… 单正态总体均值的检验，方差的检验和方差比的检验暂略。\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E4%B9%9D%E9%98%B3%E7%9C%9F%E7%BB%8F---%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E7%AF%87/","summary":"假设检验是统计推断的重要方法！统计推断包括假设检验和参数估计。 假设检验的定义 假设检验（hypothesis testing）是指从对总体参数所做的一个假设开始，然后搜集样本数据，计算出样本统计量，进而运用这些数据测定假设的总体参数在多大程度上是可靠的，并做出承认还是拒绝该假设的判断","title":"九阳真经 - 假设检验篇"},{"content":"要分清依概率收敛\u0026amp;依分布收敛和大数定律\u0026amp;中心极限定理\n大数定律 LLN 大数定律主要描绘的是样本算术平均值向期望的收敛情况。\n伯努利大数定律：频率依概率收敛于概率； 切比雪夫大数定律：随机变量的均值依概率收敛到一个常数. 要求两两不相关，方差存在且有共同上界； 马尔科夫大数定律：随机变量的均值依概率收敛到一个常数，满足马尔科夫条件即可，除此之外没有任何关于同分布、独立性和相关性的假定。 辛钦大数定律：随机变量的均值依概率收敛到一个常数。要求 iid 且期望存在，对方差没有要求。 伯努利大数定律，既是切比雪夫大数定律的特殊情况，也是辛钦大数定律的特殊情况。\n切比雪夫大数定律是马尔科夫大数定律的特殊情况\n但切比雪夫大数定律、辛钦大数定律针对的是两种不同的情况，谁也不是谁的特例。\n小数定律：认为人类行为本身并不总是理性的，在不确定性情况下，人的思维过程会系统性地偏离理性法则而走捷径，人的思维定势、表象思维、外界环境等因素，会使人出现系统性偏见，采取并不理性的行为。把从大样本中得到的结论错误地移植到小样本中。（总结：可以看作人类的一种认知谬误）\n中心极限定理 中心极限定理主要描绘的是：在许多情况下，对于 iid 的随机变量，即使原始变量本身不是正态分布，标准化样本均值的抽样分布也趋向于标准正态分布。\n棣莫佛-拉普拉斯定理（De Moivre–Laplace theorem）是中央极限定理的最初版本，讨论的是服从二项分布的随机变量序列，样本均值的抽样分布收敛于均值为 np，方差为 np(1-p) 的正态分布。典型例子：高尔顿板（高尔顿板可以看作是伯努利试验的实验模型）。注意这里需要留意拉普拉斯修正（连续性修正） 林德伯格-莱维（Lindeberg-Levy）定理，是棣莫佛-拉普拉斯定理的扩展，讨论独立同分布随机变量序列的中央极限定理。 林德伯格-费勒（Lindeberg-Feller）定理，和李雅普诺夫中心极限定理，是中心极限定理的高级形式，是对林德伯格-莱维定理的扩展。它们表明，满足一定条件时，独立的，但==不同分布==的随机变量序列的标准化和依然以标准正态分布为极限。区别是：这两个定理对应的条件不一样：分别是林德伯格条件和李雅普诺夫条件。 LLN 与 CLT 的联系 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/lln--clt/","summary":"要分清依概率收敛\u0026amp;依分布收敛和大数定律\u0026amp;中心极限定理 大数定律 LLN 大数定律主要描绘的是样本算术平均值向期望的收敛情况。 伯努利大数定律：频率依概率收敛于概率； 切比雪夫大数定律：随机变量的均值依概率收敛到一个常数. 要求两两不相关，方差存在且有共同上界； 马尔科夫大数定律：随","title":"LLN \u0026 CLT"},{"content":" 需要做例题巩固！ 参数检验：假设数据满足正态性、方差齐性、独立性等==假设==，通过估计==总体参数==（均值、方差等）进行检验。常见的如 z 检验、t 检验、方差分析等。\n而非参数检验则不依赖总体分布假设（当然也就不涉及总体参数），适用于非正态、等级数据或小样本。常见的如游程检验、秩和检验、符号检验、符号秩检验、中位数检验和弗里德曼检验等。\n采用非参数检验的原因是，多数情况我们往往并不知道总体的分布。\n游程检验 run test 针对样本随机性的检验。随机性的重要性：①代表总体；②减轻噪声因子影响。\n假设某随机变量有X和Y两种状态，那么连续的X或者连续的Y称为一个游程(run，即连续的相同事件), 如对于序列 $(XX)(Y)(X)(YY)(XXX)$ 有 $R_x=3, R_y=2, R=5$.\n假如不止两种状态，可以使用多类别游程检验。\n对于完全随机数据，$R$ 既不会太大，也不会太小。\n小样本量情形 $R_x\u0026lt;20, R_y\u0026lt;20$，使用精确法。根据 x 样本个数和 y 样本个数 $n_1, n_2$ 即可查表得 $R$ 的临界值，从而做出决策。（注意查表用的是 $n_1, n_2$, 不是 $R_x, R_y$ ！） 大样本情形 $R_x+R_y\u0026gt;40$：$R$ 的分布可用正态分布近似。 图源 统计学与质量036 - 非参数检验(Non-Parametric) - 游程检验（Run Test） 即使是连续型数据，也可做游程检验。根据中位数将数据划分为两类即可。\n符号检验 sign test 符号检验法是通过两个相关样本的每对数据之差的符号进行检验，从而比较两个样本的显著性。具体地讲，若两个样本差异不显著，正差值与负差值的个数应大致各占一半。 符号检验与参数检验中配对样本t检验相对应，不满足参数检验条件时，可采用此法来检验两相关样本的差异显著性。\n广义符号检验是对连续变量任意分位点 $x_{n}$ 进行的检验，而狭义的符号检验则是针对中位数 $M= Q_{0.5}$ 进行的检验。\n大于临界值的记为 $S^+$, 小于临界值的记为 $S^-$, 使用二项分布计算 p 值，然后与显著性水平比较即可完成检验。\n精度低，易犯第二类错误（pmf 的锅）\nWilcoxon 符号秩检验 威尔科克森符号秩检验，计算的是样本与中位数之差/两匹配样本之差…绝对值的秩，的正和和负和。如果它们有相同的样本点，每个点取平均秩。在零假设下，W+和W-应差不多\n把观测值和零假设的中心位置之差的绝对值的秩分别按照不同的符号相加作为其检验统计量。临界值参考秩和表。\n图源 统计学与质量038 - 非参数单样本中位数 - 符号秩检验(Signed Rank Test)\n其它更复杂的方法略 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E9%9D%9E%E5%8F%82%E6%95%B0%E6%A3%80%E9%AA%8C/","summary":"需要做例题巩固！ 参数检验：假设数据满足正态性、方差齐性、独立性等==假设==，通过估计==总体参数==（均值、方差等）进行检验。常见的如 z 检验、t 检验、方差分析等。 而非参数检验则不依赖总体分布假设（当然也就不涉及总体参数），适用于非正态、等级数据或小样本。常见的如游程检验、秩和检","title":"非参数检验"},{"content":"参数：描述总体特征，例如总体均值或标准差。\n统计量：描述样本特征，是样本的函数，它不依赖于未知参数。\n⚠️区分统计量和枢轴量（检验统计量也是统计量）：\n统计量不含参，但统计量的分布含参；如 $\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$; 枢轴量含参，但枢轴量的分布不含参。如 $Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}$. 枢轴量主要用于区间估计和假设检验中。 样本统计量：它是根据样本数据计算出来的一些描述样本特征的概括性数字度量，是样本的函数。如样本均值、样本标准差、样本比例等。\n抽样分布：由样本统计量这个随机变量所形成的概率分布。区分样本分布：一个具体的样本数据点的分布。\n标准差：一个总体或样本分布中的离中趋势度量。\n标准误差（标准误、抽样平均误差）：样本统计量的标准差。\n在抽样调查中我们所说的“可以对抽样误差进行控制”指的就是“标准误差（又称抽样平均误差）”\n抽样平均误差 重复抽样 不重复抽样（需要修正） 样本均值 $\\frac{\\sigma}{\\sqrt{n}}$ $\\frac{\\sigma}{\\sqrt{n}}\\times\\sqrt{\\frac{N-n}{N-1}}$ 样本比例 $\\sqrt{\\frac{p(1-p)}{n}}$ $\\sqrt{\\frac{p(1-p)}{n}}\\times\\sqrt{\\frac{N-n}{N-1}}$ 相较于不重复抽样，重复抽样抽出来的样本对总体的代表性更差（显然的）。从抽样平均误差上也可以体现出来：重复抽样的标准误＞不重复抽样的标准误。\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%A0%B7%E6%9C%AC%E7%BB%9F%E8%AE%A1%E9%87%8F%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E8%BE%A8%E6%9E%90/","summary":"参数：描述总体特征，例如总体均值或标准差。 统计量：描述样本特征，是样本的函数，它不依赖于未知参数。 ⚠️区分统计量和枢轴量（检验统计量也是统计量）： 统计量不含参，但统计量的分布含参；如 $\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$; 枢轴量含参，但枢轴量的分布不含参。如 $Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}$. 枢轴量主要用于区间估计和假设检验中。 样本统计量：它是","title":"样本统计量相关概念辨析"},{"content":"t 检验 常用于:\n正态总体方差未知，单样本均值检验；（方差已知用 z-test, 非正态总体小样本用切比雪夫不等式）； 两独立正态总体，方差未知但相等，均值差检验；（方差已知用 z-test；方差未知且不等时使用 Welch\u0026rsquo;s t 检验，大样本下正态近似也能用 z-test）； 两独立正态总体，小样本，配对样本均值差检验；（大样本用 z-test）； 回归分析中单个回归系数的显著性检验。 卡方检验 ⽤于⽐较观测频数与期望频数的差异，常⽤于分类数据\n常用于：\n正态总体单样本方差检验，适用于对正态总体的单样本的方差进行检验； 卡方拟合优度检验：主要用于确定某个非连续型变量是否可能来自指定的分布（连续型可将连续变量离散化为几个区间，亦可使用 K-S 检验）； 卡方独立性检验（列联分析）：用于检验两个分类变量之间或者一个分类变量与顺序变量之间是否存在关联或独立。 F 检验 常用于：\n两独立正态总体，方差比检验； 回归分析中整体回归模型的显著性检验； 单因素/多因素方差分析，比较多个组均值差异。 t 检验与 F 检验 引申：方差分析和两两 t-test 比较的相比的优势，每多做一次假设检验，就多一份犯错误（不管是第一类还是第二类）的概率。\n图源方差分析后的两两比较，你该不会还这样选吧！？\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E7%BB%9F%E8%AE%A1%E4%B8%AD%E7%9A%84%E4%B8%89%E5%A4%A7%E6%A3%80%E9%AA%8C/","summary":"t 检验 常用于: 正态总体方差未知，单样本均值检验；（方差已知用 z-test, 非正态总体小样本用切比雪夫不等式）； 两独立正态总体，方差未知但相等，均值差检验；（方差已知用 z-test；方差未知且不等时使用 Welch\u0026rsquo;s t 检验，大样本下正态近似也能用 z-test）； 两独立正态总体，小样本，配对样本均值差检验；","title":"统计中的三大检验"},{"content":"算法有朝一日真的会跟人类一样聪明吗？\n打开非线性空间的钥匙：激活函数 激活函数的主要作用是引入非线性，使神经网络能够处理非线性问题，从而增强模型的表达能力，使其能够拟合复杂的函数关系。\nLSTM 便是通过激活函数实现“遗忘门”、“输入门”等功能：\n各种算法 在二分类问题中，模型通常会输出一个概率得分（如 Softmax），然后通过设定一个阈值（Threshold）来决定最终的分类结果（正例 or 负例）。 ^p2v94r\n若是多分类问题，则将多分类任务拆为若干个二分类任务，常见策略包括一对一(OvO)、一对其余(OvR)和多对多(MvM)，详见机器学习中的多分类任务详解 。需要注意，编码方法通常用于自变量为非连续型变量（即分类变量）时，以便模型能够正确处理。==分清自变量为非连续型变量、因变量为非连续型变量两种情况。==\n损失函数不仅可以从代数、几何角度来理解，还可以从概率角度来理解，强推视频合集-正则化\n逻辑回归，Logistic Regression，对数几率回归 逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，$y=\\frac{1}{1+e^{-(w^Tx+b)}}$，使得逻辑回归称为了一个优秀的分类算法。注意，虽然它的名字中有“回归”二字，但其实是一个分类算法。\n似然函数 $L(w) = \\prod [p(x_i)]^{y_i} [1 - p(x_i)]^{1 - y_i}$, 关于其中的累乘项 $P(y_i | x_i) = p(x_i)^{y_i} [1 - p(x_i)]^{1 - y_i}$:\n当 $y_i = 1$ 时，该公式变为 $p(x_i)$，即模型预测 $x_i$ 为 1 的概率。 当 $y_i = 0$ 时，该公式变为 $1 - p(x_i)$，即模型预测 $x_i$ 为 0 的概率。 由于乘积运算不方便计算，我们通常取对数得到对数似然函数（log-likelihood）:$log L(w) = \\sum y_i \\log p(x_i) + (1 - y_i) \\log (1 - p(x_i))$\n最大化对数似然等价于最小化交叉熵损失函数（Cross-Entropy Loss，即对似然函数取负对数）。注意这里不使用 MSE 来构造损失函数 $\\mathcal{L}_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p(x_i))^2$ 是因为这个损失函数非凸（一个函数 $f(w)$ 关于 $w$ 是凸的，当且仅当它的 Hessian 矩阵 $H(f)$ 半正定），使得优化过程容易陷入局部最优，难以收敛到全局最优解。\n如何求解？\n梯度下降法（通过一阶导数来找下降方向，又称“最速下降法”）\n批量梯度下降法（Batch Gradient Descent，BGD）==每次迭代都使用全体样本==，最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。 随机梯度下降（Stochastic Gradient Descent，SGD）==每次迭代只使用一个样本（或全体样本的某个子集）==，最小化每条样本（或全体样本的某个子集）的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向，但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。 牛顿法（在初值点附近对函数做泰勒展开，取展开式的前几项来寻找下降方向）。\n从本质上去看，牛顿法是二阶（或更高阶，取决于取几阶泰勒展开）收敛，梯度下降是一阶收敛，所以牛顿法就更快。 从几何上说，牛顿法就是用一个二次（或更高次）曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。 牛顿法能够根据最优点距离自适应选择迭代步长（学习率）。 ==缺点是每次都需要求解复杂的Hessian矩阵的逆矩阵==。 于是改善出了：拟牛顿法：使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。 正则化，Regularizaation（花书：凡是可以减少泛化误差而不是训练误差的方法都可以称为正则化方法），目的是防止模型过拟合，让误差函数不仅取决于模型拟合的好坏（如 SSE 大小等），还取决于模型的复杂度（如模型中非线性参数的大小，因为一般模型非线性参数越多越大，模型就越复杂；另外大参数也会放大噪声和误差）\nL1 正则化，Lasso 回归，使用曼哈顿距离构造可行域（约束），L1通常用于增加稀疏性，有点类似因子旋转[[多元统计分析复习笔记#关于旋转💫]]。 L2 正则化，Ridge 回归，使用欧几里得距离构造可行域（约束），L2一般用于减小复杂度。 但是没有 L0.5 正则化，因为 L0.5 范数对应的可行域不是凸集，可见图片；那有没有 L3、L4 正则化呢？单纯从凸优化的角度来说，是没问题的。 丢弃法，Dropout，随机让神经网络中的一些神经元失效。\n逻辑回归和最大熵模型本质上没有区别。(最大熵模型可参考一文带你了解最大熵模型（含详细推导）)\n与 SVM 的对比：LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布。—— 【机器学习】逻辑回归（非常详细）\n线性判别分析 LDA \u0026amp; Fisher判别 可参考[[多元统计分析复习笔记#判别分析]]\n线性判别分析（LDA）的目标是找到一个投影方向，使得投影后的类间散度最大化，同时类内散度最小化，从而实现最佳的类别分离。\n决策树 Decision Tree 划分指标：\n信息增益（其中会使用到信息熵作为集合纯度的度量），信息增益越大，则意味着使用该属性来进行划分所获得的“纯度提升”越大。信息增益对可取值数目较多的属性有所偏好； 增益率，与信息增益相反，增益率对可取值数目较少的属性有所偏好； 基尼指数，反映从数据集中随机抽取两个样本，其类别标记不一致的概率。基尼指数值越小，数据集的纯度越高。（基尼系数：判断收入分配公平程度的指标，＞0.4 说明收入分配不均程度大） 剪枝处理：避免“过拟合”，基本策略有预剪枝和后剪枝两种。\n预剪枝：在生成决策树的过程中，边生成边剪枝；==好处==是训练时间开销小（相较于后剪枝而言，因为后剪枝要在决策树建立完成后，自底向上地对树中的所有非叶节点进行注意考察），但是“贪心”本质可能导致欠拟合 后剪枝：先把决策树生成完，最后再剪枝。优点是欠拟合风险小，但是训练时间开销大。 随机森林可见后文[[机器学习 Machine Learning#“三个臭皮匠，顶一个诸葛亮” - 模型融合与集成学习]]部分\n神经网络 感知机，指单层的人工神经网络，区别于较复杂的多层感知机。\n前馈神经网络，在前馈网络中，资讯总是朝一个方向移动，从来不会倒退。可见图\n误差逆传播算法（BP算法，反向传播算法）\n经典和目前主流算法：卷积神经网络、循环神经网络、LSTM、GRU、Transformer、BERT\n支持向量机 SVM SVM是一种二类分类模型，核心目标是在特征空间中找到一个能够最大化两类数据间隔（Margin）的决策边界。\n线性可分SVM：当训练数据线性可分时，通过==硬间隔==最大化可以学习得到一个线性分类器，即硬间隔SVM。 线性SVM：当训练数据不能线性可分但是可以近似线性可分时，通过==软间隔(允许部分数据点被错误分类，以提高泛化能力)==最大化也可以学习到一个线性分类器，即软间隔SVM。 非线性SVM：当训练数据线性不可分时，通过使用==核技巧(将数据映射到高维空间，使其线性可分)==和软间隔最大化，可以学习到一个非线性SVM。 EM 算法 是在含有隐变量的概率模型中寻找参数最大似然估计或者最大后验估计的算法。\n进行 MLE 时，当分布中有多余参数或数据为截尾或缺失时，其 MLE 的求取是比较困难的，于是提出了 EM 算法，它是一类通过迭代进行极大似然估计的优化算法，EM 算法经过两个步骤交替进行计算：\n第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值； 第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。 性质：在很一般的条件下，EM 算法是一定收敛的。EM 算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。\n💡一个最直观了解 EM 算法思路的是 K-Means 算法。在 K-Means 聚类时（每个聚类簇的质心是隐含数据）。我们会假设 K 个初始化质心，即 EM 算法的 E 步；然后计算每个样本距离最近质心的欧式距离，并把样本聚类到最近的这个质心，即 EM 算法的 M 步。重复这个 E 步和 M 步，直到质心不再变化为止，这样就完成了 K-Means 聚类。\n隐马尔可夫模型 用来描述一个含有隐含未知参数的马尔可夫过程\nhttps://www.cnblogs.com/skyme/p/4651331.html\nNLP 中有这个算法\n写累了，先暂略\n其它 贝叶斯分类器见多元统计分析相关内容[[多元统计分析复习笔记#^dkbbsu]]\n聚类算法见多元统计分析相关内容[[多元统计分析复习笔记#聚类分析]]\n如何处理模型训练中的高方差或高偏差问题？ https://www.jvruo.com/archives/1277/\n评价模型的指标 准确率、精度 $Accuracy=\\frac{TP+TN}{TP+FP+FN+TN}$ 精准率、精确率、查准率 $Precision=\\frac{TP}{TP+FP}$，预测为正例的样本中，有多少是正确的 召回率、查全率 $Recall=\\frac{TP}{TP+FN}$，真实为正例的样本中，有多少被成功找到了 ==区分查准\u0026amp;查全！精确率和召回率两者的关系是此消彼长的，于是给出 F1 值进行调和平均== F1 值： $F1=2·\\frac{Precision·Recall}{Precision+Recall}$, 即 Precision 和 Recall 的调和平均 引申 $F_\\beta$ 值，用参数 $\\beta$ 来度量 Precision 和 Recall 哪个更重要：Recall 的重要性是 Precision 的 $\\beta$ 倍，F1 值算是特殊情况，把 Precision 和 Recall 看作同等重要。$F_\\beta=(1+\\beta^2)·\\frac{Precision·Recall}{\\beta^2·Precision+Recall}$ ROC 曲线（Receiver Operating Characteristic curve 接收者操作特征曲线）与 AUC 值，==适合数据集中类别分布均衡时使用==： $\\text{True Positive Rate}=\\frac{TP}{TP+FN}$, 即 $Recall$. $\\text{False Positive Rate} = \\frac{FP}{FP+TN}$ 设置阈值从 0-1（如何理解这里的阈值？[[机器学习 Machine Learning#^p2v94r]]），可以得到不同的 TPR 和 FPR。以 TPR 为纵坐标、FPR 为横坐标，即可绘制出 ROC 曲线，我们希望 TPR↑，FPR↓，即希望曲线靠近左上角，故给出量化标准 AUC 值：ROC 曲线下的面积。AUC 值越大越好。 PR 曲线：当我们的数据集中==类别分布不均衡时==我们可以用 PR 曲线代替 ROC 曲线．PR 曲线与 ROC 曲线的区别在于 PR 曲线以 Recall 作为 x 轴，Precision 作为 y 轴。我们希望 Precision↑，Recall↑，即希望曲线靠近右上角， ROC 曲线与 PR 曲线的区别与联系： 可以说，每一条 ROC 曲线都与一条 PR 曲线相对应，因为混淆矩阵的 TPR-FPR 数值对和 Precision-Recall都有唯一的数值对。 如果在 ROC 空间中，曲线 1 优于曲线 2，那么在 PR 空间中，曲线 1 对应的 PR 曲线同样优于曲线 2 对应的 PR 曲线。 ROC 曲线对正负两类样本同样关系，而 PR 曲线则对正例更加关心。 ROC 适合类别均衡时使用，而 PR 曲线适用类别不均衡场景。 敏感性、灵敏度、真阳性率 $Sensitivity=\\frac{TP}{TP+FN}$ 特异性、特异度 $Specificity=\\frac{TN}{TN+FN}$ AIC 赤池信息量和BIC 贝叶斯信息量，是基于似然函数的估计方法，主要用于统计建模，尤其是在回归分析和时间序列分析等领域，==在机器学习领域并不常用==。[[回归分析#^grw9h5]] Kaggle比赛中提高成绩的3个主要地方 特征工程 特征工程（Feature Engineering）特征工程是将原始数据转化成更好的表达问题本质的特征的过程，使得将这些特征运用到预测模型中能提高对不可见数据的模型预测精度。\n图源深度了解特征工程。感觉有点像数据分析中的数据预处理。\n异常值的检测与处理 检测异常值的方法： $3\\sigma$ 准则、百分位数方法、箱线图中低于箱形图下触须（或 Q1 − 1.5x IQR）或高于箱形图上触须（或 Q3 + 1.5x IQR）的观测值、其它机器学习方法。\n不同方法效果对比可见：链接\n处理异常值的方法：\n删除异常值：最简单的方法是直接删除包含异常值的观测。这在异常值数量较少或对整体分析影响较大时可能是一种有效的方法。 修正异常值：将异常值修正为合理的数值。这可以通过替换为均值、中位数或根据其他统计指标来实现。 盖帽法：整行替换数据框里99%以上和1%以下的点，将99%以上的点值=99%的点值；小于1%的点值=1%的点值。 分箱（Binning）或分组：将数据分组成多个区间，将异常值归入相邻的区间。这可以减小异常值的影响，同时保留了原始数据的趋势。 变换数据：对数据进行变换，使其更符合正态分布。例如，可以使用对数变换、平方根变换等。这有助于减小极端值对分析的影响。 不处理异常值：在某些情况下，特别是当异常值是真实的、有意义的观测时，可以选择不处理异常值，而是在分析中保留它们。 数据标准化 图源网络\n编码方式 Encoding Method 频率编码（Frequency Encoding）、嵌入编码（Embedding Encoding）的优缺点。\n调参 参考机器学习：模型调参大法总结\n“三个臭皮匠，顶一个诸葛亮” - 模型融合与集成学习 模型融合（Model Fusion）指的是将多个不同的机器学习模型的预测结果进行组合，生成一个新的、更为准确的预测。以提高最终的预测性能、稳定性和泛化能力。它的核心思想是利用多个模型的优势，减少单一模型的偏差和方差，通常用于提升分类或回归任务的精度。\n简单加权融合： 分类问题：voting 回归问题：average 综合：排序融合 (Rank averaging)，log 融合等 更为复杂的融合方法，详见下面的集成学习方法。 集成学习（Ensemble Learning）是指训练多个弱模型（基学习器，Base Learners），并将它们组合成一个更强的模型，以提高整体的泛化能力和鲁棒性。目标是让多个弱模型相互补充，形成一个强模型，而不仅仅是融合多个已有的强模型。\n常见的集成学习方法：\nBagging（Bootstrap AGGregatING）装袋算法 核心思想：通过 Bootstrap 自助采样法对训练数据集进行有放回抽样，训练多个相同类型的弱模型，然后取平均值（回归任务）或投票（分类任务）。==从偏差-方差分解的角度看，Bagging 主要关注降低方差==。 代表算法：随机森林（Random Forest）。 Boosting（提升方法）将弱学习器提升为强学习器 核心思想：让每个新的弱学习器重点关注之前模型分错的样本后生成新的学习器。最后将之前所有学习器加权结合。==从偏差-方差分解的角度看，Boosting 主要关注降低偏差==。 代表算法：AdaBoost、Gradient Boosting（GBDT）、XGBoost、LightGBM、CatBoost。 Stacking（堆叠泛化） 核心思想：将多个基模型的预测作为新特征，输入到更高一级的元模型（Meta Learner） 进行学习，如此循环，最后输出一个最终模型。 区别于模型融合的 Stacking：这里的基模型可能是专门为集成学习设计的弱模型。 可参考其它优秀博文机器学习-集成学习（模型融合）方法概述、【机器学习】模型融合方法概述\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-machine-learning/","summary":"算法有朝一日真的会跟人类一样聪明吗？ 打开非线性空间的钥匙：激活函数 激活函数的主要作用是引入非线性，使神经网络能够处理非线性问题，从而增强模型的表达能力，使其能够拟合复杂的函数关系。 LSTM 便是通过激活函数实现“遗忘门”、“输入门”等功能： 各种算法 在二分类问题中，模型通常会输出一个概率得","title":"机器学习 Machine Learning"},{"content":"pmf pdf cdf 概念辨析 其实就是英文单词的首字母缩写。\n概率质量函数 pmf，用于离散型随机变量：非负性、正则性。pmf 的函数值不能大于 1\n概率密度函数 pdf，用于连续型随机变量：非负性、正则性。pdf 函数值可能大于 1（用密度去理解）\n概率分布函数 cdf，单调性（单调非减）、有界性、右连续性、非负性\n二维 r.v. 的一个特殊性质 连续型 r.v. 的分布函数必是连续函数, 证明；但其 pdf 不一定连续，可能有跳跃间断点。\n特例 - 既非离散又非连续型随机变量 ⚠️存在既非离散又非连续型的随机变量！如：\n$$ F(x) = \\begin{cases} 0, \u0026amp; x \u0026lt; 0 \\ \\frac{1+x}{2}, \u0026amp; 0 \\leq x \u0026lt; 1 \\ 1, \u0026amp; x \\geq 1 \\end{cases} $$\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/pdf-pmf-cdf/","summary":"pmf pdf cdf 概念辨析 其实就是英文单词的首字母缩写。 概率质量函数 pmf，用于离散型随机变量：非负性、正则性。pmf 的函数值不能大于 1 概率密度函数 pdf，用于连续型随机变量：非负性、正则性。pdf 函数值可能大于 1（用密度去理解） 概率分布函数 cdf，单调性（单调非减）、有界性、右连续性、非负","title":"pdf pmf cdf"},{"content":"茎叶图 优点：\n直观展示数据分布：茎叶图通过将数据分成“茎”和“叶”两部分，能够直观地展示数据的分布情况。 保留原始数据：与直⽅图不同，茎叶图保留了原始数据的详细信息，每个数据点都可以从图中直接读取。 简单易绘制：茎叶图不需要复杂的计算或⼯具，⼿⼯即可快速绘制。 适合⼩数据集：茎叶图特别适合展示⼩数据集，能够清晰地显示每个数据点的位置。 缺点：\n茎叶图只便于表示两位有效数字的数据； 茎叶图只方便记录两组的数据； 茎叶图不适用于大数据集。 作⽤：\n数据探索：在数据分析的初期，茎叶图可以帮助快速了解数据的分布和集中趋势。 异常值检测：通过观察茎叶图，可以很容易地发现数据中的异常值或离群点。 数据⽐较：可以绘制多个茎叶图来⽐较不同数据集之间的分布情况。 茎叶图优化？\n直方图 由一系列高度不等的纵向条纹或线段表示数据分布的情况。 一般用横轴表示数据类型，纵轴表示分布情况。\n优点：\n直观展示数据分布 适用于大规模数据集 可以通过多个直方图对比不同数据集的分布差异。 缺点：\n丢失数据原始信息 绘制复杂 作用：\n数据探索：直观感受的集中趋势、离散程度、偏态和峰态；发现数据的模式，比如是否呈正态分布、偏态分布或多峰分布。 识别异常值，发现数据中的极端情况。 条形图（柱状图）Vs 直方图：\n箱线图 优点：\n方便多组数据比较； 识别异常值； 使用数据量大的情形 缺点：\n丢失数据原始信息 对数据量较少的情况不够直观 不适用小数据情形 作用：\n直观展示数据的分布特征（如中位数、四分位数、离散程度）。 识别异常值，帮助发现数据中的极端值。 便于对比多个数据集的分布差异，尤其适用于数据间的偏态比较。 改良版的箱线图：增加上下限阈值。四分位间距 $IQR = Q3 - Q1$, 上限阈值：$Q3 + 1.5 \\times IQR$, 下限阈值：$Q1 - 1.5 \\times IQR$；超出上述阈值的点定义为疑似异常值，并在图中标注（如用星号表示）\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/","summary":"茎叶图 优点： 直观展示数据分布：茎叶图通过将数据分成“茎”和“叶”两部分，能够直观地展示数据的分布情况。 保留原始数据：与直⽅图不同，茎叶图保留了原始数据的详细信息，每个数据点都可以从图中直接读取。 简单易绘制：茎叶图不需要复杂的计算或⼯具，⼿⼯即可快速绘制。 适合⼩数据集：茎叶图特别适","title":"数据可视化"},{"content":" ——转自 《一张图说明二项分布、泊松分布、指数分布、几何分布、负二项分布、伽玛分布的联系》\n🐈注：图片中的“连续情况”并非指“连续型随机变量”。有点“时间”的 sense？\n二项分布的近似 n 大 p 不是很小, $np \\rightarrow \\infty$, 实际中常用 $np\u0026gt;5, nq\u0026gt;5$ 来作为使用正态分布来近似二项分布的条件（棣莫弗-拉普拉斯中心极限定理）\nn 大 p 很小且 $np \\rightarrow \\lambda$ 时可用泊松分布近似\n泊松分布是在不知道事件的可能发生总次数的情况下对小概率事件建模，又叫泊松小数法则。是一个计数过程。$\\lambda\u0026gt;10$ 时也可用正态分布近似泊松分布（不同教材对 $\\lambda$ 大小要求不同）\n根据CLT，在大样本且满足独立同分布、总体方差有限的条件下，==样本均值（或样本和）==的分布近似服从正态分布，无论原总体分布如何。\n泊松分布与指数分布 这两个分布的参数 $\\lambda$ 的含义是同样的，用于衡量事件发生的频率（单位时间发生 $\\lambda$ 次）\n上早八 7-8 点来的人数计算用啥分布？（np＞5） - Ans: 泊松分布、正态分布近似\n几何分布与超几何分布 几何分布，对应几何级数\n超几何分布，对应超几何级数\n详见 知乎问答\n分布的可加性 独立可加性定义：相互独立的 xx 分布加完之后还是 xx 分布。\n具有可加性的分布：\n泊松分布，设 $X_1 \\sim Poisson(\\lambda_1), X_2 \\sim Poisson(\\lambda_2)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim Poisson(\\lambda_1+\\lambda_2)$. 伽马分布，设 $X_1 \\sim \\Gamma(\\alpha_1, \\lambda), X_2 \\sim \\Gamma(\\alpha_2, \\lambda)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim \\Gamma(\\alpha_1+\\alpha_2, \\lambda)$. 二项分布，设 $X_1 \\sim b(n, p), X_2 \\sim b(m, p)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim b(n+m, p)$. 正态分布，设 $X \\sim N(\\mu_1, \\sigma_1^2), Y \\sim N(\\mu_y, \\sigma_y^2)$, $X$ 和 $Y$ 相互独立，则 $aX + bY \\sim N\\left(a\\mu_1 + b\\mu_2, a^2\\sigma_1^2 + b^2\\sigma_2^2\\right)$. 负二项分布（r 取整数时为帕斯卡分布），设 $X_1 \\sim Nb(r_1, p), X_2 \\sim Nb(r_2, p)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim Nb(r_1+r_2, p)$. 卡方分布，设 $X_1 \\sim \\chi^2(n), X_2 \\sim \\chi^2(m)$, $X_1$ 和 $X_2$ 相互独立，则 $X_1+X_2 \\sim \\chi^2(n+m)$. 柯西分布。 不具有可加性的分布：\n0-1分布，参数 $p$ 相同的 $0-1$ 分布相加为二项分布，不是 $0-1$ 分布。 几何分布 $Ge(p)$, “常在河边站哪有不湿鞋”，指事件首次发生时所进行的试验次数。参数 $p$ 相同的几何分布相加为负二项分布，不是几何分布。 均匀分布。 指数分布，$Exp(\\lambda) \\sim Ga(1, \\lambda)$, 参数 $\\lambda$ 相同的指数分布相加为伽马分布，不是指数分布。 贝塔分布。 超几何分布。 神通广大的伽马和贝塔分布 （函数）： 伽马分布：\n伽马分布不仅可以与指数分布关联，也可与卡方分布相关联: $Ga(\\frac{n}{2}, \\frac{1}{2}) \\sim \\chi^2(n)$. 若 $X\\sim Ga(\\alpha, \\lambda)$, 则当 $k\u0026gt;0$ 时，$kX \\sim Ga(\\alpha, \\frac{\\lambda}{k})$. 伽马函数:\n$n \\in N^+, \\Gamma(n+1)=n\\Gamma(n)=n!$ $\\Gamma(1)=0!=1, \\quad \\Gamma(\\frac{1}{2})=\\sqrt{\\pi}$. 贝塔分布：\n$Be(1, 1)=U(0, 1)$. 贝塔函数（注意不是贝塔分布）:\n$B(a, b)=B(b, a)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$. 分布的无记忆性 $P(X\u0026gt;t+s|X\u0026gt;s)=P(X\u0026gt;t)$. 无记忆是指任何特定情况下的条件概率看起来就像初始的无条件概率。推导见 推文\n指数分布。指数分布通常被称为寿命分布，灯泡 $P(还能用5年|已经用了1年)=P(能用5年)$, 看上去很反常，但其实这里指的是“理想灯泡”，寿命足够长，只会因为其它不可控因素报废。 几何分布。 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%85%B3%E4%BA%8E%E5%88%86%E5%B8%83/","summary":"——转自 《一张图说明二项分布、泊松分布、指数分布、几何分布、负二项分布、伽玛分布的联系》 🐈注：图片中的“连续情况”并非指“连续型随机变量”。有点“时间”的 sense？ 二项分布的近似 n 大 p 不是很小, $np \\rightarrow \\infty$, 实际中常用 $np\u0026gt;5, nq\u0026gt;5$ 来作为使用正态分布来近似二项分布的条件（棣莫弗-拉普拉斯中心极限","title":"关于分布"},{"content":"抽样方法 不同抽样方法各有千秋。抽样方法的选取，取决于研究目的和对性价比的考量。\n抽样调查可以分为两类，即概率抽样和非概率抽样。==概率抽样==是按照随机原则进行抽样，不加主观因素，组成总体的每个单位都有被抽中的概率（非零概率），可以避免样本出现偏差，样本对总体有很强的代表性。==非概率抽样==是按主观意向进行的抽样（非随机的），组成总体的很大部分单位没有被抽中的机会（零概率），使调查很容易出现倾向性偏差。—— 抽样调查的主要方法\n概率抽样的本质是通过随机化和可量化概率实现科学推断，其优势在于①结果的客观性、②可推广性以及③支持基于概率论的参数估计和假设检验。然而，它对抽样框和资源的要求较高。非概率抽样虽灵活便捷，但牺牲了统计推断的严谨性。研究设计中，选择何种方法需权衡研究目的（探索性 vs 推断性）、资源限制及对误差的容忍度。在需严格推论的场景下，概率抽样是不可替代的基石。\n概率抽样 统计理论支持：\n大数定律：样本量越大，样本均值趋近总体均值； 中心极限定理：样本统计量分布趋近正态，支持置信区间计算； 设计效应：复杂抽样设计（如分层、整群）需调整方差计算。 常见概率抽样方法：\n==简单随机抽样==，就是从总体中不加任何分组、划类、排队等，完全随机地抽取调查单位。需要抽样框（有关总体全部单位的名录）当样本量N很大时，构造这样的抽样框并不容易。且根据这种方法抽出的个体较为分散，会给后续调查实施增加困难。因此，在规模较大的调查中很少直接采用简单随机抽样，一般是把这种方法和其他抽样方法结合起来使用。 ==分层抽样==（类型抽样），它首先将要研究的总体按某种特征或某种规则划分为不同的层（组），然后按照等比例或最优比例的方式从每一层（组）中独立、随机地抽取个体，最后将各层的样本结合起来对总体的目标量进行估计。优点是抽样误差小，缺点是不适用当总体无法彻底划分为不相交的子组时的场景。 ==整群抽样==（集团抽样），理论上要求群内差异大，群间差异小（每个群抽到哪个都差不多，希望群内差异大，蕴含信息量大）。首先，抽取样本时只需要群的抽样框，而不必要求包括所有单位的抽样框。这大大简化了编制抽样框的工作量；其次，由于群通常是由那些地理位置邻近的或隶属于同一系统的单位所构成，因此调查的地点相对集中，从而节省了调查费用，方便了调查的实施。其主要缺点是估计的精度较差，因为同一群内的单位或多或少有些相似，在样本量相同的条件下整群抽样的抽样误差通常比较大。 ==系统抽样==（等距抽样、机械抽样），它是将总体N个个体按某种顺序排列，按规则确定一个随机起点，再每隔一定间隔逐个抽取样本单位的抽样方法。典型的系统抽样是先从数字1-k之间随机抽取一个数字r作为初始单位，以后依次取r+k，r+2k，…。系统抽样的主要优点是操作简便，如果有辅助信息，对总体内的单位进行有组织的排列，可以有效地提高估计的精度；缺点是对估计量方差的估计比较困难（因为样本单位间的相关性？）。 ==多阶段抽样==，将抽样过程分阶段进行，每个阶段使用的抽样方法往往不同，即将各种抽样方法结合使用。每增加一个抽样阶段，就会增添一份估计误差。 非概率抽样 ==方便抽样==（便利抽样、偶遇抽样），主要用于初期评估的探索性研究。调查过程中由调查员依据方便的原则自行确定抽入样本的个体。 ==判断抽样==，调查者根据主观经验和判断从总体中选取有代表性的个体构成样本的一种非概率抽样方法。它不能获得估计值的精度，其精度取决于抽样者的经验，适用于总体中的个体极不相同而样本容量又很小的情况。 ==自愿抽样==，指被调查者自愿参加，成为样本中的一分子，向调查人员提供有关信息。它可以反映某类群体的一般看法。 ==滚雪球抽样==，先找到最初的样本，然后根据他们提供的信息去获得新的个体形成样本。这种过程不断继续，直到完成规定的样本容量为止。滚雪球抽样常用于对稀少的特定群体的调查。 ==配额抽样==，非概率版的分层抽样。==配额抽样==，非概率版的分层抽样。调查人员将调查总体样本按一定标志分类或分层，确定各类（层）单位的样本数额，在配额内主观地任意抽选样本的抽样方式。 抽样误差\u0026amp;非抽样误差 抽样误差，是随机性误差，==只存在概率抽样中存在==，可以计算并控制！普查的 Sample Error 为 0. 估计总体均值时 $E=z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}$, 估计总体比例时 $E=z_{\\alpha/2}\\sqrt{\\frac{\\pi(1-\\pi)}{n}}$, 移项即可计算出样本量 $n$. ⚠️注意！在计算某抽样调查所需样本量 $n$ 时，抽样调查还没开始！还不知道样本标准差 $s$ 和样本比例 $p$, 故抽样误差的计算公式中使用的是总体参数。 最主要与样本量大小有关； 其次还与总体变异性、抽样方法(有放回和无放回差个修正系数)、抽样调查的组织形式、辅助信息的使用（如分层抽样中的分层依据）和抽样框的准确性等因素有关。 非抽样误差，与样本随机性无关，因而==在概率抽样和非概率抽样中都存在==。不能通过增大样本容量来控制，仅能通过调查过程的质量控制来控制 ==抽样框误差==，仅在概率抽样中存在。指目标总体和抽样总体不一致时产生的误差。目标总体是指调查研究对象的全体，而抽样总体是从目标总体中抽选样本的总体。 丢失目标总体单元 包含非目标总体单元 复合连接，抽样框中的单元与总体目标单元不完全是一对一连接 不正确的辅助信息等 回答误差 理解误差 记忆误差 有意识误差 无回答误差 调查员误差 测量误差 有无放回、有序无序，共四种情况 上图图源知乎 https://zhuanlan.zhihu.com/p/48248142\n无序抽样使用 $C$ 组合；有序抽样使用 $A$ 排列。还是很好理解的。\n是否放回，决定了：\n样本是否可能重复 所抽取的样本间是否独立 抽样时总体数量是否发生变化 ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%8A%BD%E6%A0%B7%E6%8A%80%E6%9C%AF/","summary":"抽样方法 不同抽样方法各有千秋。抽样方法的选取，取决于研究目的和对性价比的考量。 抽样调查可以分为两类，即概率抽样和非概率抽样。==概率抽样==是按照随机原则进行抽样，不加主观因素，组成总体的每个单位都有被抽中的概率（非零概率），可以避免样本出现偏差，样本对总体有很强的代表性。==非","title":"抽样技术"},{"content":"概率的定义 概率有古典定义、几何定义、频率定义（统计定义）和公理化定义（柯尔莫戈洛夫公理）共四种定义。（算上茆书后面提到的主观定义，那就一共有 5 种定义）\n其中古典定义、几何定义和频率定义（统计定义）都满足概率的公理化定义？\n具体介绍可参见该博文 概率的四种定义及公理化定义产生\n区别古典概率与古典统计学派：\n古典概率的定义基于等可能性假设，主要用于有限离散样本空间。 古典统计学派（频率学派），区别于贝叶斯学派，通过大量独立实验将概率解释为统计均值（大数定律）。古典统计学派（频率学派）使用的像是概率的频率定义（统计定义），而不是古典定义，不要搞混了！ 贝特朗悖论等概率悖论推动了概率公理化定义的出现（在此之前概率并未得到严谨定义，从而造成了许多悖论）。==概率的公理化定义== $(\\Omega, \\mathscr{F}, P)$ 约束了：\n样本空间 $\\Omega$ (随机现象的一切可能的基本结果组成的集合) 随机变量就是定义在样本空间 $\\Omega$ 上的函数。 测度空间（或称事件域） $\\mathscr{F}$ (样本空间的某些子集所组成的集合类) 定义在 $\\mathscr{F}$ 上满足非负性、正则性和可列可加性的概率测度（或称实值函数） $P$ . 那么贝塔朗悖论对应的问题应该如何来解决呢？计算机随机模拟是一种办法（在圆上任意选择两个不同的点连成弦，测量弦长，模拟个十万八千次即可用频率逼近概率）。\n其中测度空间 $\\mathscr{F}$ 是样本空间 $\\Omega$ 的子集族，通常可以取 $2^{\\Omega}$. (空集+单元素集+……全集)\n例如：投掷一枚硬币的实验在概率公理化定义下可以描述如下：$H$ 表示正面，$T$ 表示反面。\n样本空间：$\\Omega = {H, T}$。 事件空间：$\\mathscr{F} = {\\varnothing, {H}, {T}, {H, T}}$。 概率测度：对于所有 $A \\in \\mathscr{F}$，定义： $P(\\varnothing) = 0, \\quad P(H) = \\frac{1}{2}, \\quad P(T) = \\frac{1}{2}, \\quad P({H, T}) = 1$ 关于概率 \u0026amp; 测度 有限或无限可列个概率为 0 的事件的并仍然是 0。 无限不可列个概率为 0 的事件的并可以有正概率。 概率 0 ≠ 绝对不可能发生，只是测度为 0(人家只是零测集，不是不可能事件)；但 P (不可能事件)=0\n同样，概率 1≠必然事件，一定发生，只是测度为 1. 但 P (必然事件)=1\nLebesgue 测度告诉我们，整个区间的测度是 1，而单个点的测度是 0。\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%A6%82%E7%8E%87%E7%9A%84%E5%AE%9A%E4%B9%89/","summary":"概率的定义 概率有古典定义、几何定义、频率定义（统计定义）和公理化定义（柯尔莫戈洛夫公理）共四种定义。（算上茆书后面提到的主观定义，那就一共有 5 种定义） 其中古典定义、几何定义和频率定义（统计定义）都满足概率的公理化定义？ 具体介绍可参见该博文 概率的四种定义及公理化定义产生 区别古典概率","title":"概率的定义"},{"content":"定义：用于估计未知参数 $\\theta$ 的统计量 $\\hat{\\theta}=\\theta(x_1, x_2, \u0026hellip;, x_n)$ 称为 $\\theta$ 的估计量，或称为 $\\theta$ 的点估计，简称估计。注意：==点估计是统计量（样本的函数）==\n补充统计量定义：设 $x_1​, x_2​, \u0026hellip;, x_n​$ 为取自某总体的样本，若样本函数 $T=T(x_1​, x_2​, \u0026hellip;, x_n​)$ 中不含有任何未知参数，则称 $T$ 为统计量。统计量的分布称为抽样分布。\n常用的点估计方法包括矩估计、极大似然估计、LSE、MUVE、BLUE、最大后验估计等\n点估计是指使用样本数据构造样本统计量来估计总体参数时，使用一个点的数值表示“最佳估计值”。\n与区间估计形成对比：区间估计是在点估计的基础上，构造一个置信区间，==以反映估计值的不确定性==，使得总体参数有一定概率($1-\\alpha$)落在该区间内。\n矩估计 Momnet 用样本矩替换总体矩，用经验分布替换总体函数。\n理论基础：\n格列文科定理（茆定理 5.2.1：当样本量相当大时，经验分布函数 $F_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} I(X_i \\leq x)$ 是总体分布函数 $F(x)$ 的一个良好的近似，这也正是经典统计学中一切统计推断都以样本为依据的原因） 一阶矩的收敛：若一阶矩存在（如 $E(X)\u0026lt;∞$），辛钦大数定律，或切比雪夫大数定律。 高阶矩的收敛：若高阶矩存在（如 $E(X^k)\u0026lt;∞$），强大数定律（将弱大数定律拓展到 k 阶矩）。 ⚠️注意！\n分母为$N-1$的方差的无偏估计 $\\hat{\\sigma}^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\hat{\\mu})^2$ 不是矩估计！有偏估计 $\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{\\mu})^2$ 才是。\n极大似然估计 又名“事后诸葛亮估计” 极大似然估计（Maximum Likelihood Estimation, MLE）是一种基于样本数据估计概率模型参数的经典方法。其核心思想是：在给定观测数据的情况下，选择使得这些数据出现概率最大的参数值作为点估计值。\n似然函数（Likelihood Function）是一个==关于参数的函数==，它表示在给定参数 $\\theta$ 的情况下观察到数据 $X$ 的概率: $L(\\theta)=L(\\theta; X)=P(X|\\theta)$. 从这个式子可以看出，似然函数是关于样本 $X$ 的函数，没错，似然函数的概念听起来很陌生，但实际上就是我们在概率论当中看到的各种概率分布 $f(x)$. 而极大似然估计就是找到使似然函数最大的参数 $\\theta$, 即 $\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} L (\\theta) = \\arg\\max_{\\theta} P (X|\\theta)$\nMLE 之所以又名“事后诸葛亮估计”，或“马后炮”估计，是因为它就是“看结果猜原因，怎么合理怎么来”；擅长解释已发生的事，但未必能预测未来。\n引入亿点贝叶斯 人人都应该懂一些贝叶斯。\n理解贝叶斯公式 贝叶斯后验概率 $P(\\theta_i|X)=\\frac{P(X|\\theta_i)×P(\\theta_i)}{P(X)}=\\frac{L(\\theta_i|X)·P(\\theta_i)}{P(X)}=\\frac{似然×先验}{边缘概率(配分函数)}$\n其中分母部分 $P(X)=\\sum P(X|\\theta)·P(\\theta)=P(X|\\theta_i)·P(\\theta_i)+\\sum_{k≠i}P(X|\\theta_k)·P(\\theta_k)$\n故可将后验概率表示为 $P(\\theta_i|X)=\\frac{P(X|\\theta_i)×P(\\theta_i)}{P(X|\\theta_i)·P(\\theta_i)+\\sum_{k≠i}P(X|\\theta_k)·P(\\theta_k)}$, 总这个式子可以比较直观地反映出贝叶斯==使用样本更新后验概率==的思想：若 $P(X|\\theta_i)×P(\\theta_i)$ 值↑，说明 $\\theta_i$ 是参数真值的可能性大，应该给它更高的比重；反之则给它更小的比重。\nMLE VS MAP（Maximum A Posteriori） 最大似然估计的目标是最大化似然函数；而最大后验估计的目标则是最大化后验概率，==相对于最大似然估计额外还考虑了先验概率的影响==。\n而 MLE 假设模型是均匀分布，即先验概率P(θ)=1。可以说，MLE是MAP的一种特例\n我们的直觉是按照最大似然估计的方式去思考的，但是更理性的方式是要用到贝叶斯公式，用最大后验估计的方式去思考。通过这个问题我们也可以发现：人类的直觉很多时候不靠谱，不靠谱的原因，很可能就是我们的直觉没有用贝叶斯的方式去思考。\n人的思维模式不就是贝叶斯吗，人工智能有望突破意识枷锁！！！\n—— 上图和文本来自视频 贝叶斯解释“L1和L2正则化”，本质上是最大后验估计。如何深入理解贝叶斯公式？\n再举个例子？🤔\n怎么理解贝叶斯后验概率公式？ #统计学 ✅ 2025-03-08 古典频率学派 VS 贝叶斯学派 说到贝叶斯，就不得不提及古典频率学派和贝叶斯学派：\n频率学派（古典统计学）：认为参数真值是固定的且未知的常数，观察到的数据是随机的。 具体做法：根据大数定律，通过大量重复试验，用频率逼近概率 贝叶斯学派：贝叶斯学派认为参数真值不是固定的，是一个随机变量，而观察到的数据是固定的。这个随机变量可用一个概率分布去描述，这个分布称为先验分布， 具体做法：开始时给定先验概率，通过数据不断修正，最后得出后验概率，或后验分布。因引入主观的先验分布而受到大量质疑，但记得在一个视频里讲到过：随着修正的不断进行，先验分布的“权重”也在不断下降。 置信区间\n(confidence interval, CI) 可信区间\n(credibility interval,CI) 学派 古典统计学派，或频率学派 贝叶斯学派 观点 参数是一个未知的常数 参数是随机变量 两个区间的区别 对于一个给定的 95% 置信区间，含义是：构造区间 100 次，有 95 次包含参数真值。 对于一个给定的 95% 可信区间，含义是：参数真值有 95% 的概率落入该区间内。 回归 MLE：MLE 与 Bayes 的区别 要将『似然函数』与『后验概率』相区别：与似然函数 $P(X|\\theta)$ 恰好相反，后验概率是在给定数据 $X$ 后，参数 $\\theta$ 的概率 $P(\\theta|X)$. 注意是参数 $\\theta$ 的概率。进一步地，『后验分布』则是关于参数 $\\theta$ 的分布。\nMLE 不具有唯一性 $\\hat{\\beta}=(X^TX)^{-1}X^TY$\n唯⼀性条件： 如果似然函数是严格凸的，且数据满⾜⼀定的正则条件, $(X^TX)$ 可逆，那么极⼤似然估计的结果是唯⼀的。例如，在⾼斯分布中，均值和⽅差的极⼤似然估计是唯⼀的。对于简单的模型（如线性回归），极⼤似然估计通常有唯⼀解。\n⾮唯⼀性情况： 似然函数⾮凸或多峰，或者数据存在共线性问题, $(X^TX)$ 不可逆，极⼤似然估计可能会有多个局部极⼤值，导致解不唯⼀。\n矩估计 VS 极大似然估计 常考！ 矩估计 极大似然估计 原理简单、使用方便、计算便捷 计算复杂 可以在不知道总体分布但知道样本矩的情况下对总体进行推断 需在总体分布已知的情况下才能使用（需对总体分布做假设） 未充分利用样本信息，即不一定是充分统计量的函数,估计精度不高，只有当样本量较大时才能保证其优良性 充分利用样本信息，一般都与充分统计量有关，估计精度高 一般情况下具有相合性 具有不变性和渐进正态性，在满足一定的正则条件下才具有相合性 不适用于总体矩不存在的分布，如柯西分布； 不唯一性 矩估计也不唯一 MVUE 等 Rao-Blackwell 定理：假设 $g(X)$ 是 $\\theta$ 的一个任意估计， $T$ 是一个充分统计量。那么 $g(X)$ 相对于给定 $T(X)$ 的条件期望是一个比 $g(X)$ 更好的估计量（至少不差于）。\n由于MVUE不一定总能达到C-R下界，这个定理提供了不用C-R下界的寻找MVUE的方法。\n定理告诉我们，通过条件期望，可以利⽤充分统计量改进估计量的效率。\n评估点估计的标准 很多概念，需要多过几遍，晕🥴\n无偏性 定义：若对 $\\forall \\theta \\in \\Theta,$ 有 $E_\\theta(\\hat{\\theta}) = \\theta$ 则称 $\\hat{\\theta}$ 为 $\\theta$ 的无偏估计。\n并非所有参数都存在无偏估计。\n参数是否可估：取决于是否有无偏估计。\n有效性 无偏估计比较方差，有偏估计比较均方误差。\n方差一般用来计算样本的离散程度，而均方误差则可以用做衡量模型拟合的一个度量\n无偏估计量不一定是最有效的！\n附：MSE 分解公式推导： 大样本情形 - 相合性，又称一致性 Consistency ==弱相合性==，当样本量 $n \\to \\infty$ 时，估计量 $\\hat{\\theta}_n$ 依概率收敛于真实参数 $\\theta.$ 记为 $\\hat{\\theta}_n \\xrightarrow{P} \\theta$ 即 $\\forall \\epsilon \u0026gt; 0, \\quad \\lim_{n \\to \\infty} P\\left (|\\hat{\\theta}_n - \\theta| \\geq \\epsilon\\right) = 0.$\n弱相合性被认为是对估计的一个最基本的要求，如果一个估计量，在样本量不断增大的时，他都不能把被估参数估计到任意的指定的精度 $\\varepsilon$ ，那么这个估计是很值得怀疑的。通常，不满足弱相合性要求的估计不予考虑。\n弱相合估计不止一个：样本有偏方差和样本无偏方差都是 $\\sigma^2$ 的相合估计。\n弱相合性判定：$lim_{n \\to \\infty}E(\\hat{\\theta}_n)=\\theta, \\quad lim_{n \\to \\infty}Var(\\hat{\\theta}_n)=0$\n==强相合性==，当样本量 $n \\to \\infty$ 时，估计量 $\\hat{\\theta}_n$ 几乎必然收敛于真实参数 $\\theta$\n即 $P\\left (\\lim_{n \\to \\infty} \\hat{\\theta}_n = \\theta\\right) = 1.$ 也称 $\\hat{\\theta}_n$ 几乎必然(Alomst Sure)收敛到 $\\theta$, 记为 $\\hat{\\theta}_n \\xrightarrow{a.s.} \\theta.$\n强弱相合性之间的差异类似强弱大数定律之间的差异：\n弱大数定律 强大数定律 依概率收敛 几乎必然收敛 $\\bar{X}_n \\xrightarrow{P} \\mu$ $\\bar{X}_n \\xrightarrow{a.s.} \\mu$ 允许样本均值无限次偏离 $\\mu$（但概率趋近于零）例如抛硬币实验中偶尔出现连续多次正面 彻底排除了无限次偏离的可能，确保样本均值最终“锁定”在 $\\mu$ 附近 由强大数定律可知，矩估计总是相合的。样本均值是总体均值的相合估计、样本标准差总体标准差的相合估计，样本变异系数是总体变异系数的相合估计。==而 MLE 要在满足一定正则条件的前提下才具有相合性==。\n大样本情形 - 渐进正态性 当样本容量趋向无穷大时，某些估计量（如最大似然估计）会收敛到一个正态分布。\n在相合性的基础上进一步关注收敛速度和分布特性。\n极大似然估计具有渐进正态性。\n不变性 定义：如果某一个统计量 $T(x)$ 是某个参数 $\\theta$ 的无偏估计具有某个性质，将统计量经过 $g$ 变换之后为 $g(T(x))$，变换后的参数 $g(\\theta)$ 同样具有原有性质。\n极大似然估计具有不变性，完备性具有不变性，矩估计也具有不变性\n无偏估计不具有不变性，若 $\\hat{\\theta}$ 是 $\\theta$ 的无偏估计， $g(\\hat{\\theta})$ 则不一定是 $\\theta$ 的无偏估计，除非 $g(\\theta)$ 是线性函数；\n充分性不具有不变性，除非 $g(\\theta)$ 是严格单调函数\n相合性不具有不变性，除非 $g(\\theta)$ 是连续函数\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E7%82%B9%E4%BC%B0%E8%AE%A1%E7%9B%B8%E5%85%B3%E5%BD%92%E7%BA%B3/","summary":"定义：用于估计未知参数 $\\theta$ 的统计量 $\\hat{\\theta}=\\theta(x_1, x_2, \u0026hellip;, x_n)$ 称为 $\\theta$ 的估计量，或称为 $\\theta$ 的点估计，简称估计。注意：==点估计是统计量（样本的函数）== 补充统计量定义：设 $x_1​, x_2​, \u0026hellip;, x_n​$ 为取自某总体的样本，若样本函数 $T=T(x_1​, x_2​, \u0026hellip;, x_n​)$ 中不含有任何未知参数，则称 $T$","title":"点估计相关归纳"},{"content":"主成分分析 主成分分析（Principal Component Analysis，PCA）是研究如何==通过原始变量的少数几个线性组合来解释原始变量绝大多数信息==的多元统计分析方法。在尽可能地保留原始变量信息的前提下进行降维，从而简化问题的复杂性，==抓住问题的主要矛盾==。 该方法主要基于众多原始变量之间有一定的相关性(即共线性)，则必然存在着起支配作用的共同因素这一想法，来对原始变量==协方差矩阵或相关系数矩阵==内部结构进行研究。\n使用前提 开展PCA一般需要满足以下前提条件：\n原始数据的变量数目较多，或有数据降维的需求，否则做主成分分析没有意义。 原始数据各个变量之间的共线性或相关关系较强，如果原始变量之间的线性相关程度很小，它们之间不存在简化的数据结构，这时进行主成分分析实际是没有意义的。 在应用PCA之前，需要对其适用性进行统计检验，检验方法有 KMO 抽样适合性检验(Measure of Sampling Adequacy，检验原始变量之间的相关系数和偏相关系数的相对大小) 巴特利特(Bartlett)球形检验(检验原始变量间的相关程度，即共线性程度) 等等。 PCA使用前提摘自主成分分析(Principal Component Analysis)——理论介绍\n通俗理解主成分分析 假如原始数据中总共有 100 个原始变量，为方便分析及解释，我们希望把这 100 个原始变量给“浓缩”一下。注意，“浓缩”体现出主成分分析的精髓：\n浓：原始变量蕴含的信息，在操作中要最大程度地保留； 缩：维度要降低。 我们想要尽可能地保留原始数据中的信息（因为降维不可避免地会造成信息的丢失，想象从 3 维空间到 2 维空间）什么时候信息的“利用率”最高呢？答案是正交的时候。变量间相互正交，可以理解为不同变量所蕴含的信息不重复，每个变量中蕴含的信息都是独一无二、无可替代的。\n信息这个词有点抽象，其实有个概念可以很好地量化“信息”这个词：方差。变量的“方差”所蕴含的是“信息量”（理解：变异程度越大，其中所含信息就越多）\n所以：思路清楚了：把原始变量组合成方差大且正交的新变量即可。\n具体操作 我们从包含变量方差信息的矩阵——协方差矩阵入手。\n既想降维，减少变量个数，又想尽可能地保留信息：把方差大的几个原始变量组合起来不就好了。\n对协方差矩阵进行==特征值分解==（实对称矩阵必可相似对角化）不就好了。取最大的几个特征值所对应的特征向量作为主成分即可。\n但是特征值分解有个问题，只适用于方阵，且更致命的是：计算量成本巨高。\n所以便引入使用==奇异值分解==的方法。详见推文\n关于奇异值分解，其思想跟 LoRA (针对LLM的一种轻量级微调技术)相似：通过“低秩近似”以减小算法上的开支：\n例如一个 985 × 211 的矩阵，有 20w+ 个元素需要计算；通过低秩近似，用 985 × 10 乘 10 × 211，便只需要计算不到 2w 个元素\n特征值分解和奇异值分解两种方法，都能达到目的。但途径不同，各有千秋。\n使用协方差矩阵还是相关系数矩阵 ？ 各变量量纲差不多时，使用协方差矩阵即可。反之则使用相关系数矩阵（多了一步标准化的操作）\n个人感觉无脑使用相关系数矩阵即可，不知道协方差矩阵在相关系数矩阵面前有什么优势？如有知道的朋友，还请指教。\n因子分析 面对主成分分析中主成分难以解释的问题，因子分析，被当做主成分分析的推广和发展，得到广泛应用。\n因子模型 $$x = \\mu + Af + \\varepsilon$$\n式中 f = $(f_1, f_2, \\cdots, f_m)\u0026rsquo;$ 为公共因子向量，$\\varepsilon = (\\varepsilon_1, \\varepsilon_2, \\cdots, \\varepsilon_p)\u0026rsquo;$ 为特殊因子向量，$A = (a_{ij}): p \\times m$ 称为因子载荷矩阵。\n模型假设：\n公共因子之间不相关； 特殊因子之间不相关； 公共因子和特殊因子之间不相关 下面的任务便是：估计因子载荷矩阵 $A$ 和公共因子向量 $f$。\n可以对比一下回归分析中 $Y=X\\beta+\\varepsilon$ 主要估计的是 $\\beta$ ，$X$ 是已知的观测值。\n估计方法 分为探索性因子分析和验证性因子分析：\n探索性因子分析：用于识别数据中可能存在的潜在因子结构，适合没有明确理论假设的情况。需要估计因子载荷矩阵、并进行因子旋转。适用于新开发的量表。 验证性因子分析：用于检验已有理论假设的因子结构是否与数据匹配，通常用于量表验证。研究者预先设定因子个数，并指定哪些观测变量属于哪个因子。建模完成后进行拟合优度检验。 探索性因子分析中因子载荷矩阵常用的估计方法：\n主成分法：（直接像 PCA 一样进行特征分解，又称谱分解） 极大似然法。使用极大似然法求得的载荷矩阵一般需要通过因子旋转进行优化，而使用主成分法得到的载荷矩阵则一般都不用。 主因子法 计算因子得分矩阵时使用 OLS 最小二乘法，此法有可能会失效。（因子分析的一个缺点）\n主成分分析与因子分析的区别与联系 “降维”，顾名思义就是降低数据维度（数据维度 = 原始变量个数）。多元统计分析中，常见的降维方法，除了 PFC 和 FA 外，还有独立成分分析或独立分量分析（Independent components analysis，缩写：ICA）\n主成分分析和因子分析都是基于线性模型的分析方法、都运用了原始变量的协方差矩阵或相关系数矩阵、使用的都是“降维”的思想，但两者“降维”的思路不一样，两者“降维”的思路差异，也就造成了两者的“可解释性”差异：\n主成分分析是通过将原始变量进行线性组合（称为主成分）来达到“降维”的目的，==可解释性弱==（主成分分析的一个缺点）； 而因子分析则是先提炼因子，再用所提炼的因子来表达原始变量，==可解释性强==。 主成分分析只涉及变量变换，不能作为模型来描述，本质上==不需要任何假定==；因子分析需要构造因子模型，并==有一些关键性的假定==。 举个例子：假设我们收集了 100 名学生在 4 门课程（语文、数学、地理、生物）中的考试成绩，我们希望找出数据的潜在结构，减少变量维度，便于分析。\n主成分分析（这里不太严谨，因为没保证正交）： 理科主成分 = 数学 + 0.7 生物 + 0.4 地理； 文科主成分 = 语文 + 0.3 生物 + 0.6 地理。 因子分析： 先设定因子：文科因子和理科因子 语文 = 1 × 文科因子 + 0 × 理科因子 数学 = 0 × 文科因子 + 1 × 理科因子 生物 = 0.3 × 文科因子 + 0.7 × 理科因子 地理 = 0.6 × 文科因子 + 0.4 × 理科因子 原始变量与主成分/因子间的关系 主成分分析：相关系数和载荷 $x_i = t_{i1}y_1 + t_{i2}y_2 + \\cdots + t_{ip}y_p, \\quad i = 1, 2, \\ldots, p$\n两个指标：相关系数 $\\rho(x_i, y_k)=\\frac{\\sqrt{\\lambda_k}}{\\sqrt{\\sigma_{ii}}}t_{ik}$ 和载荷(即主成分表达式中的系数)\n相关系数：单变量角度，忽略了其它原始变量的前提下==某原始变量与某主成分之间的关系==； 载荷：多变量角度，考虑到了其它原始变量在场的情况下某==原始变量与某主成分之间的关系==。 所有主成分对某原始变量的贡献率：m 个主成分 $y_1, y_2, \u0026hellip;, y_m$ 对原始变量 $x_i$ 的贡献率 $\\rho_{i \\cdot 1, \\cdots, m}^2 = \\sum_{k=1}^{m} \\rho^2(x_i, y_k) = \\sum_{k=1}^{m} \\frac{\\lambda_k t_{ik}^2}{\\sigma_{ii}}$；\n在解释主成分时，看哪个呢？马哲告诉我们：既需要考察相关系数，又需要考察载荷。\n因子分析：因子载荷矩阵和因子得分矩阵 ==因子载荷矩阵==是一个 变量×因子 的矩阵，其中元素即各个原始变量的因子表达式的系数，表达提取的公因子对原始变量的影响程度。\n通过因子载荷矩阵可以得到原始指标变量的线性组合，如 $X_1=a_{11}*F_1+a_{12}*F_2+a_{13}*F_3$, 其中 $X_1$ 为指标变量 1，$a_{11}, a_{12}, a_{13}$ 分别为与变量 $X_1$ 在同一行的因子载荷，$F_1, F_2, F_3$ 分别为提取的公因子；\n因子载荷矩阵 $A$ 中的元素 $a_{ij}$ ：表示 $x_i$ 与 $f_j$ 之间的==协方差或相关系数==（取决于是否标准化） 因子载荷矩阵 $A$ 的行元素平方和：反映公共因子对 $x_i$ 的影响，称为==共性方差==。特殊因子对 $x_i$ 的方差贡献则称为 $\\sigma_i^2$ ==特殊方差==。 因子载荷矩阵 $A$ 的列元素平方和：可视为公共因子 $f_i$ 对 $x_1, x_2, \u0026hellip;, x_p$ 的总方差贡献，是衡量公共因子 $f_i$ 重要性的一个尺度。 因子载荷矩阵 $A$ 的所有元素平方和：公共因子 $f_1, f_2, \u0026hellip;, f_m$ 对总方差的累计贡献。 公共因子的估计值, 称为因子得分 (factor scores)\n==因子得分矩阵==是一个 样本×因子 的矩阵，表示个体在潜在因子上的得分。它显示了每个观测对象（样本）在各个因子上的得分。因子得分反映了每个观测对象在因子上的位置或表现，数值越大，表示该对象在该因子上的得分越高。\n因子得分可以通过两种方法：加权最小二乘法和回归法（之所以称为回归法，是因为在回归分析中，条件均值被称之为回归函数）对不可观测的随机变量 $f_1, f_2, \u0026hellip;, f_m$ 的取值进行估计（但不算是参数估计）\n通过一个例子来加深理解：\n关于旋转💫 主成分分析和因子分析中的旋转，就是让各变量在单主成分（因子）上有高额载荷，而在其它主成分（因子）是只有小到中等的载荷。其目的是为了提高可解释性。\n让每个变量对某个主成分/因子的贡献更清晰（即让因子载荷更接近 0 或 1，减少多个因子共享同一变量的情况）。例如 某主成分 = $0.6X + 0.5Y + 0.5Z + \u0026hellip;$ 有些含糊 某主成分 = $0.9X\u0026rsquo; + 0.1Y\u0026rsquo; + 0.05Z\u0026rsquo; + \u0026hellip;$ 更清晰明了 让主成分/因子更加符合实际意义（即调整主成分/因子，使其更接近数据中方差较大的方向，同时减少信息混杂）。 可以看作一种对“主成分”或“因子”的优化。\n主成分分析中也可以旋转，但一般情况下并不需要旋转，因为既然选择了主成分分析，一般都不会太在意主成分的“可解释性”。而且虽然技术上可对主成分载荷矩阵进行旋转，但这会破坏主成分的正交性和方差排序特性。旋转后的成分不再保证方差递减，可能更接近因子分析的逻辑。\n时序数据能进行 PCA 和因子分析吗？ 绝大多数情况都不能，因为时间序列数据在绝大多数情况下都存在自相关性，不是简单随机样本 (要求 iid)，样本协方差矩阵 (相关系数矩阵) 不是总体协方差矩阵 (相关系数矩阵) 的无偏估计，贸然应用样本协方差矩阵 (相关系数矩阵) 会产生较大偏差。\n如处理后的数据消除了自相关性，则可考虑进行 PCA 和因子分析。\n聚类分析 聚类分析（Cluster Analysis）是一种无监督学习技术，主要用于将数据样本划分成多个类别或簇（Cluster），使得在同一簇内的数据相似性较高，不同簇之间的数据相似性较低。\n距离度量：闵考夫斯基距离（绝对值距离（又名曼哈顿距离）、欧氏距离、切比雪夫距离）、兰氏距离（加权版的曼哈顿距离）、马氏距离（协方差距离） 可参考其它推文链接1、 链接2 相似系数：夹角余弦、相关系数 不同的聚类方法： 不同聚类方法效果对比图 系统聚类法（层次聚类法） 凝聚的层次聚类：最短距离法、最长距离法、类平均法、重心法、中间距离法、离差平方和法（Ward\u0026rsquo;s method）. 上图图源 CSDN @阿伦很努力 推文 分裂的层次聚类：略，分裂其实就和凝聚反过来一样 动态聚类法（逐步聚类法，西瓜书中称为原型聚类）：K-means（初值敏感）、K-means++（随机初值，轮盘法更新聚类中心）、bi-kmeans（优化局部最优问题）。==K-Means 简单快速，适用大数据集，但只适用于平均值能被定义的情形，且初值敏感==。 基于密度的聚类方法：DBSCAN 新方法：核聚类、谱聚类、量子聚类等 用目测法在主成分得分图上可以进行直观的聚类，其中包含着正规聚类方法所反映不出的丰富信息，由此或许可以得到比正规聚类方法更为合理的聚类结果。\n判别分析 判别分析(discriminant analysis)属于“有监督学习”方法，从所谓“训练样本”经过分析计算得到一个判别规则，对新的数据可以利用判别规则判断新数据观测的类属。训练样本中既有用来分类的解释变量（自变量），又有真实的类属（标签，因变量）。\n常用判别分析方法有距离判别、Fisher判别和Bayes判别，Logistics回归也经常用在判别问题中（尤其是两类的判别），分类树也是用于判别的方法。\n距离判别：根据待判定样本与已知类别样本之间的距离远近做出判别。典型算法如：K最近邻(K-Nearest Neighbor，简称 KNN，“邻居是啥，我就是啥”)\n费希尔判别：通过“投影”简化问题。（与 SVM 的思想刚好相反，SVM 是通过超平面升维）。如二分类问题上的线性判别分析（Linear Discriminant Analysis，简称LDA）。 线性判别分析LDA 详见 [[机器学习 Machine Learning]] 贝叶斯判别：最大后验概率法、最小期望误判代价法。典型算法：朴素贝叶斯分类器(Naive Bayesian Classification)，哪个类别后验概率大，就属于哪个类别（拉普拉斯平滑解决特征未出现在训练集导致后验概率为 0 的问题） ^dkbbsu\n当标签（因变量）只有两个类时，判别分析问题与假设检验问题有相似之处。\n假设检验问题更强调统计推断的严谨性（拒绝原假设的理由一定要充分）； 两类的判别问题并不强调某个类别，或者按照先验概率、损失函数对不同类别施加不同的影响。 可以这样理解：假设检验是有“偏向”的，而判别分析是“平等”的 [[九阳真经 - 假设检验篇#假设检验的精髓：不平衡（Imbalance）]] 线性判别分析与其他分类算法有以下联系：转载\n与逻辑回归的联系：逻辑回归是一种基于概率模型的分类算法，它可以看作是线性判别分析在多元正态分布假设下的一种特殊情况。==逻辑回归假设输入变量之间是独立的，而线性判别分析则不作这一假设==。 与支持向量机的联系：支持向量机是一种通过最小化损失函数来训练分类器的算法。在线性情况下，==支持向量机可以看作是线性判别分析在不使用正态分布假设的情况下的一种特殊情况==。 与决策树的联系：决策树是一种基于树状结构的分类算法，它可以自动选择特征并构建分类器。决策树与线性判别分析的主要区别在于==决策树不需要任何假设，而线性判别分析则需要正态分布假设==。 与朴素贝叶斯分类器的联系：朴素贝叶斯分类器是一种基于贝叶斯定理的分类算法，它假设输入变量之间是独立的。朴素贝叶斯分类器与线性判别分析的主要区别在于==朴素贝叶斯分类器不需要正态分布假设==。 ==区分 K-Means 和 KNN 两种不同算法！K-Means 用于聚类分析；而 KNN 用于判别分析。==\n","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/","summary":"主成分分析 主成分分析（Principal Component Analysis，PCA）是研究如何==通过原始变量的少数几个线性组合来解释原始变量绝大多数信息==的多元统计分析方法。在尽可能地保留原始变量信息的前提下进行降维，从而简化问题的复杂性，==抓住问题的主要矛盾==。 该方法主要基于众多原始变","title":"多元统计分析复习笔记"},{"content":"随机事件层面的独立 在概率论里，\u0026ldquo;独立\u0026quot;并不意味着两个事件没有任何关系。 独立意味着一个事件的发生与否都不改变另一个事件发生的概率。 - 知乎文章\n数学定义见下：\n两个随机事件间的独立性定义： $P(AB)=P(A)P(B)$，即 $P(A|B)=P(A)$ 。不独立，又称相依。\n多个随机事件间的独立性定义：设有 N 个事件 $A_1, A_2, \u0026hellip;, A_N$ 对任意的 $1 ≤ i ≤ j ≤ k ≤ N$ 如下式成立：为什么不使用 $i, j, k \\in [1, N]$ ？避免重复吗？\n$$ \\begin{aligned} P(A_i A_j) \u0026amp;= P(A_i) P(A_j) \\quad \\text{两两独立} \\\\ P(A_i A_j A_k) \u0026amp;= P(A_i) P(A_j) P(A_k) \\quad \\text{三三独立} \\\\ \u0026amp;\\quad \\vdots \\\\ P(A_i A_j \\cdots A_N) \u0026amp;= P(A_i) P(A_j) \\cdots P(A_N) \\quad \\text{NN独立} \\end{aligned} $$ 则称 N 个事件 $A_1, A_2, \u0026hellip;, A_N$ 相互独立。\n多个事件间相互独立 \u0026lt;=\u0026gt; 多个事件间两两独立、三三独立、……、NN 独立\n概率为 0 的事件与任何事件都独立。\n将相互独立事件中的任一部分转换为对立事件，所得诸事件依然是相互独立的。\n如若事件 A 与事件 B 独立，则 $A$ 与 $\\bar{B}$, $\\bar{A}$ 与 $\\bar{B}$, $\\bar{A}$ 与 $B$ 均独立。\n多个随机事件情形同样满足\n随机试验层面的独立 独立性与相容性 相容性定义：如果 A 与 B 没有相同的样本点，则称 A 与 B 互不相容。即 A 与 B 不可能同时发生。\n==在事件概率不为 0 的前提==下讨论独立性与相容性的关系：\n独立 =\u0026gt; 相容：$P(AB)=P(A)P(B)≠0$ 说明 A 与 B 之间有交集，两者可能同时发生；但反之则不成立：相容事件不一定独立，例如掷一枚骰子，A: 点数\u0026lt;4 , B: 点数\u0026gt;3，AB 有交集，两者可能同时发生，相容，但不独立。 逆否命题：互不相容 =\u0026gt; 相依（不独立），互不相容 =\u0026gt; $P(AB)=0≠P(A)P(B)$ 说明 A 与 B 不独立。反之同样不成立：相依事件可能相容，例如掷两枚骰子，A：第一次为 6，B：总和大于 8，AB 两事件既相容又相依。 独立性：概率层面，反映前后实验结果是否相互影响； 相容性：事件层面，反映不同事件能否同时发生。\n可以说，两者其实是不同维度的概念。\n独立性与相关性 不相关是指两个变量的相关系数为0，$E(XY)=E(X)E(Y)$，两个变量之间没有线性关系。\n相关与否，仅是线性层面的。而独立与否，不仅包括线性层面，还包括非线性层面。\n于是可以说：独立的要求，比不相关，要更加严格。\n独立 =\u0026gt; 不相关，反之不成立，事件间不相关（无线性关系）≠\u0026gt; 事件间独立（可能存在非线性关系） 逆否命题：相关 =\u0026gt; 相依，反之同样不成立，相依（不独立，说明事件间有关系）≠\u0026gt; 相关（因为可能不是线性关系，而是非线性关系） ","permalink":"https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%85%B3%E4%BA%8E%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%AD%E7%9A%84%E7%8B%AC%E7%AB%8B/","summary":"随机事件层面的独立 在概率论里，\u0026ldquo;独立\u0026quot;并不意味着两个事件没有任何关系。 独立意味着一个事件的发生与否都不改变另一个事件发生的概率。 - 知乎文章 数学定义见下： 两个随机事件间的独立性定义： $P(AB)=P(A)P(B)$，即 $P(A|B)=P(A)$ 。不独立，又称相依。 多个随机事件间的独立","title":"关于概率论中的独立"},{"content":"本文使用腾讯 ima.copilt 来搭建知识库\n背景 在准备复试时从小红书、闲鱼等各种渠道获取到很多复试资料。但是文件繁多，部分资料还存在重复问题，不想花太多时间在搜索信息上面，想到前几天在小红书看到很多人推荐用腾讯的 ima.copilt 来搭建知识库。于是灵机一动，便萌生了通过搭建知识库以提升信息检索效率的法子。\n具体操作步骤 首先进入 https://ima.qq.com/ 下载并安装软件客户端； 登录，进入知识库界面； 上传文件，等待系统自动解析； 通过 RAG 技术，ima 会根据你所上传的文件（即你搭建的知识库）来回答你问的问题。 🙏 希望能够顺利进入复试\n","permalink":"https://20250303.xyz/posts/ai%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E4%BD%BF%E7%94%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%8F%90%E5%8D%87%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E5%87%86%E5%A4%87%E6%95%88%E7%8E%87/","summary":"本文使用腾讯 ima.copilt 来搭建知识库 背景 在准备复试时从小红书、闲鱼等各种渠道获取到很多复试资料。但是文件繁多，部分资料还存在重复问题，不想花太多时间在搜索信息上面，想到前几天在小红书看到很多人推荐用腾讯的 ima.copilt 来搭建知识库。于是灵机一动，便萌生了通过搭建知识库以提升信息检索效率的法子。 具体操作","title":"使用知识库提升考研复试准备效率"},{"content":"本人通过 Obsidian + Hugo + Github Page + Github Action 来实现文章管理及博客自动化搭建.\nHugo 渲染白名单 因为不想将 Obsidian 仓库中的一些隐私笔记发布到博客上, 于是选择使用 module.mounts 来配置允许 Hugo 渲染的文件夹.\n1module: 2 mounts: 3 - source: \u0026#34;content/posts/3. Resources-资源 未来/AI人工智能\u0026#34; 4 target: \u0026#34;content/posts/AI人工智能\u0026#34; 5 - source: \u0026#34;content/posts/4. Archives-归档/技术类归档\u0026#34; 6 target: \u0026#34;content/posts/技术类归档\u0026#34; 7\t...... 8 - source: \u0026#34;content/about.md\u0026#34; 9 target: \u0026#34;content/about.md\u0026#34; 10\t...... Hugo 主页展示白名单 因为主要想在博客主页展示技术类相关博文, 所以需要对主页展示内容进行过滤, 在 layouts/_default/list.html 进行配置即可, 具体可参考下面的代码:\n1{{- if .IsHome }} 2{{- $pages = where site.RegularPages \u0026#34;File.Dir\u0026#34; \u0026#34;in\u0026#34; (slice \u0026#34;posts\\\\AI人工智能\\\\\u0026#34; \u0026#34;posts\\\\技术类归档\\\\\u0026#34;) }} 3{{- $pages = where $pages \u0026#34;Params.hiddenInHomeList\u0026#34; \u0026#34;!=\u0026#34; \u0026#34;true\u0026#34; }} 4{{- end }} 倘若不知道文件夹(File. Dir)路径, 尤其是我这种有 mounts 映射的情况, 可以通过在 layouts/_default/list.html 中添加 {{ range site.RegularPages }} \u0026lt;p\u0026gt;{{ .File.Path }} → {{ .File.Dir }}\u0026lt;/p\u0026gt; {{ end }} 代码块, Hugo 会输出文件夹 (File. Dir) 路径 (类似 Python 的 print 函数.), 见下图:\n踩坑记录: 一开始以为在 config.yml 里面设置 mainSections 相关参数即可, 折腾半天. 后面静下心来慢慢读 Hugo 的官网网站上的相关教程, 最后也多亏了 GPT 帮忙.\n上传 Github 白名单 可通过 .gitignore 文件进行设置.\n1 2content/posts/.obsidian/ # .obsidian文件夹内里面有很多杂七杂八的东西 3content/posts/-1. Books/ # 这一目录用于存放电子书, 占用空间 4 5.history/ # VSC的自动保存 6 7public # 无需上传public文件, 因为设置了Github Action自动部署 这个网上很多大佬都有教程 ","permalink":"https://20250303.xyz/posts/%E6%8A%80%E6%9C%AF%E7%B1%BB%E5%BD%92%E6%A1%A3/hugo-%E6%B8%B2%E6%9F%93%E5%8F%8A%E4%B8%BB%E9%A1%B5%E5%B1%95%E7%A4%BA%E7%99%BD%E5%90%8D%E5%8D%95/","summary":"本人通过 Obsidian + Hugo + Github Page + Github Action 来实现文章管理及博客自动化搭建. Hugo 渲染白名单 因为不想将 Obsidian 仓库中的一些隐私笔记发布到博客上, 于是选择使用 module.mounts 来配置允许 Hugo 渲染的文件夹. 1module: 2 mounts: 3 - source: \u0026#34;content/posts/3. Resources-资源 未来/AI人工智能\u0026#34; 4 target: \u0026#34;content/posts/AI人工智能\u0026#","title":"Hugo 渲染及主页展示白名单"},{"content":"参考: # hexo之mongodb修改twikoo评论的管理密码 以及 # Twikoo 找回暗号及密码\n核心目标: 把 MongoDB 中的\u0026quot;ADMIN_PASS\u0026quot;值删除.\n","permalink":"https://20250303.xyz/posts/%E6%8A%80%E6%9C%AF%E7%B1%BB%E5%BD%92%E6%A1%A3/%E5%BF%98%E8%AE%B0-twikoo-%E7%AE%A1%E7%90%86%E9%9D%A2%E6%9D%BF%E7%9A%84%E5%AF%86%E7%A0%81%E6%80%8E%E4%B9%88%E5%8A%9E/","summary":"参考: # hexo之mongodb修改twikoo评论的管理密码 以及 # Twikoo 找回暗号及密码 核心目标: 把 MongoDB 中的\u0026quot;ADMIN_PASS\u0026quot;值删除.","title":"忘记 Twikoo 管理面板的密码怎么办"},{"content":"记得在看《被讨厌的勇气》后，里面有一句话让我印象特别深刻：“影响你的不是事件本身，而是你==看待事件的方式==。”比如精神内耗与否，就与==如何看待事情==息息相关[[关于精神内耗#内耗和复盘]]。再比如经验不是事情本身, 是我们理解事情的方式, 构成了经验。但是需要注意的是，==不要脱离或弱化了“物质的决定性”==。\n","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E4%B8%BB%E8%A7%82%E8%83%BD%E5%8A%A8%E6%80%A7/","summary":"记得在看《被讨厌的勇气》后，里面有一句话让我印象特别深刻：“影响你的不是事件本身，而是你==看待事件的方式==。”比如精神内耗与否，就与==如何看待事情==息息相关[[关于精神内耗#内耗和复盘]]。再比如经验不是事情本身, 是我们理解事情的方式, 构成了经验。但是需要注意的是，==不","title":"关于主观能动性"},{"content":" 如果你因为失去了太阳而流泪，那么你也将失去群星。 - 泰戈尔\n什么是精神内耗 精神内耗表现为个体对未知不确定事件可能的消极原因或后果进行反复的揣摩、思考与分析。 - 百度百科\n心理内耗是指人的自我控制需要消耗心理资源，当资源不足时，个体即处于一种所谓内耗的状态，长期的内耗会让人感到疲惫。 - MBA智库\n内耗和复盘 精神内耗 Vs 真正的复盘，他们之间区别在于：==精神内耗==是过度的无用思考，不断地==反刍==：“我当时为什么要这样做”；==真正的复盘==，虽然也会分析当时做错事的原因，但其重点是：以后==怎么做==以避免再犯类似的错误。 ^ah26bl\n如何应对精神内耗 不要试图去压制自己的反刍思维，这样反而可能会因为白熊效应让负面思考变得更加频繁，这可能会形成恶性循环。\n白熊效应，又称白象效应或反弹效应：刻意抑制某些想法时，实际上会使这些想法更容易浮出水面。一个例子是，当某人积极地试图不去想一只白熊时，他实际上更有可能想象一只白熊。 - Wikipedia\n具体怎么做：\n自我接纳（只要你完成了今天的目标，你就问心无愧。其它的事，无需今日承担。 - 小红书评论） 旁观视角（分清情绪和问题） 积极行动（行动是治愈反刍的良药） 有意识地转移注意力（如正念冥想、心理干预）等等 ","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E7%B2%BE%E7%A5%9E%E5%86%85%E8%80%97/","summary":"如果你因为失去了太阳而流泪，那么你也将失去群星。 - 泰戈尔 什么是精神内耗 精神内耗表现为个体对未知不确定事件可能的消极原因或后果进行反复的揣摩、思考与分析。 - 百度百科 心理内耗是指人的自我控制需要消耗心理资源，当资源不足时，个体即处于一种所谓内耗的状态，长期的内耗会让人感到疲惫。 - MB","title":"关于精神内耗"},{"content":"\u0026ldquo;矛盾\u0026quot;的意思即\u0026quot;两面性\u0026rdquo;(道家的\u0026quot;阴阳\u0026quot;)?\n思考的两面性 - 人类认知边界 × 语言符号系统 思考的正面性，或者通俗来说，思考的优点，是显而易见的。\n下面主要来说说==思考的反面性==：\n语言的界限就是世界的界限。 - 《维特根斯坦与哲学》 知乎的一个解读 没有语言，人类就无法思考。 - 索绪尔 知乎对索绪尔或语言学的一个介绍 道可道，非常（恒）道。 - 老子《道德经》\n结构主义认为语言的结构决定了我们的思维模式。\nearly-nineteenth-century science was in the grip of philosophical determinism—the belief that everything that happens is determined in advance by the initial conditions of the universe and the mathematical formulas that describe its motions. - The lady tasting tea. 也是结构主义？\n是的，但传统结构主义主要应用于语言学、人类学、文化研究等，主要强调符号和意义的系统性。\n深入了解一下结构主义, 索绪尔, 列维斯特劳斯 #哲学 我们只能想到我们能够想到的东西（意象无法表达）, 或者说，我们只能表达出语言能够表达出地东西，只能思考『能够被语言表达』的东西。语言之外的东西，我们无法思考。就好比盲人无法思考光明、聋人无法思考声音、人类无法思考高维空间（“上帝面前，我们都是聋子”）\n人类在认知进程中不断重构\u0026quot;可言说\u0026quot;与\u0026quot;不可言说\u0026quot;的边界形态，例如随着阅历或认知的进长，我们渐渐能够表达从前无法用语言表达的想法或观点，但是，这个边界本身是永远不能被消除的。\n《庄子·内篇·养生主第三》中的哲思：\u0026ldquo;吾生也有涯，而知也无涯。以有涯随无涯，殆已！\u0026quot;，人类的伟大恰在于明知\u0026quot;殆\u0026quot;而仍不懈追寻。\nDDL 的双面性 设计应该是感性的还是理性的？设计是感性重要还是理性重要？ 此类问题一经出现便会引起争议无数，每个人对此都有自己的见解与理由，还有些人主张“理性与感性需要平衡”“既要理性也要感性”，==此类观点看似正确，却缺乏任何实质性的指导价值==。 - 设计的两面性：理性决策与感性表达\n马哲所谓的\u0026quot;辩证分析法\u0026rdquo;, 听上去似乎正确, 但实际上, 就像上面引用的文章所提到的: 缺乏实质性的指导价值.\n","permalink":"https://20250303.xyz/posts/%E9%9A%8F%E7%AC%94/%E5%85%B3%E4%BA%8E%E9%A9%AC%E5%93%B2%E4%B8%AD%E7%9F%9B%E7%9B%BE%E4%B8%8E%E9%98%B4%E9%98%B3/","summary":"\u0026ldquo;矛盾\u0026quot;的意思即\u0026quot;两面性\u0026rdquo;(道家的\u0026quot;阴阳\u0026quot;)? 思考的两面性 - 人类认知边界 × 语言符号系统 思考的正面性，或者通俗来说，思考的优点，是显而易见的。 下面主要来说说==思考的反面性==： 语言的界限就是世界的界限。 - 《维特根斯坦","title":"关于马哲中矛盾与阴阳"},{"content":"《湖南农民运动考察报告》 矫枉必须过正, 不过正不能矫枉.\n政权、族权、神权、夫权这四种权利代表了全部封建宗法的思想与制度.\n引而不发，跃如也。 - 农民的使命要农民去完成, 别人代庖是不对的\n《井冈山的斗争》 民主主义: 官长不打士兵, 官兵待遇平等, 士兵有开会说话的自由, 废除繁琐的礼节, 经济公开.\n家族主义: 一个村子一个姓, 支部会议简直同时就是家族会议.\n群众的政治训练 - 群众需要有政治素养\n党与政府的关系\n民权主义革命 - 民权主义是三民主义之一.\n《关于纠正党内的错误思想》 ==本位主义==, 一切只知道为四军打仗, 不知道武装地方群众是红军的重要任务之一. 这是一种放大了的小团体主义.\n俘虏兵的加入, 带来了浓厚的雇佣军队的思想, 使单纯军事观点有了==下层基础==.\n过分相信军事力量, 而不相信人民群众的力量.\n党对军事工作没有积极的注意和讨论, 也是形成一部分同志的单纯军事观点的原因.\n==关于极端民主化==, 『由下而上的民主集权制』, 『先交下级讨论, 再交上级决议』?\n有争论的问题, 要把是非弄明白, 不要调和敷衍.\n党内批评, 不要成了攻击个人.\n绝对平均主义的来源, 和政治上的极端民主化一样, 是手工业者和小农经济的产物, 不过一则见之于政治方面, 一则见之于物质生活方面罢了.\n看这篇文章有看加缪的《堕落》的感觉，被作者当面揭露出劣根性的感觉。原来自己曾经有过的许多想法，其实都有一个专有名词（如 xx 主义），一下抓住事情的本质并提炼成文字，伟人果然是伟人。\n","permalink":"https://20250303.xyz/posts/%E9%98%85%E8%AF%BB/%E6%AF%9B%E9%80%89%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","summary":"《湖南农民运动考察报告》 矫枉必须过正, 不过正不能矫枉. 政权、族权、神权、夫权这四种权利代表了全部封建宗法的思想与制度. 引而不发，跃如也。 - 农民的使命要农民去完成, 别人代庖是不对的 《井冈山的斗争》 民主主义: 官长不打士兵, 官兵待遇平等, 士兵有开会说话的自由, 废除繁琐的礼节, 经济公开.","title":"毛选读书笔记"},{"content":"本地部署都很吃配置\n知识库 KB 知识库（Knowledge base）是用于知识管理的一种==特殊的数据库==，以便于有关领域知识的采集、整理以及提取。知识库中的知识源于领域专家，它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。 - Wikipedia\n例如：法院判例库、医院病例库、产品数据库、某领域论文库、哈利波特魔咒库、章鱼哥的食谱……\n注意！知识库与 LLM 预训练语料库两者不是一个概念，预训练语料库为非结构化文本（乱七八糟的文字, 未清洗）, 且一旦模型预训练完成就不能再更改预训练语料 (如ChatGPT 4o的预训练语料库便截止2023年10月); 而知识库则为结构化或半结构化文本 (有一级标题、二级标题或其他逻辑结构)，且是定期更新与维护的。\n看到很多人推荐 IMA 来搭建知识库.\n智能体 Agent 智能体（英语：intelligent agent）指一个可以==观察周遭环境并作出行动以达致目标==的自主实体。它通常是指（但不一定是）一个软件程序。“智能体”是目前人工智能研究的一个核心概念，统御和联系着各个子领域的研究。 - Wikipedia\n智能体（Agent）是指能够==感知环境并采取行动以实现特定目标==的代理体。它可以是软件、硬件或一个系统，具备自主性、适应性和交互能力。智能体通过感知环境中的变化（如通过传感器或数据输入），根据自身学习到的知识和算法进行判断和决策，进而执行动作以影响环境或达到预定的目标。 - 百度百科\n常见 AI Agent 平台: 字节扣子 (Coze)、腾讯元器、Dify、FastGPT\n所有妄图通过缩短工序而提升效率的低代码平台都会死. (Agent 平台就是 LLM 时代的低代码平台) - 小红书评论\n检索增强生成 RAG Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so ==it references an authoritative knowledge base outside of its training data sources before generating a response.== Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. ==RAG extends the already powerful capabilities of LLMs to specific domains or an organization\u0026rsquo;s internal knowledge base, all without the need to retrain the model.== It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. - AWS\n==检索增强生成==（英语：Retrieval-augmented generation, RAG ) 是赋予生成式人工智能模型信息检索能力的技术。检索增强生成优化大型语言模型(LLM) 的交互方式，让模型根据指定的一组文件回应用户的查询，并使用这些信息增强模型从自身庞大的静态训练数据中提取的信息。检索增强生成技术促使大型语言模型能够使用特定领域或更新后的信息。 - Wikipedia\n可以将RAG理解为\u0026quot;知识库+LLM\u0026quot;, \u0026ldquo;==AI 老员工==\u0026ldquo;的比喻十分形象。\n主流 RAG 框架: LangChain、RAGFlow、AnythingLLM\nRAG 具体工作逻辑 参考 Why does the LLM not use my documents?\nRAG 对知识库中的文件，并不是以一个文档一个文档地对待的，而是将所有文档都“打碎”并进行词嵌入； 当用户发起对话时，也将用户发送的对话内容“打碎”并进行词嵌入； 通过余弦相似度从知识库中寻找与用户发送内容“相似”的“碎片”。 整合 LLM，最后输出 RAG (which is what we use) enables us to chunk the document and then ask retrieve only the bits and pieces the make sense for your question and use that in the context window. This makes larger documents easier to use, but it is at the expense of these types of \u0026ldquo;whole document\u0026rdquo; understandings.- AnythingLLM 社区 RAG, by its very nature is pieces of relevant content. Not the entire text - AnythingLLM 社区\n总结：RAG 的作用不是全文理解，而是从知识库找到与我们所问的问题相关的“碎片”。至于全文理解，这件事是智能体干的事情，当然 LLM 也干得不错。为什么不本地部署一个强悍的 LLM ? 可以参考这里[[#总结]].\n微调 Fine-tuning 微调（又称大模型微调，英语：fine-tuning）是==深度学习中迁移学习的一种方法==，其中预训练模型的权重会在新数据上进行训练。微调可以在整个神经网络上执行，也可以仅在其部分层上执行，此时未进行微调的层会被“冻结”（在反向传播步骤中不更新） - Wikipedia 微调通常通过==监督学习==完成，但也有使用弱监督进行模型微调的技术。 - Wikipedia Low-rank adaptation (==LoRA==) is an adapter-based technique for efficiently fine-tuning models. - Wikipedia\nLLM 总不能样样精通.\n可以理解为：通过利用已有的知识来提高模型在新任务上的表现。\n任何预训练好的模型都能微调！预训练-微调方法属于基于模型的迁移方法（Parameter/Model-based TransferLearning）\n总结 角色 作用 依赖 知识库 ==专业知识库==，提供事实性、权威性的知识数据 被RAG或智能体查询 智能体 ==Agent==，负责与用户交互，并调用RAG或知识库来回答问题 可能使用RAG提升回答能力 RAG ==一种AI架构==，从知识库检索信息，并结合语言模型生成答案 需要知识库作为数据来源 微调 ==迁移学习的一种方法==，使通用模型适配特定领域任务 需要知识库作为微调数据 形象的比喻：微调是考前复习，RAG 是开卷考试。\n然而本地部署并微调一个大模型，对于个人来说成本是极大的；如果退而求其次，微调一个小一点的模型，效果又可能不好；所以实践证实:==在本地部署一个蒸馏版的大模型+RAG 技术是一个更优的解决方案==。\n","permalink":"https://20250303.xyz/posts/ai%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%99%BA%E8%83%BD%E4%BD%93rag%E5%BE%AE%E8%B0%83/","summary":"本地部署都很吃配置 知识库 KB 知识库（Knowledge base）是用于知识管理的一种==特殊的数据库==，以便于有关领域知识的采集、整理以及提取。知识库中的知识源于领域专家，它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。 - Wikipedia 例如：法院判例库、医院病例库、产品数据","title":"知识库、智能体、RAG、微调"},{"content":"✨ 关于我 ✨\n本科统计学在读，对数据科学感兴趣。兴趣广泛，涉猎天文、业余天文摄影、音乐\n目前正在准备考研复试 / 春招\n📚 这里有什么 📚\n▫️ 数据科学相关：统计学、机器学习的学习笔记\n▫️ 人工智能相关：记录捣鼓 AI 过程中的踩坑经历以及灵感\n▫️ 一些阅读笔记：偶尔写些读书记录\n欢迎同好一块交流 🤝\n","permalink":"https://20250303.xyz/about/","summary":"✨ 关于我 ✨ 本科统计学在读，对数据科学感兴趣。兴趣广泛，涉猎天文、业余天文摄影、音乐 目前正在准备考研复试 / 春招 📚 这里有什么 📚 ▫️ 数据科学相关：统计学、机器学习的学习笔记 ▫️ 人工智能相关：记录捣鼓 AI 过程中的踩坑经历以及灵感 ▫️ 一些阅读笔记：偶尔写些读书记录 欢迎同好一块交流 🤝","title":"关于我"}]