<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>机器学习 Machine Learning | Lovegood&#39;s Blog</title>
<meta name="keywords" content="统计学, 机器学习">
<meta name="description" content="打开非线性空间的钥匙：激活函数 激活函数的主要作用是引入非线性，使神经网络能够处理非线性问题，从而增强模型的表达能力，使其能够拟合复杂的函数关系。 LSTM 便是通过激活函数实现“遗忘门”、“输入门”等功能： 各种算法 在二分类问题中，模型通常会输出一个概率得分（如 Softmax），然后通过设定">
<meta name="author" content="Lovegood">
<link rel="canonical" href="https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-machine-learning/">
<meta name="google-site-verification" content="G-LZESR1K8WK">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="../../../assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://20250303.xyz/img/xx.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://20250303.xyz/img/xx.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://20250303.xyz/img/xx.gif">
<link rel="apple-touch-icon" href="https://20250303.xyz/xx.gif">
<link rel="mask-icon" href="https://20250303.xyz/xx.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-machine-learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>



<link rel="stylesheet" href="https://cdn.staticfile.org/lxgw-wenkai-screen-webfont/1.6.0/style.css" />


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-LZESR1K8WK"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-LZESR1K8WK', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="机器学习 Machine Learning" />
<meta property="og:description" content="打开非线性空间的钥匙：激活函数 激活函数的主要作用是引入非线性，使神经网络能够处理非线性问题，从而增强模型的表达能力，使其能够拟合复杂的函数关系。 LSTM 便是通过激活函数实现“遗忘门”、“输入门”等功能： 各种算法 在二分类问题中，模型通常会输出一个概率得分（如 Softmax），然后通过设定" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-machine-learning/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-03-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-13T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="机器学习 Machine Learning"/>
<meta name="twitter:description" content="打开非线性空间的钥匙：激活函数 激活函数的主要作用是引入非线性，使神经网络能够处理非线性问题，从而增强模型的表达能力，使其能够拟合复杂的函数关系。 LSTM 便是通过激活函数实现“遗忘门”、“输入门”等功能： 各种算法 在二分类问题中，模型通常会输出一个概率得分（如 Softmax），然后通过设定"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://20250303.xyz/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "机器学习 Machine Learning",
      "item": "https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-machine-learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "机器学习 Machine Learning",
  "name": "机器学习 Machine Learning",
  "description": "打开非线性空间的钥匙：激活函数 激活函数的主要作用是引入非线性，使神经网络能够处理非线性问题，从而增强模型的表达能力，使其能够拟合复杂的函数关系。 LSTM 便是通过激活函数实现“遗忘门”、“输入门”等功能： 各种算法 在二分类问题中，模型通常会输出一个概率得分（如 Softmax），然后通过设定",
  "keywords": [
    "统计学", "机器学习"
  ],
  "articleBody": "打开非线性空间的钥匙：激活函数 激活函数的主要作用是引入非线性，使神经网络能够处理非线性问题，从而增强模型的表达能力，使其能够拟合复杂的函数关系。\nLSTM 便是通过激活函数实现“遗忘门”、“输入门”等功能：\n各种算法 在二分类问题中，模型通常会输出一个概率得分（如 Softmax），然后通过设定一个阈值（Threshold）来决定最终的分类结果（正例 or 负例）。 ^p2v94r\n若是多分类问题，则将多分类任务拆为若干个二分类任务，拆分方法有一对一、一对其余和多对多，详见机器学习中的多分类任务详解\n损失函数不仅可以从代数、几何角度来理解，还可以从概率角度来理解，强推视频合集-正则化\n逻辑回归，Logistic Regression，对数几率回归 逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，$y=\\frac{1}{1+e^{-(w^Tx+b)}}$，使得逻辑回归称为了一个优秀的分类算法。注意，虽然它的名字中有“回归”二字，但其实是一个分类算法。\n似然函数 $L(w) = \\prod [p(x_i)]^{y_i} [1 - p(x_i)]^{1 - y_i}$, 关于其中的累乘项 $P(y_i | x_i) = p(x_i)^{y_i} [1 - p(x_i)]^{1 - y_i}$:\n当 $y_i = 1$ 时，该公式变为 $p(x_i)$，即模型预测 $x_i$ 为 1 的概率。 当 $y_i = 0$ 时，该公式变为 $1 - p(x_i)$，即模型预测 $x_i$ 为 0 的概率。 由于乘积运算不方便计算，我们通常取对数得到对数似然函数（log-likelihood）:$log L(w) = \\sum y_i \\log p(x_i) + (1 - y_i) \\log (1 - p(x_i))$\n最大化对数似然等价于最小化交叉熵损失函数（Cross-Entropy Loss，即对似然函数取负对数）。注意这里不使用 MSE 来构造损失函数 $\\mathcal{L}_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p(x_i))^2$ 是因为这个损失函数非凸（一个函数 $f(w)$ 关于 $w$ 是凸的，当且仅当它的 Hessian 矩阵 $H(f)$ 半正定），使得优化过程容易陷入局部最优，难以收敛到全局最优解。\n如何求解？\n梯度下降法（通过一阶导数来找下降方向，又称“最速下降法”）\n批量梯度下降法（Batch Gradient Descent，BGD）==每次迭代都使用全体样本==，最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。 随机梯度下降（Stochastic Gradient Descent，SGD）==每次迭代只使用一个样本（或全体样本的某个子集）==，最小化每条样本（或全体样本的某个子集）的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向，但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。 牛顿法（在初值点附近对函数做泰勒展开，取展开式的前几项来寻找下降方向）。\n从本质上去看，牛顿法是二阶（或更高阶，取决于取几阶泰勒展开）收敛，梯度下降是一阶收敛，所以牛顿法就更快。 从几何上说，牛顿法就是用一个二次（或更高次）曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。 牛顿法能够根据最优点距离自适应选择迭代步长（学习率）。 ==缺点是每次都需要求解复杂的Hessian矩阵的逆矩阵==。 于是改善出了：拟牛顿法：使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。 正则化，Regularizaation（花书：凡是可以减少泛化误差而不是训练误差的方法都可以称为正则化方法），目的是防止模型过拟合，让误差函数不仅取决于模型拟合的好坏（如 SSE 大小等），还取决于模型的复杂度（如模型中非线性参数的大小，因为一般模型非线性参数越多越大，模型就越复杂；另外大参数也会放大噪声和误差）\nL1 正则化，Lasso 回归，使用曼哈顿距离构造可行域（约束），L1通常用于增加稀疏性，有点类似因子旋转[[多元统计分析复习笔记#关于旋转💫]]。 L2 正则化，Ridge 回归，使用欧几里得距离构造可行域（约束），L2一般用于减小复杂度。 但是没有 L0.5 正则化，因为 L0.5 范数对应的可行域不是凸集，可见图片；那有没有 L3、L4 正则化呢？单纯从凸优化的角度来说，是没问题的。 丢弃法，Dropout，随机让神经网络中的一些神经元失效。\n逻辑回归和最大熵模型本质上没有区别。(最大熵模型可参考一文带你了解最大熵模型（含详细推导）)\n与 SVM 的对比：LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布。—— 【机器学习】逻辑回归（非常详细）\n线性判别分析 LDA \u0026 Fisher判别 可参考[[多元统计分析复习笔记#判别分析]]\n线性判别分析（LDA）的目标是找到一个投影方向，使得投影后的类间散度最大化，同时类内散度最小化，从而实现最佳的类别分离。\n决策树 Decision Tree 划分指标：\n信息增益（其中会使用到信息熵作为集合纯度的度量），信息增益越大，则意味着使用该属性来进行划分所获得的“纯度提升”越大。信息增益对可取值数目较多的属性有所偏好； 增益率，与信息增益相反，增益率对可取值数目较少的属性有所偏好； 基尼指数，反映从数据集中随机抽取两个样本，其类别标记不一致的概率。基尼指数值越小，数据集的纯度越高。 剪枝处理：避免“过拟合”，基本策略有预剪枝和后剪枝两种。\n预剪枝：在生成决策树的过程中，边生成边剪枝；==好处==是训练时间开销小（相较于后剪枝而言，因为后剪枝要在决策树建立完成后，自底向上地对树中的所有非叶节点进行注意考察），但是“贪心”本质可能导致欠拟合 后剪枝：先把决策树生成完，最后再剪枝。优点是欠拟合风险小，但是训练时间开销大。 随机森林可见后文[[机器学习 Machine Learning#“三个臭皮匠，顶一个诸葛亮” - 模型融合与集成学习]]部分\n神经网络 感知机，指单层的人工神经网络，区别于较复杂的多层感知机。\n前馈神经网络，在前馈网络中，资讯总是朝一个方向移动，从来不会倒退。可见图\n误差逆传播算法（BP算法，反向传播算法）\n经典和目前主流算法：卷积神经网络、循环神经网络、LSTM、GRU、Transformer、BERT\n支持向量机 SVM SVM是一种二类分类模型，核心目标是在特征空间中找到一个能够最大化两类数据间隔（Margin）的决策边界。\n线性可分SVM：当训练数据线性可分时，通过==硬间隔==最大化可以学习得到一个线性分类器，即硬间隔SVM。 线性SVM：当训练数据不能线性可分但是可以近似线性可分时，通过==软间隔(允许部分数据点被错误分类，以提高泛化能力)==最大化也可以学习到一个线性分类器，即软间隔SVM。 非线性SVM：当训练数据线性不可分时，通过使用==核技巧(将数据映射到高维空间，使其线性可分)==和软间隔最大化，可以学习到一个非线性SVM。 EM 算法 是在含有隐变量的概率模型中寻找参数最大似然估计或者最大后验估计的算法。\n最大期望算法经过两个步骤交替进行计算：\n第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值； 第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。 隐马尔可夫模型 用来描述一个含有隐含未知参数的马尔可夫过程\nhttps://www.cnblogs.com/skyme/p/4651331.html\nNLP 中有这个算法\n写累了，先暂略\n其它 贝叶斯分类器见多元统计分析相关内容[[多元统计分析复习笔记#^dkbbsu]]\n聚类算法见多元统计分析相关内容[[多元统计分析复习笔记#聚类分析]]\n如何处理模型训练中的高方差或高偏差问题？ https://www.jvruo.com/archives/1277/\n评价模型的指标 准确率、精度 $Accuracy=\\frac{TP+TN}{TP+FP+FN+TN}$ 精准率、精确率、查准率 $Precision=\\frac{TP}{TP+FP}$，预测为正例的样本中，有多少是正确的 召回率、查全率 $Recall=\\frac{TP}{TP+FN}$，真实为正例的样本中，有多少被成功找到了 ==区分查准\u0026查全！精确率和召回率两者的关系是此消彼长的，于是给出 F1 值进行调和平均== F1 值： $F1=2·\\frac{Precision·Recall}{Precision+Recall}$, 即 Precision 和 Recall 的调和平均 引申 $F_\\beta$ 值，用参数 $\\beta$ 来度量 Precision 和 Recall 哪个更重要：Recall 的重要性是 Precision 的 $\\beta$ 倍，F1 值算是特殊情况，把 Precision 和 Recall 看作同等重要。$F_\\beta=(1+\\beta^2)·\\frac{Precision·Recall}{\\beta^2·Precision+Recall}$ ROC 曲线（Receiver Operating Characteristic curve 接收者操作特征曲线）与 AUC 值，==适合数据集中类别分布均衡时使用==： $\\text{True Positive Rate}=\\frac{TP}{TP+FN}$, 即 $Recall$. $\\text{False Positive Rate} = \\frac{FP}{FP+TN}$ 设置阈值从 0-1（如何理解这里的阈值？[[机器学习 Machine Learning#^p2v94r]]），可以得到不同的 TPR 和 FPR。以 TPR 为纵坐标、FPR 为横坐标，即可绘制出 ROC 曲线，我们希望 TPR↑，FPR↓，即希望曲线靠近左上角，故给出量化标准 AUC 值：ROC 曲线下的面积。AUC 值越大越好。 PR 曲线：当我们的数据集中==类别分布不均衡时==我们可以用 PR 曲线代替 ROC 曲线．PR 曲线与 ROC 曲线的区别在于 PR 曲线以 Recall 作为 x 轴，Precision 作为 y 轴。我们希望 Precision↑，Recall↑，即希望曲线靠近右上角， ROC 曲线与 PR 曲线的区别与联系： 可以说，每一条 ROC 曲线都与一条 PR 曲线相对应，因为混淆矩阵的 TPR-FPR 数值对和 Precision-Recall都有唯一的数值对。 如果在 ROC 空间中，曲线 1 优于曲线 2，那么在 PR 空间中，曲线 1 对应的 PR 曲线同样优于曲线 2 对应的 PR 曲线。 ROC 曲线对正负两类样本同样关系，而 PR 曲线则对正例更加关心。 ROC 适合类别均衡时使用，而 PR 曲线适用类别不均衡场景。 敏感性、灵敏度、真阳性率 $Sensitivity=\\frac{TP}{TP+FN}$ 特异性、特异度 $Specificity=\\frac{TN}{TN+FN}$ AIC 赤池信息量和BIC 贝叶斯信息量，是基于似然函数的估计方法，主要用于统计建模，尤其是在回归分析和时间序列分析等领域，==在机器学习领域并不常用==。[[回归分析#^grw9h5]] Kaggle比赛中提高成绩的3个主要地方 特征工程 特征工程（Feature Engineering）特征工程是将原始数据转化成更好的表达问题本质的特征的过程，使得将这些特征运用到预测模型中能提高对不可见数据的模型预测精度。\n图源深度了解特征工程。感觉有点像数据分析中的数据预处理。\n编码方式 Encoding Method 频率编码（Frequency Encoding）、嵌入编码（Embedding Encoding）的优缺点。\n调参 参考机器学习：模型调参大法总结\n“三个臭皮匠，顶一个诸葛亮” - 模型融合与集成学习 模型融合（Model Fusion）指的是将多个不同的机器学习模型的预测结果进行组合，生成一个新的、更为准确的预测。以提高最终的预测性能、稳定性和泛化能力。它的核心思想是利用多个模型的优势，减少单一模型的偏差和方差，通常用于提升分类或回归任务的精度。\n简单加权融合： 分类问题：voting 回归问题：average 综合：排序融合 (Rank averaging)，log 融合等 更为复杂的融合方法，详见下面的集成学习方法。 集成学习（Ensemble Learning）是指训练多个弱模型（基学习器，Base Learners），并将它们组合成一个更强的模型，以提高整体的泛化能力和鲁棒性。目标是让多个弱模型相互补充，形成一个强模型，而不仅仅是融合多个已有的强模型。\n常见的集成学习方法：\nBagging（Bootstrap AGGregatING）装袋算法 核心思想：通过 Bootstrap 自助采样法对训练数据集进行有放回抽样，训练多个相同类型的弱模型，然后取平均值（回归任务）或投票（分类任务）。==从偏差-方差分解的角度看，Bagging 主要关注降低方差==。 代表算法：随机森林（Random Forest）。 Boosting（提升方法）将弱学习器提升为强学习器 核心思想：让每个新的弱学习器重点关注之前模型分错的样本后生成新的学习器。最后将之前所有学习器加权结合。==从偏差-方差分解的角度看，Boosting 主要关注降低偏差==。 代表算法：AdaBoost、Gradient Boosting（GBDT）、XGBoost、LightGBM、CatBoost。 Stacking（堆叠泛化） 核心思想：将多个基模型的预测作为新特征，输入到更高一级的元模型（Meta Learner） 进行学习，如此循环，最后输出一个最终模型。 区别于模型融合的 Stacking：这里的基模型可能是专门为集成学习设计的弱模型。 可参考其它优秀博文机器学习-集成学习（模型融合）方法概述、【机器学习】模型融合方法概述\n",
  "wordCount" : "5159",
  "inLanguage": "en",
  "datePublished": "2025-03-11T00:00:00Z",
  "dateModified": "2025-03-13T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lovegood"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-machine-learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lovegood's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://20250303.xyz/img/xx.gif"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://20250303.xyz/" accesskey="h" title="Lovegood&#39;s Blog (Alt + H)">
                <img src="https://20250303.xyz/img/xx.gif" alt="" aria-label="logo"
                    height="35">Lovegood&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://20250303.xyz/search" title="🔍搜索 (Alt &#43; /)" accesskey=/>
                    <span>🔍搜索</span>
                </a>
            </li>
            <li>
                <a href="https://20250303.xyz/" title="🏠主页">
                    <span>🏠主页</span>
                </a>
            </li>
            <li>
                <a href="https://20250303.xyz/posts" title="📚博文">
                    <span>📚博文</span>
                </a>
            </li>
            <li>
                <a href="https://20250303.xyz/archives/" title="⏱时间轴">
                    <span>⏱时间轴</span>
                </a>
            </li>
            <li>
                <a href="https://20250303.xyz/tags" title="🔖标签">
                    <span>🔖标签</span>
                </a>
            </li>
            <li>
                <a href="https://20250303.xyz/about" title="🙋🏻‍♂️关于我">
                    <span>🙋🏻‍♂️关于我</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">
<style>
  i[id*="post_meta_style"] {
    display: flex;
    align-items: center;
    margin: 0 0 10px 0;
  }
</style>

<article class="post-single">
  <div id="single-content">
  <div id="single-content">
    <header class="post-header">
      <div class="breadcrumbs"><a href="https://20250303.xyz/">Home</a>&nbsp;»&nbsp;<a href="https://20250303.xyz/posts/">Posts</a></div>
      <h1 class="post-title">
        机器学习 Machine Learning
      </h1>
      <div class="post-meta">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-03-11
            &nbsp;&nbsp;
        </span>
    </span>
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>5159 words
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>11 minutes
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Lovegood
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://20250303.xyz/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" style="color: var(--secondary)!important;">统计学</a>
                &nbsp;<a href="https://20250303.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="color: var(--secondary)!important;">机器学习</a>
            </span>
        </span>
    </span>
</span>
        
        <span style="opacity: 0.8;">
          <span id="post_meta_style_7">
            &nbsp;&nbsp;
            <span class="fa fa-eye"></span>
            <span>
              <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
              &nbsp;&nbsp;
            </span>
          </span>
          <span id="post_meta_style_8">
            <span class="fa fa-commenting-o"></span>
            <span>
              <script
                src="https://cdn.staticfile.org/twikoo/1.6.41/twikoo.all.min.js"></script>
              <script>
                let url = document.documentURI
                
                let dnsUrl = "https://20250303.xyz/"
                let urlSplit = url.split(dnsUrl)
                let finalUrl = urlSplit[1]
                if (finalUrl[0] !== '/') {
                  finalUrl = '/' + finalUrl
                }
                twikoo.getCommentsCount({
                  envId: "https://twikoo.20250303.xyz", 
                  region:  null , 
                  urls: [ 
                  
                  finalUrl,
                ],
                  includeReply: false 
                                }).then(function (res) {
                    let count = res[0].count
                    const obj = document.getElementById("comment_count");
                    obj.innerText = count
                    
                    
                    
                  }).catch(function (err) {
                    
                    console.error(err);
                  });
              </script>
              <span id="comment_count"></span>
            </span>
          </span>
        </span>

</div>
    </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e6%89%93%e5%bc%80%e9%9d%9e%e7%ba%bf%e6%80%a7%e7%a9%ba%e9%97%b4%e7%9a%84%e9%92%a5%e5%8c%99%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0" aria-label="打开非线性空间的钥匙：激活函数">打开非线性空间的钥匙：激活函数</a></li>
                    <li>
                        <a href="#%e5%90%84%e7%a7%8d%e7%ae%97%e6%b3%95" aria-label="各种算法">各种算法</a><ul>
                            
                    <li>
                        <a href="#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92logistic-regression%e5%af%b9%e6%95%b0%e5%87%a0%e7%8e%87%e5%9b%9e%e5%bd%92" aria-label="逻辑回归，Logistic Regression，对数几率回归">逻辑回归，Logistic Regression，对数几率回归</a></li>
                    <li>
                        <a href="#%e7%ba%bf%e6%80%a7%e5%88%a4%e5%88%ab%e5%88%86%e6%9e%90-lda--fisher%e5%88%a4%e5%88%ab" aria-label="线性判别分析 LDA &amp;amp; Fisher判别">线性判别分析 LDA &amp; Fisher判别</a></li>
                    <li>
                        <a href="#%e5%86%b3%e7%ad%96%e6%a0%91-decision-tree" aria-label="决策树 Decision Tree">决策树 Decision Tree</a></li>
                    <li>
                        <a href="#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" aria-label="神经网络">神经网络</a></li>
                    <li>
                        <a href="#%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba-svm" aria-label="支持向量机 SVM">支持向量机 SVM</a></li>
                    <li>
                        <a href="#em-%e7%ae%97%e6%b3%95" aria-label="EM 算法">EM 算法</a></li>
                    <li>
                        <a href="#%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b" aria-label="隐马尔可夫模型">隐马尔可夫模型</a></li>
                    <li>
                        <a href="#%e5%85%b6%e5%ae%83" aria-label="其它">其它</a></li>
                    <li>
                        <a href="#%e5%a6%82%e4%bd%95%e5%a4%84%e7%90%86%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e4%b8%ad%e7%9a%84%e9%ab%98%e6%96%b9%e5%b7%ae%e6%88%96%e9%ab%98%e5%81%8f%e5%b7%ae%e9%97%ae%e9%a2%98" aria-label="如何处理模型训练中的高方差或高偏差问题？">如何处理模型训练中的高方差或高偏差问题？</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e8%af%84%e4%bb%b7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%8c%87%e6%a0%87" aria-label="评价模型的指标">评价模型的指标</a></li>
                    <li>
                        <a href="#kaggle%e6%af%94%e8%b5%9b%e4%b8%ad%e6%8f%90%e9%ab%98%e6%88%90%e7%bb%a9%e7%9a%843%e4%b8%aa%e4%b8%bb%e8%a6%81%e5%9c%b0%e6%96%b9" aria-label="Kaggle比赛中提高成绩的3个主要地方">Kaggle比赛中提高成绩的3个主要地方</a><ul>
                            
                    <li>
                        <a href="#%e7%89%b9%e5%be%81%e5%b7%a5%e7%a8%8b" aria-label="特征工程">特征工程</a><ul>
                            
                    <li>
                        <a href="#%e7%bc%96%e7%a0%81%e6%96%b9%e5%bc%8f-encoding-method" aria-label="编码方式 Encoding Method">编码方式 Encoding Method</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e8%b0%83%e5%8f%82" aria-label="调参">调参</a></li>
                    <li>
                        <a href="#%e4%b8%89%e4%b8%aa%e8%87%ad%e7%9a%ae%e5%8c%a0%e9%a1%b6%e4%b8%80%e4%b8%aa%e8%af%b8%e8%91%9b%e4%ba%ae---%e6%a8%a1%e5%9e%8b%e8%9e%8d%e5%90%88%e4%b8%8e%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0" aria-label="“三个臭皮匠，顶一个诸葛亮” - 模型融合与集成学习">“三个臭皮匠，顶一个诸葛亮” - 模型融合与集成学习</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>
    <div class="post-content">
      <h2 id="打开非线性空间的钥匙激活函数">打开非线性空间的钥匙：激活函数<a hidden class="anchor" aria-hidden="true" href="#打开非线性空间的钥匙激活函数">#</a></h2>
<p><img loading="lazy" src="https://pictest.20250303.xyz/img/202503111417024.png" alt="image.png"  />
</p>
<p>激活函数的主要作用是引入非线性，使神经网络能够处理非线性问题，从而增强模型的表达能力，使其能够拟合复杂的函数关系。</p>
<p>LSTM 便是通过激活函数实现“遗忘门”、“输入门”等功能：</p>
<p><img loading="lazy" src="https://pictest.20250303.xyz/img/202503111436325.png" alt="image.png"  />
</p>
<hr>
<h2 id="各种算法">各种算法<a hidden class="anchor" aria-hidden="true" href="#各种算法">#</a></h2>
<p>在二分类问题中，模型通常会输出一个概率得分（如 Softmax），然后通过设定一个阈值（Threshold）来决定最终的分类结果（正例 or 负例）。 ^p2v94r</p>
<p>若是多分类问题，则将多分类任务拆为若干个二分类任务，拆分方法有一对一、一对其余和多对多，详见<a href="https://blog.csdn.net/anshuai_aw1/article/details/82973039">机器学习中的多分类任务详解</a></p>
<blockquote>
<p>损失函数不仅可以从代数、几何角度来理解，还可以从概率角度来理解，强推视频<a href="https://space.bilibili.com/504715181/lists/203075?type=season">合集-正则化</a></p>
</blockquote>
<h3 id="逻辑回归logistic-regression对数几率回归">逻辑回归，Logistic Regression，对数几率回归<a hidden class="anchor" aria-hidden="true" href="#逻辑回归logistic-regression对数几率回归">#</a></h3>
<p>逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，$y=\frac{1}{1+e^{-(w^Tx+b)}}$，使得逻辑回归称为了一个优秀的分类算法。注意，虽然它的名字中有“回归”二字，但其实是一个分类算法。</p>
<p><strong>似然函数</strong> $L(w) = \prod [p(x_i)]^{y_i} [1 - p(x_i)]^{1 - y_i}$, 关于其中的累乘项 $P(y_i | x_i) = p(x_i)^{y_i} [1 - p(x_i)]^{1 - y_i}$:</p>
<ul>
<li>当 $y_i = 1$ 时，该公式变为 $p(x_i)$，即模型预测 $x_i$ 为 1 的概率。</li>
<li>当 $y_i = 0$ 时，该公式变为 $1 - p(x_i)$，即模型预测 $x_i$ 为 0 的概率。</li>
</ul>
<p>由于乘积运算不方便计算，我们通常取对数得到<strong>对数似然函数（log-likelihood）</strong>:$log L(w) = \sum y_i \log p(x_i) + (1 - y_i) \log (1 - p(x_i))$</p>
<p>最大化对数似然等价于最小化<strong>交叉熵损失函数</strong>（Cross-Entropy Loss，即对似然函数取负对数）。注意这里<strong>不使用 MSE 来构造损失函数</strong> $\mathcal{L}_{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - p(x_i))^2$ 是因为这个损失函数非凸（一个函数 $f(w)$ 关于 $w$ 是凸的，当且仅当它的 Hessian 矩阵 $H(f)$ 半正定），使得优化过程容易陷入局部最优，难以收敛到全局最优解。</p>
<p>如何求解？</p>
<ul>
<li>
<p><strong>梯度下降法</strong>（通过一阶导数来找下降方向，又称“最速下降法”）</p>
<ul>
<li>批量梯度下降法（Batch Gradient Descent，BGD）<mark>每次迭代都使用全体样本</mark>，最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</li>
<li>随机梯度下降（Stochastic Gradient Descent，SGD）<mark>每次迭代只使用一个样本（或全体样本的某个子集）</mark>，最小化每条样本（或全体样本的某个子集）的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向，但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</li>
</ul>
</li>
<li>
<p><strong>牛顿法</strong>（在初值点附近对函数做泰勒展开，取展开式的前几项来寻找下降方向）。</p>
<ul>
<li>从本质上去看，牛顿法是二阶（或更高阶，取决于取几阶泰勒展开）收敛，梯度下降是一阶收敛，所以牛顿法就更快。</li>
<li>从几何上说，牛顿法就是用一个二次（或更高次）曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</li>
<li>牛顿法能够根据最优点距离自适应选择迭代步长（学习率）。</li>
<li><mark>缺点是每次都需要求解复杂的Hessian矩阵的逆矩阵</mark>。</li>
<li>于是改善出了：<strong>拟牛顿法</strong>：使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</li>
</ul>
</li>
<li>
<p>正则化，Regularizaation（花书：凡是可以减少泛化误差而不是训练误差的方法都可以称为正则化方法），目的是防止模型过拟合，让误差函数不仅取决于模型拟合的好坏（如 SSE 大小等），还取决于模型的复杂度（如模型中非线性参数的大小，因为一般模型非线性参数越多越大，模型就越复杂；另外大参数也会放大噪声和误差）</p>
<ul>
<li>L1 正则化，Lasso 回归，使用曼哈顿距离构造可行域（约束），L1通常用于增加稀疏性，有点类似因子旋转[[多元统计分析复习笔记#关于旋转💫]]。</li>
<li>L2 正则化，Ridge 回归，使用欧几里得距离构造可行域（约束），L2一般用于减小复杂度。</li>
<li>但是没有 L0.5 正则化，因为 L0.5 范数对应的可行域不是凸集，可见<a href="https://towardsdatascience.com/wp-content/uploads/2023/11/1ZIjL7fI-OsPdWGl5p6147w.png">图片</a>；那有没有 L3、L4 正则化呢？单纯从凸优化的角度来说，是没问题的。</li>
<li><img loading="lazy" src="https://pictest.20250303.xyz/img/202503122125608.png" alt="image.png"  />
</li>
</ul>
</li>
<li>
<p>丢弃法，Dropout，随机让神经网络中的一些神经元失效。</p>
</li>
</ul>
<p>逻辑回归和最大熵模型本质上没有区别。(最大熵模型可参考<a href="https://zhuanlan.zhihu.com/p/548253677">一文带你了解最大熵模型（含详细推导）</a>)</p>
<blockquote>
<p>与 SVM 的对比：LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布。—— <a href="https://zhuanlan.zhihu.com/p/74874291">【机器学习】逻辑回归（非常详细）</a></p>
</blockquote>
<h3 id="线性判别分析-lda--fisher判别">线性判别分析 LDA &amp; Fisher判别<a hidden class="anchor" aria-hidden="true" href="#线性判别分析-lda--fisher判别">#</a></h3>
<p>可参考[[多元统计分析复习笔记#判别分析]]</p>
<p>线性判别分析（LDA）的目标是<strong>找到一个投影方向</strong>，使得投影后的类间散度最大化，同时类内散度最小化，从而实现最佳的类别分离。</p>
<h3 id="决策树-decision-tree">决策树 Decision Tree<a hidden class="anchor" aria-hidden="true" href="#决策树-decision-tree">#</a></h3>
<p>划分指标：</p>
<ul>
<li>信息增益（其中会使用到信息熵作为集合纯度的度量），信息增益越大，则意味着使用该属性来进行划分所获得的“纯度提升”越大。信息增益对可取值数目较多的属性有所偏好；</li>
<li>增益率，与信息增益相反，增益率对可取值数目较少的属性有所偏好；</li>
<li>基尼指数，反映从数据集中随机抽取两个样本，其类别标记不一致的概率。基尼指数值越小，数据集的纯度越高。</li>
</ul>
<p>剪枝处理：避免“过拟合”，基本策略有预剪枝和后剪枝两种。</p>
<ul>
<li>预剪枝：在生成决策树的过程中，边生成边剪枝；<mark>好处</mark>是训练时间开销小（相较于后剪枝而言，因为后剪枝要在决策树建立完成后，自底向上地对树中的所有非叶节点进行注意考察），但是“贪心”本质可能导致欠拟合</li>
<li>后剪枝：先把决策树生成完，最后再剪枝。优点是欠拟合风险小，但是训练时间开销大。</li>
</ul>
<p>随机森林可见后文[[机器学习 Machine Learning#“三个臭皮匠，顶一个诸葛亮” - 模型融合与集成学习]]部分</p>
<h3 id="神经网络">神经网络<a hidden class="anchor" aria-hidden="true" href="#神经网络">#</a></h3>
<p>感知机，指单层的人工神经网络，区别于较复杂的多层感知机。</p>
<p>前馈神经网络，在前馈网络中，资讯总是朝一个方向移动，从来不会倒退。可见<a href="https://bkimg.cdn.bcebos.com/pic/55e736d12f2eb938d3a375f9d4628535e4dd6fea?x-bce-process=image/format,f_auto/watermark,image_d2F0ZXIvYmFpa2UyNzI,g_7,xp_5,yp_5,P_20/resize,m_lfit,limit_1,h_1080">图</a></p>
<p>误差逆传播算法（BP算法，反向传播算法）</p>
<p>经典和目前主流算法：卷积神经网络、循环神经网络、LSTM、GRU、Transformer、BERT</p>
<h3 id="支持向量机-svm">支持向量机 SVM<a hidden class="anchor" aria-hidden="true" href="#支持向量机-svm">#</a></h3>
<p><img loading="lazy" src="https://miro.medium.com/v2/resize:fit:1254/0*-38436iwNouasVz2.png" alt="SVM配图"  />
</p>
<p>SVM是一种二类分类模型，核心目标是在特征空间中找到一个能够最大化两类数据间隔（Margin）的决策边界。</p>
<ul>
<li>线性可分SVM：当训练数据线性可分时，通过<mark>硬间隔</mark>最大化可以学习得到一个线性分类器，即硬间隔SVM。</li>
<li>线性SVM：当训练数据不能线性可分但是可以近似线性可分时，通过<mark>软间隔(允许部分数据点被错误分类，以提高泛化能力)</mark>最大化也可以学习到一个线性分类器，即软间隔SVM。</li>
<li>非线性SVM：当训练数据线性不可分时，通过使用<mark>核技巧(将数据映射到高维空间，使其线性可分)</mark>和软间隔最大化，可以学习到一个非线性SVM。</li>
</ul>
<h3 id="em-算法">EM 算法<a hidden class="anchor" aria-hidden="true" href="#em-算法">#</a></h3>
<p>是在含有隐变量的概率模型中寻找参数最大似然估计或者最大后验估计的算法。</p>
<p>最大期望算法经过两个步骤交替进行计算：</p>
<ol>
<li>第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；</li>
<li>第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。</li>
</ol>
<h3 id="隐马尔可夫模型">隐马尔可夫模型<a hidden class="anchor" aria-hidden="true" href="#隐马尔可夫模型">#</a></h3>
<p>用来描述一个含有隐含未知参数的马尔可夫过程</p>
<p><a href="https://www.cnblogs.com/skyme/p/4651331.html">https://www.cnblogs.com/skyme/p/4651331.html</a></p>
<p>NLP 中有这个算法</p>
<p>写累了，先暂略</p>
<h3 id="其它">其它<a hidden class="anchor" aria-hidden="true" href="#其它">#</a></h3>
<p>贝叶斯分类器见多元统计分析相关内容[[多元统计分析复习笔记#^dkbbsu]]</p>
<p>聚类算法见多元统计分析相关内容[[多元统计分析复习笔记#聚类分析]]</p>
<h3 id="如何处理模型训练中的高方差或高偏差问题">如何处理模型训练中的高方差或高偏差问题？<a hidden class="anchor" aria-hidden="true" href="#如何处理模型训练中的高方差或高偏差问题">#</a></h3>
<p><a href="https://www.jvruo.com/archives/1277/">https://www.jvruo.com/archives/1277/</a></p>
<hr>
<h2 id="评价模型的指标">评价模型的指标<a hidden class="anchor" aria-hidden="true" href="#评价模型的指标">#</a></h2>
<p><img loading="lazy" src="https://pictest.20250303.xyz/img/202503122126356.png" alt="image.png"  />
</p>
<ul>
<li>准确率、精度 $Accuracy=\frac{TP+TN}{TP+FP+FN+TN}$</li>
<li>精准率、精确率、查准率 $Precision=\frac{TP}{TP+FP}$，预测为正例的样本中，有多少是正确的</li>
<li>召回率、查全率 $Recall=\frac{TP}{TP+FN}$，真实为正例的样本中，有多少被成功找到了</li>
<li><mark>区分查准&amp;查全！精确率和召回率两者的关系是此消彼长的，于是给出 F1 值进行调和平均</mark></li>
<li>F1 值： $F1=2·\frac{Precision·Recall}{Precision+Recall}$, 即 Precision 和 Recall 的调和平均
<ul>
<li>引申 $F_\beta$ 值，用参数 $\beta$ 来度量 Precision 和 Recall 哪个更重要：Recall 的重要性是 Precision 的 $\beta$ 倍，F1 值算是特殊情况，把 Precision 和 Recall 看作同等重要。$F_\beta=(1+\beta^2)·\frac{Precision·Recall}{\beta^2·Precision+Recall}$</li>
</ul>
</li>
<li>ROC 曲线（Receiver Operating Characteristic curve 接收者操作特征曲线）与 AUC 值，<mark>适合数据集中类别分布均衡时使用</mark>：
<ul>
<li>$\text{True Positive Rate}=\frac{TP}{TP+FN}$, 即 $Recall$.</li>
<li>$\text{False Positive Rate} = \frac{FP}{FP+TN}$</li>
<li>设置阈值从 0-1（如何理解这里的阈值？[[机器学习 Machine Learning#^p2v94r]]），可以得到不同的 TPR 和 FPR。以 TPR 为纵坐标、FPR 为横坐标，即可绘制出 ROC 曲线，我们希望 TPR↑，FPR↓，即希望曲线靠近左上角，故给出量化标准 AUC 值：ROC 曲线下的面积。AUC 值越大越好。</li>
</ul>
</li>
<li>PR 曲线：当我们的数据集中<mark>类别分布不均衡时</mark>我们可以用 PR 曲线代替 ROC 曲线．PR 曲线与 ROC 曲线的区别在于 PR 曲线以 Recall 作为 x 轴，Precision 作为 y 轴。我们希望 Precision↑，Recall↑，即希望曲线靠近右上角，</li>
<li>ROC 曲线与 PR 曲线的区别与联系：
<ul>
<li>可以说，每一条 ROC 曲线都与一条 PR 曲线相对应，因为混淆矩阵的 TPR-FPR 数值对和 Precision-Recall都有唯一的数值对。</li>
<li>如果在 ROC 空间中，曲线 1 优于曲线 2，那么在 PR 空间中，曲线 1 对应的 PR 曲线同样优于曲线 2 对应的 PR 曲线。</li>
<li>ROC 曲线对正负两类样本同样关系，而 PR 曲线则对正例更加关心。</li>
<li>ROC 适合类别均衡时使用，而 PR 曲线适用类别不均衡场景。</li>
</ul>
</li>
<li>敏感性、灵敏度、真阳性率 $Sensitivity=\frac{TP}{TP+FN}$</li>
<li>特异性、特异度 $Specificity=\frac{TN}{TN+FN}$</li>
<li>AIC 赤池信息量和BIC 贝叶斯信息量，是基于似然函数的估计方法，主要用于统计建模，尤其是在回归分析和时间序列分析等领域，<mark>在机器学习领域并不常用</mark>。[[回归分析#^grw9h5]]</li>
</ul>
<hr>
<h2 id="kaggle比赛中提高成绩的3个主要地方">Kaggle比赛中提高成绩的3个主要地方<a hidden class="anchor" aria-hidden="true" href="#kaggle比赛中提高成绩的3个主要地方">#</a></h2>
<h3 id="特征工程">特征工程<a hidden class="anchor" aria-hidden="true" href="#特征工程">#</a></h3>
<p>特征工程（Feature Engineering）特征工程是将原始数据转化成更好的表达问题本质的特征的过程，使得将这些特征运用到预测模型中能提高对不可见数据的模型预测精度。</p>
<p><img loading="lazy" src="https://pic1.zhimg.com/v2-e98197d5c976879e1515c078a3134512_1440w.jpg" alt="图片"  />

图源<a href="https://zhuanlan.zhihu.com/p/111296130">深度了解特征工程</a>。感觉有点像数据分析中的数据预处理。</p>
<h4 id="编码方式-encoding-method">编码方式 Encoding Method<a hidden class="anchor" aria-hidden="true" href="#编码方式-encoding-method">#</a></h4>
<p>频率编码（Frequency Encoding）、嵌入编码（Embedding Encoding）的优缺点。</p>
<h3 id="调参">调参<a hidden class="anchor" aria-hidden="true" href="#调参">#</a></h3>
<p>参考<a href="https://cloud.tencent.com/developer/article/1803092">机器学习：模型调参大法总结</a></p>
<h3 id="三个臭皮匠顶一个诸葛亮---模型融合与集成学习">“三个臭皮匠，顶一个诸葛亮” - 模型融合与集成学习<a hidden class="anchor" aria-hidden="true" href="#三个臭皮匠顶一个诸葛亮---模型融合与集成学习">#</a></h3>
<p>模型融合（Model Fusion）指的是将多个不同的机器学习模型的预测结果进行组合，生成一个新的、更为准确的预测。以提高最终的预测性能、稳定性和泛化能力。它的核心思想是利用多个模型的优势，减少单一模型的偏差和方差，通常用于提升分类或回归任务的精度。</p>
<ul>
<li>简单加权融合：
<ul>
<li>分类问题：voting</li>
<li>回归问题：average</li>
<li>综合：排序融合 (Rank averaging)，log 融合等</li>
</ul>
</li>
<li>更为复杂的融合方法，详见下面的集成学习方法。</li>
</ul>
<p>集成学习（Ensemble Learning）是指训练多个弱模型（基学习器，Base Learners），并将它们组合成一个更强的模型，以提高整体的泛化能力和鲁棒性。目标是让多个弱模型相互补充，形成一个强模型，而不仅仅是融合多个已有的强模型。</p>
<p>常见的集成学习方法：</p>
<ol>
<li>Bagging（Bootstrap AGGregatING）装袋算法
<ul>
<li>核心思想：通过 Bootstrap 自助采样法对训练数据集进行有放回抽样，训练多个相同类型的弱模型，然后取平均值（回归任务）或投票（分类任务）。<mark>从偏差-方差分解的角度看，Bagging 主要关注降低方差</mark>。</li>
<li>代表算法：随机森林（Random Forest）。</li>
</ul>
</li>
<li>Boosting（提升方法）将弱学习器提升为强学习器
<ul>
<li>核心思想：让每个新的弱学习器重点关注之前模型分错的样本后生成新的学习器。最后将之前所有学习器加权结合。<mark>从偏差-方差分解的角度看，Boosting 主要关注降低偏差</mark>。</li>
<li>代表算法：AdaBoost、Gradient Boosting（GBDT）、XGBoost、LightGBM、CatBoost。</li>
</ul>
</li>
<li>Stacking（堆叠泛化）
<ul>
<li>核心思想：将多个基模型的预测作为新特征，输入到更高一级的元模型（Meta Learner） 进行学习，如此循环，最后输出一个最终模型。</li>
<li>区别于模型融合的 Stacking：这里的基模型可能是专门为集成学习设计的弱模型。</li>
</ul>
</li>
</ol>
<p>可参考其它优秀博文<a href="https://blog.csdn.net/weixin_41140174/article/details/126291940">机器学习-集成学习（模型融合）方法概述</a>、<a href="https://zhuanlan.zhihu.com/p/25836678">【机器学习】模型融合方法概述</a></p>


    </div>

    <footer class="post-footer">
      
<nav class="paginav">
  <a class="next" href="https://20250303.xyz/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6-ml/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/">
    <span class="title"> »</span>
    <br>
    <span>回归分析</span>
  </a>
</nav>

    </footer>
  </div><style>
    .comments_details summary::marker {
        font-size: 20px;
        content: '👉展开评论';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: '👇关闭评论';
        color: var(--content);
    }
</style>


<div>
    <details class="comments_details">
        <summary style="cursor: pointer; margin: 50px 0 20px 0;width: 130px;">
            <span style="font-size: 20px;color: var(--content);">...</span>
        </summary>
        <div id="tcomment"></div>
    </details>
    <script src=" https://cdn.jsdelivr.net/npm/twikoo@1.6.41/dist/twikoo.min.js 1.6.41/twikoo.all.min.js">
    
    </script>
    <script>
        twikoo.init({
            envId: "https://twikoo.20250303.xyz",
            el: "#tcomment",
            lang: 'zh-CN',
            region:  null ,
            
        })
    </script>
</div>
</article>
    </main>
    
<footer class="footer">
    <span>
        Copyright
        &copy;
        2024-2025
        <a href="https://20250303.xyz/" style="color:#939393;">Lovegood&#39;s Blog</a>
        All Rights Reserved.
    </span>
    
    <div class="busuanzi-footer">
        <span id="busuanzi_container_site_pv">
            本站总访问量<span id="busuanzi_value_site_pv"></span>次
        </span>
        <span id="busuanzi_container_site_uv">
            本站访客数<span id="busuanzi_value_site_uv"></span>人次
        </span>
        </div>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"Lovegood's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"Lovegood's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\n————————————————\r\n' +
                    '版权声明：本文为「'+"Lovegood's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            
        }
    });
</script></body>







</html>
